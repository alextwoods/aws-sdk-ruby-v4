# frozen_string_literal: true

# WARNING ABOUT GENERATED CODE
#
# This file was code generated using smithy-ruby.
# https://github.com/awslabs/smithy-ruby
#
# WARNING ABOUT GENERATED CODE

require 'stringio'

require_relative 'middleware/request_id'

module AWS::SDK::Glue
  # An API client for AWSGlue
  # See {#initialize} for a full list of supported configuration options
  # <fullname>Glue</fullname>
  #          <p>Defines the public endpoint for the Glue service.</p>
  #
  class Client
    include Hearth::ClientStubs

    @middleware = Hearth::MiddlewareBuilder.new

    def self.middleware
      @middleware
    end

    # @param [Config] config
    #   An instance of {Config}
    #
    def initialize(config = AWS::SDK::Glue::Config.new, options = {})
      @config = config
      @middleware = Hearth::MiddlewareBuilder.new(options[:middleware])
      @stubs = Hearth::Stubbing::Stubs.new
      @retry_quota = Hearth::Retry::RetryQuota.new
      @client_rate_limiter = Hearth::Retry::ClientRateLimiter.new
    end

    # <p>Creates one or more partitions in a batch operation.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchCreatePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the catalog in which the partition is to be created. Currently, this should be
    #         the Amazon Web Services account ID.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the metadata database in which the partition is
    #         to be created.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the metadata table in which the partition is to be created.</p>
    #
    # @option params [Array<PartitionInput>] :partition_input_list
    #   <p>A list of <code>PartitionInput</code> structures that define
    #         the partitions to be created.</p>
    #
    # @return [Types::BatchCreatePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_create_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_input_list: [
    #       {
    #         values: [
    #           'member'
    #         ],
    #         last_access_time: Time.now,
    #         storage_descriptor: {
    #           columns: [
    #             {
    #               name: 'Name', # required
    #               type: 'Type',
    #               comment: 'Comment',
    #               parameters: {
    #                 'key' => 'value'
    #               }
    #             }
    #           ],
    #           location: 'Location',
    #           additional_locations: [
    #             'member'
    #           ],
    #           input_format: 'InputFormat',
    #           output_format: 'OutputFormat',
    #           compressed: false,
    #           number_of_buckets: 1,
    #           serde_info: {
    #             name: 'Name',
    #             serialization_library: 'SerializationLibrary',
    #           },
    #           bucket_columns: [
    #             'member'
    #           ],
    #           sort_columns: [
    #             {
    #               column: 'Column', # required
    #               sort_order: 1 # required
    #             }
    #           ],
    #           skewed_info: {
    #             skewed_column_values: [
    #               'member'
    #             ],
    #             skewed_column_value_location_maps: {
    #               'key' => 'value'
    #             }
    #           },
    #           stored_as_sub_directories: false,
    #           schema_reference: {
    #             schema_id: {
    #               schema_arn: 'SchemaArn',
    #               schema_name: 'SchemaName',
    #               registry_name: 'RegistryName'
    #             },
    #             schema_version_id: 'SchemaVersionId',
    #             schema_version_number: 1
    #           }
    #         },
    #         last_analyzed_time: Time.now
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchCreatePartitionOutput
    #   resp.data.errors #=> Array<PartitionError>
    #   resp.data.errors[0] #=> Types::PartitionError
    #   resp.data.errors[0].partition_values #=> Array<String>
    #   resp.data.errors[0].partition_values[0] #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_create_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchCreatePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchCreatePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchCreatePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::BatchCreatePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchCreatePartition,
        stubs: @stubs,
        params_class: Params::BatchCreatePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_create_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a list of connection definitions from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchDeleteConnectionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the connections reside. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [Array<String>] :connection_name_list
    #   <p>A list of names of the connections to delete.</p>
    #
    # @return [Types::BatchDeleteConnectionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_delete_connection(
    #     catalog_id: 'CatalogId',
    #     connection_name_list: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchDeleteConnectionOutput
    #   resp.data.succeeded #=> Array<String>
    #   resp.data.succeeded[0] #=> String
    #   resp.data.errors #=> Hash<String, ErrorDetail>
    #   resp.data.errors['key'] #=> Types::ErrorDetail
    #   resp.data.errors['key'].error_code #=> String
    #   resp.data.errors['key'].error_message #=> String
    #
    def batch_delete_connection(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchDeleteConnectionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchDeleteConnectionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchDeleteConnection
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchDeleteConnection
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchDeleteConnection,
        stubs: @stubs,
        params_class: Params::BatchDeleteConnectionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_delete_connection
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes one or more partitions in a batch operation.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchDeletePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partition to be deleted resides. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the table in question
    #         resides.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table that contains the partitions to be deleted.</p>
    #
    # @option params [Array<PartitionValueList>] :partitions_to_delete
    #   <p>A list of <code>PartitionInput</code> structures that define
    #         the partitions to be deleted.</p>
    #
    # @return [Types::BatchDeletePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_delete_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partitions_to_delete: [
    #       {
    #         values: [
    #           'member'
    #         ] # required
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchDeletePartitionOutput
    #   resp.data.errors #=> Array<PartitionError>
    #   resp.data.errors[0] #=> Types::PartitionError
    #   resp.data.errors[0].partition_values #=> Array<String>
    #   resp.data.errors[0].partition_values[0] #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_delete_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchDeletePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchDeletePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchDeletePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchDeletePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchDeletePartition,
        stubs: @stubs,
        params_class: Params::BatchDeletePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_delete_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes multiple tables at once.</p>
    #          <note>
    #             <p>After completing this operation, you no longer have access to the table versions and
    #         partitions that belong to the deleted table. Glue deletes these "orphaned" resources
    #         asynchronously in a timely manner, at the discretion of the service.</p>
    #             <p>To ensure the immediate deletion of all related resources, before calling
    #           <code>BatchDeleteTable</code>, use <code>DeleteTableVersion</code> or
    #           <code>BatchDeleteTableVersion</code>, and <code>DeletePartition</code> or
    #           <code>BatchDeletePartition</code>, to delete any resources that belong to the
    #         table.</p>
    #          </note>
    #
    # @param [Hash] params
    #   See {Types::BatchDeleteTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the table resides. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the tables to delete reside. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [Array<String>] :tables_to_delete
    #   <p>A list of the table to delete.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to delete the table contents.</p>
    #
    # @return [Types::BatchDeleteTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_delete_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     tables_to_delete: [
    #       'member'
    #     ], # required
    #     transaction_id: 'TransactionId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchDeleteTableOutput
    #   resp.data.errors #=> Array<TableError>
    #   resp.data.errors[0] #=> Types::TableError
    #   resp.data.errors[0].table_name #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_delete_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchDeleteTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchDeleteTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchDeleteTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchDeleteTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchDeleteTable,
        stubs: @stubs,
        params_class: Params::BatchDeleteTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_delete_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified batch of versions of a table.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchDeleteTableVersionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the tables reside. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The database in the catalog in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table. For Hive compatibility,
    #         this name is entirely lowercase.</p>
    #
    # @option params [Array<String>] :version_ids
    #   <p>A list of the IDs of versions to be deleted. A <code>VersionId</code> is a string representation of an integer. Each version is incremented by 1.</p>
    #
    # @return [Types::BatchDeleteTableVersionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_delete_table_version(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     version_ids: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchDeleteTableVersionOutput
    #   resp.data.errors #=> Array<TableVersionError>
    #   resp.data.errors[0] #=> Types::TableVersionError
    #   resp.data.errors[0].table_name #=> String
    #   resp.data.errors[0].version_id #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_delete_table_version(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchDeleteTableVersionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchDeleteTableVersionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchDeleteTableVersion
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchDeleteTableVersion
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchDeleteTableVersion,
        stubs: @stubs,
        params_class: Params::BatchDeleteTableVersionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_delete_table_version
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves information about a list of blueprints.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetBlueprintsInput}.
    #
    # @option params [Array<String>] :names
    #   <p>A list of blueprint names.</p>
    #
    # @option params [Boolean] :include_blueprint
    #   <p>Specifies whether or not to include the blueprint in the response.</p>
    #
    # @option params [Boolean] :include_parameter_spec
    #   <p>Specifies whether or not to include the parameters, as a JSON string, for the blueprint in the response.</p>
    #
    # @return [Types::BatchGetBlueprintsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_blueprints(
    #     names: [
    #       'member'
    #     ], # required
    #     include_blueprint: false,
    #     include_parameter_spec: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetBlueprintsOutput
    #   resp.data.blueprints #=> Array<Blueprint>
    #   resp.data.blueprints[0] #=> Types::Blueprint
    #   resp.data.blueprints[0].name #=> String
    #   resp.data.blueprints[0].description #=> String
    #   resp.data.blueprints[0].created_on #=> Time
    #   resp.data.blueprints[0].last_modified_on #=> Time
    #   resp.data.blueprints[0].parameter_spec #=> String
    #   resp.data.blueprints[0].blueprint_location #=> String
    #   resp.data.blueprints[0].blueprint_service_location #=> String
    #   resp.data.blueprints[0].status #=> String, one of ["CREATING", "ACTIVE", "UPDATING", "FAILED"]
    #   resp.data.blueprints[0].error_message #=> String
    #   resp.data.blueprints[0].last_active_definition #=> Types::LastActiveDefinition
    #   resp.data.blueprints[0].last_active_definition.description #=> String
    #   resp.data.blueprints[0].last_active_definition.last_modified_on #=> Time
    #   resp.data.blueprints[0].last_active_definition.parameter_spec #=> String
    #   resp.data.blueprints[0].last_active_definition.blueprint_location #=> String
    #   resp.data.blueprints[0].last_active_definition.blueprint_service_location #=> String
    #   resp.data.missing_blueprints #=> Array<String>
    #   resp.data.missing_blueprints[0] #=> String
    #
    def batch_get_blueprints(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetBlueprintsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetBlueprintsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetBlueprints
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetBlueprints
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetBlueprints,
        stubs: @stubs,
        params_class: Params::BatchGetBlueprintsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_blueprints
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of resource metadata for a given list of crawler names. After calling the <code>ListCrawlers</code> operation, you can call this operation to access the data to which you have been granted permissions. This operation supports all IAM permissions, including permission conditions that uses tags.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetCrawlersInput}.
    #
    # @option params [Array<String>] :crawler_names
    #   <p>A list of crawler names, which might be the names returned from the
    #           <code>ListCrawlers</code> operation.</p>
    #
    # @return [Types::BatchGetCrawlersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_crawlers(
    #     crawler_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetCrawlersOutput
    #   resp.data.crawlers #=> Array<Crawler>
    #   resp.data.crawlers[0] #=> Types::Crawler
    #   resp.data.crawlers[0].name #=> String
    #   resp.data.crawlers[0].role #=> String
    #   resp.data.crawlers[0].targets #=> Types::CrawlerTargets
    #   resp.data.crawlers[0].targets.s3_targets #=> Array<S3Target>
    #   resp.data.crawlers[0].targets.s3_targets[0] #=> Types::S3Target
    #   resp.data.crawlers[0].targets.s3_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].exclusions #=> Array<String>
    #   resp.data.crawlers[0].targets.s3_targets[0].exclusions[0] #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].sample_size #=> Integer
    #   resp.data.crawlers[0].targets.s3_targets[0].event_queue_arn #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].dlq_event_queue_arn #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets #=> Array<JdbcTarget>
    #   resp.data.crawlers[0].targets.jdbc_targets[0] #=> Types::JdbcTarget
    #   resp.data.crawlers[0].targets.jdbc_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets[0].exclusions #=> Array<String>
    #   resp.data.crawlers[0].targets.mongo_db_targets #=> Array<MongoDBTarget>
    #   resp.data.crawlers[0].targets.mongo_db_targets[0] #=> Types::MongoDBTarget
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawlers[0].targets.dynamo_db_targets #=> Array<DynamoDBTarget>
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0] #=> Types::DynamoDBTarget
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].scan_rate #=> Float
    #   resp.data.crawlers[0].targets.catalog_targets #=> Array<CatalogTarget>
    #   resp.data.crawlers[0].targets.catalog_targets[0] #=> Types::CatalogTarget
    #   resp.data.crawlers[0].targets.catalog_targets[0].database_name #=> String
    #   resp.data.crawlers[0].targets.catalog_targets[0].tables #=> Array<String>
    #   resp.data.crawlers[0].targets.catalog_targets[0].tables[0] #=> String
    #   resp.data.crawlers[0].targets.catalog_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.delta_targets #=> Array<DeltaTarget>
    #   resp.data.crawlers[0].targets.delta_targets[0] #=> Types::DeltaTarget
    #   resp.data.crawlers[0].targets.delta_targets[0].delta_tables #=> Array<String>
    #   resp.data.crawlers[0].targets.delta_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.delta_targets[0].write_manifest #=> Boolean
    #   resp.data.crawlers[0].database_name #=> String
    #   resp.data.crawlers[0].description #=> String
    #   resp.data.crawlers[0].classifiers #=> Array<String>
    #   resp.data.crawlers[0].classifiers[0] #=> String
    #   resp.data.crawlers[0].recrawl_policy #=> Types::RecrawlPolicy
    #   resp.data.crawlers[0].recrawl_policy.recrawl_behavior #=> String, one of ["CRAWL_EVERYTHING", "CRAWL_NEW_FOLDERS_ONLY", "CRAWL_EVENT_MODE"]
    #   resp.data.crawlers[0].schema_change_policy #=> Types::SchemaChangePolicy
    #   resp.data.crawlers[0].schema_change_policy.update_behavior #=> String, one of ["LOG", "UPDATE_IN_DATABASE"]
    #   resp.data.crawlers[0].schema_change_policy.delete_behavior #=> String, one of ["LOG", "DELETE_FROM_DATABASE", "DEPRECATE_IN_DATABASE"]
    #   resp.data.crawlers[0].lineage_configuration #=> Types::LineageConfiguration
    #   resp.data.crawlers[0].lineage_configuration.crawler_lineage_settings #=> String, one of ["ENABLE", "DISABLE"]
    #   resp.data.crawlers[0].state #=> String, one of ["READY", "RUNNING", "STOPPING"]
    #   resp.data.crawlers[0].table_prefix #=> String
    #   resp.data.crawlers[0].schedule #=> Types::Schedule
    #   resp.data.crawlers[0].schedule.schedule_expression #=> String
    #   resp.data.crawlers[0].schedule.state #=> String, one of ["SCHEDULED", "NOT_SCHEDULED", "TRANSITIONING"]
    #   resp.data.crawlers[0].crawl_elapsed_time #=> Integer
    #   resp.data.crawlers[0].creation_time #=> Time
    #   resp.data.crawlers[0].last_updated #=> Time
    #   resp.data.crawlers[0].last_crawl #=> Types::LastCrawlInfo
    #   resp.data.crawlers[0].last_crawl.status #=> String, one of ["SUCCEEDED", "CANCELLED", "FAILED"]
    #   resp.data.crawlers[0].last_crawl.error_message #=> String
    #   resp.data.crawlers[0].last_crawl.log_group #=> String
    #   resp.data.crawlers[0].last_crawl.log_stream #=> String
    #   resp.data.crawlers[0].last_crawl.message_prefix #=> String
    #   resp.data.crawlers[0].last_crawl.start_time #=> Time
    #   resp.data.crawlers[0].version #=> Integer
    #   resp.data.crawlers[0].configuration #=> String
    #   resp.data.crawlers[0].crawler_security_configuration #=> String
    #   resp.data.crawlers[0].lake_formation_configuration #=> Types::LakeFormationConfiguration
    #   resp.data.crawlers[0].lake_formation_configuration.use_lake_formation_credentials #=> Boolean
    #   resp.data.crawlers[0].lake_formation_configuration.account_id #=> String
    #   resp.data.crawlers_not_found #=> Array<String>
    #   resp.data.crawlers_not_found[0] #=> String
    #
    def batch_get_crawlers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetCrawlersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetCrawlersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetCrawlers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetCrawlers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetCrawlers,
        stubs: @stubs,
        params_class: Params::BatchGetCrawlersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_crawlers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the details for the custom patterns specified by a list of names.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetCustomEntityTypesInput}.
    #
    # @option params [Array<String>] :names
    #   <p>A list of names of the custom patterns that you want to retrieve.</p>
    #
    # @return [Types::BatchGetCustomEntityTypesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_custom_entity_types(
    #     names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetCustomEntityTypesOutput
    #   resp.data.custom_entity_types #=> Array<CustomEntityType>
    #   resp.data.custom_entity_types[0] #=> Types::CustomEntityType
    #   resp.data.custom_entity_types[0].name #=> String
    #   resp.data.custom_entity_types[0].regex_string #=> String
    #   resp.data.custom_entity_types[0].context_words #=> Array<String>
    #   resp.data.custom_entity_types[0].context_words[0] #=> String
    #   resp.data.custom_entity_types_not_found #=> Array<String>
    #   resp.data.custom_entity_types_not_found[0] #=> String
    #
    def batch_get_custom_entity_types(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetCustomEntityTypesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetCustomEntityTypesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetCustomEntityTypes
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetCustomEntityTypes
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetCustomEntityTypes,
        stubs: @stubs,
        params_class: Params::BatchGetCustomEntityTypesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_custom_entity_types
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of resource metadata for a given list of development endpoint names. After
    #       calling the <code>ListDevEndpoints</code> operation, you can call this operation to access the
    #       data to which you have been granted permissions. This operation supports all IAM permissions,
    #       including permission conditions that uses tags.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetDevEndpointsInput}.
    #
    # @option params [Array<String>] :dev_endpoint_names
    #   <p>The list of <code>DevEndpoint</code> names, which might be the names returned from the
    #           <code>ListDevEndpoint</code> operation.</p>
    #
    # @return [Types::BatchGetDevEndpointsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_dev_endpoints(
    #     dev_endpoint_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetDevEndpointsOutput
    #   resp.data.dev_endpoints #=> Array<DevEndpoint>
    #   resp.data.dev_endpoints[0] #=> Types::DevEndpoint
    #   resp.data.dev_endpoints[0].endpoint_name #=> String
    #   resp.data.dev_endpoints[0].role_arn #=> String
    #   resp.data.dev_endpoints[0].security_group_ids #=> Array<String>
    #   resp.data.dev_endpoints[0].security_group_ids[0] #=> String
    #   resp.data.dev_endpoints[0].subnet_id #=> String
    #   resp.data.dev_endpoints[0].yarn_endpoint_address #=> String
    #   resp.data.dev_endpoints[0].private_address #=> String
    #   resp.data.dev_endpoints[0].zeppelin_remote_spark_interpreter_port #=> Integer
    #   resp.data.dev_endpoints[0].public_address #=> String
    #   resp.data.dev_endpoints[0].status #=> String
    #   resp.data.dev_endpoints[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.dev_endpoints[0].glue_version #=> String
    #   resp.data.dev_endpoints[0].number_of_workers #=> Integer
    #   resp.data.dev_endpoints[0].number_of_nodes #=> Integer
    #   resp.data.dev_endpoints[0].availability_zone #=> String
    #   resp.data.dev_endpoints[0].vpc_id #=> String
    #   resp.data.dev_endpoints[0].extra_python_libs_s3_path #=> String
    #   resp.data.dev_endpoints[0].extra_jars_s3_path #=> String
    #   resp.data.dev_endpoints[0].failure_reason #=> String
    #   resp.data.dev_endpoints[0].last_update_status #=> String
    #   resp.data.dev_endpoints[0].created_timestamp #=> Time
    #   resp.data.dev_endpoints[0].last_modified_timestamp #=> Time
    #   resp.data.dev_endpoints[0].public_key #=> String
    #   resp.data.dev_endpoints[0].public_keys #=> Array<String>
    #   resp.data.dev_endpoints[0].public_keys[0] #=> String
    #   resp.data.dev_endpoints[0].security_configuration #=> String
    #   resp.data.dev_endpoints[0].arguments #=> Hash<String, String>
    #   resp.data.dev_endpoints[0].arguments['key'] #=> String
    #   resp.data.dev_endpoints_not_found #=> Array<String>
    #   resp.data.dev_endpoints_not_found[0] #=> String
    #
    def batch_get_dev_endpoints(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetDevEndpointsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetDevEndpointsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetDevEndpoints
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::BatchGetDevEndpoints
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetDevEndpoints,
        stubs: @stubs,
        params_class: Params::BatchGetDevEndpointsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_dev_endpoints
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of resource metadata for a given list of job names. After calling the <code>ListJobs</code> operation, you can call this operation to access the data to which you have been granted permissions. This operation supports all IAM permissions, including permission conditions that uses tags.
    # </p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetJobsInput}.
    #
    # @option params [Array<String>] :job_names
    #   <p>A list of job names, which might be the names returned from the <code>ListJobs</code>
    #         operation.</p>
    #
    # @return [Types::BatchGetJobsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_jobs(
    #     job_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetJobsOutput
    #   resp.data.jobs #=> Array<Job>
    #   resp.data.jobs[0] #=> Types::Job
    #   resp.data.jobs[0].name #=> String
    #   resp.data.jobs[0].description #=> String
    #   resp.data.jobs[0].log_uri #=> String
    #   resp.data.jobs[0].role #=> String
    #   resp.data.jobs[0].created_on #=> Time
    #   resp.data.jobs[0].last_modified_on #=> Time
    #   resp.data.jobs[0].execution_property #=> Types::ExecutionProperty
    #   resp.data.jobs[0].execution_property.max_concurrent_runs #=> Integer
    #   resp.data.jobs[0].command #=> Types::JobCommand
    #   resp.data.jobs[0].command.name #=> String
    #   resp.data.jobs[0].command.script_location #=> String
    #   resp.data.jobs[0].command.python_version #=> String
    #   resp.data.jobs[0].default_arguments #=> Hash<String, String>
    #   resp.data.jobs[0].default_arguments['key'] #=> String
    #   resp.data.jobs[0].non_overridable_arguments #=> Hash<String, String>
    #   resp.data.jobs[0].connections #=> Types::ConnectionsList
    #   resp.data.jobs[0].connections.connections #=> Array<String>
    #   resp.data.jobs[0].connections.connections[0] #=> String
    #   resp.data.jobs[0].max_retries #=> Integer
    #   resp.data.jobs[0].allocated_capacity #=> Integer
    #   resp.data.jobs[0].timeout #=> Integer
    #   resp.data.jobs[0].max_capacity #=> Float
    #   resp.data.jobs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.jobs[0].number_of_workers #=> Integer
    #   resp.data.jobs[0].security_configuration #=> String
    #   resp.data.jobs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.jobs[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.jobs[0].glue_version #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes #=> Hash<String, CodeGenConfigurationNode>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'] #=> Types::CodeGenConfigurationNode
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source #=> Types::AthenaConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.schema_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0] #=> Types::GlueSchema
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns #=> Array<GlueStudioSchemaColumn>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0] #=> Types::GlueStudioSchemaColumn
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source #=> Types::JDBCConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options #=> Types::JDBCConnectorOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.filter_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.partition_column #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.lower_bound #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.upper_bound #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.num_partitions #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys_sort_order #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping['key'] #=> String, one of ["DATE", "STRING", "TIMESTAMP", "INT", "FLOAT", "LONG", "BIGDECIMAL", "BYTE", "SHORT", "DOUBLE"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.query #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source #=> Types::SparkConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.additional_options['key'] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source #=> Types::CatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source #=> Types::RedshiftSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.redshift_tmp_dir #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.tmp_dir_iam_role #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source #=> Types::S3CatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.partition_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_files #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source #=> Types::S3CsvSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_files #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.enable_sample_path #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.sample_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.separator #=> String, one of ["comma", "ctrla", "pipe", "semicolon", "tab"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.escaper #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.quote_char #=> String, one of ["quote", "quillemet", "single_quote", "disabled"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.multiline #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.with_header #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.write_header #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.skip_first #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.optimize_performance #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source #=> Types::S3JsonSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.json_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.multiline #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source #=> Types::S3ParquetSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.compression_type #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source #=> Types::RelationalCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source #=> Types::DynamoDBCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target #=> Types::JDBCConnectorTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target #=> Types::SparkConnectorTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target #=> Types::BasicCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target #=> Types::RedshiftTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.redshift_tmp_dir #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.tmp_dir_iam_role #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options #=> Types::UpsertRedshiftTargetOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.table_location #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target #=> Types::S3CatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target #=> Types::S3GlueParquetTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.compression #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target #=> Types::S3DirectTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.compression #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.format #=> String, one of ["json", "csv", "avro", "orc", "parquet"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping #=> Types::ApplyMapping
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping #=> Array<Mapping>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0] #=> Types::Mapping
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_key #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].dropped #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].children #=> Array<Mapping>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields #=> Types::SelectFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields #=> Types::DropFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field #=> Types::RenameField
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.source_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.target_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot #=> Types::Spigot
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.topk #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.prob #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join #=> Types::Join
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.join_type #=> String, one of ["equijoin", "left", "right", "outer", "leftsemi", "leftanti"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns #=> Array<JoinColumn>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0] #=> Types::JoinColumn
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0].from #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0].keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields #=> Types::SplitFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection #=> Types::SelectFromCollection
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.index #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values #=> Types::FillMissingValues
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.imputed_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.filled_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter #=> Types::Filter
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.logical_operator #=> String, one of ["AND", "OR"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters #=> Array<FilterExpression>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0] #=> Types::FilterExpression
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].operation #=> String, one of ["EQ", "LT", "GT", "LTE", "GTE", "REGEX", "ISNULL"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].negated #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values #=> Array<FilterValue>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0] #=> Types::FilterValue
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0].type #=> String, one of ["COLUMNEXTRACTED", "CONSTANT"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0].value #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code #=> Types::CustomCode
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.code #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.class_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql #=> Types::SparkSQL
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_query #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases #=> Array<SqlAlias>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0] #=> Types::SqlAlias
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].from #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].alias #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source #=> Types::DirectKinesisSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.endpoint_url #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.classification #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.delimiter #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.starting_position #=> String, one of ["latest", "trim_horizon", "earliest"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_time_in_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_records_per_shard #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_record_per_read #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.add_idle_time_between_reads #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.idle_time_between_reads_in_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.describe_shard_interval #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.num_retries #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.avoid_empty_batches #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_arn #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_arn #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_session_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.polling_time #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.record_polling_limit #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source #=> Types::DirectKafkaSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.bootstrap_servers #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.security_protocol #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.topic_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.assign #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.subscribe_pattern #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.classification #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.delimiter #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.starting_offsets #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.ending_offsets #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.poll_timeout_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.num_retries #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.max_offsets_per_trigger #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.min_partitions #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source #=> Types::CatalogKinesisSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source #=> Types::CatalogKafkaSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields #=> Types::DropNullFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list #=> Types::NullCheckBoxList
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_empty #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_null_string #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_neg_one #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list #=> Array<NullValueField>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0] #=> Types::NullValueField
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].value #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype #=> Types::Datatype
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.id #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.label #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge #=> Types::Merge
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.source #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.primary_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union #=> Types::Union
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.union_type #=> String, one of ["ALL", "DISTINCT"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection #=> Types::PIIDetection
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.pii_type #=> String, one of ["RowAudit", "RowMasking", "ColumnAudit", "ColumnMasking"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.entity_types_to_detect #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.output_column_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.sample_fraction #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.threshold_fraction #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.mask_value #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate #=> Types::Aggregate
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.groups #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs #=> Array<AggregateOperation>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0] #=> Types::AggregateOperation
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0].column #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0].agg_func #=> String, one of ["avg", "countDistinct", "count", "first", "last", "kurtosis", "max", "min", "skewness", "stddev_samp", "stddev_pop", "sum", "sumDistinct", "var_samp", "var_pop"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates #=> Types::DropDuplicates
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns[0] #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns[0][0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target #=> Types::GovernedCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source #=> Types::GovernedCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.partition_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source #=> Types::MicrosoftSQLServerCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source #=> Types::MySQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source #=> Types::OracleSQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source #=> Types::PostgreSQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target #=> Types::MicrosoftSQLServerCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target #=> Types::MySQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target #=> Types::OracleSQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target #=> Types::PostgreSQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.table #=> String
    #   resp.data.jobs_not_found #=> Array<String>
    #   resp.data.jobs_not_found[0] #=> String
    #
    def batch_get_jobs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetJobsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetJobsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetJobs
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetJobs
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetJobs,
        stubs: @stubs,
        params_class: Params::BatchGetJobsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_jobs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves partitions in a batch request.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetPartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<PartitionValueList>] :partitions_to_get
    #   <p>A list of partition values identifying the partitions to retrieve.</p>
    #
    # @return [Types::BatchGetPartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partitions_to_get: [
    #       {
    #         values: [
    #           'member'
    #         ] # required
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetPartitionOutput
    #   resp.data.partitions #=> Array<Partition>
    #   resp.data.partitions[0] #=> Types::Partition
    #   resp.data.partitions[0].values #=> Array<String>
    #   resp.data.partitions[0].values[0] #=> String
    #   resp.data.partitions[0].database_name #=> String
    #   resp.data.partitions[0].table_name #=> String
    #   resp.data.partitions[0].creation_time #=> Time
    #   resp.data.partitions[0].last_access_time #=> Time
    #   resp.data.partitions[0].storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.partitions[0].storage_descriptor.columns #=> Array<Column>
    #   resp.data.partitions[0].storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.partitions[0].storage_descriptor.columns[0].name #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].type #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].comment #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.partitions[0].storage_descriptor.location #=> String
    #   resp.data.partitions[0].storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.additional_locations[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.input_format #=> String
    #   resp.data.partitions[0].storage_descriptor.output_format #=> String
    #   resp.data.partitions[0].storage_descriptor.compressed #=> Boolean
    #   resp.data.partitions[0].storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.partitions[0].storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.partitions[0].storage_descriptor.serde_info.name #=> String
    #   resp.data.partitions[0].storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.partitions[0].storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.partitions[0].storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.partitions[0].storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.partitions[0].storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.partitions[0].parameters #=> Hash<String, String>
    #   resp.data.partitions[0].last_analyzed_time #=> Time
    #   resp.data.partitions[0].catalog_id #=> String
    #   resp.data.unprocessed_keys #=> Array<PartitionValueList>
    #   resp.data.unprocessed_keys[0] #=> Types::PartitionValueList
    #   resp.data.unprocessed_keys[0].values #=> Array<String>
    #
    def batch_get_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetPartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetPartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetPartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InvalidStateException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetPartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetPartition,
        stubs: @stubs,
        params_class: Params::BatchGetPartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of resource metadata for a given list of trigger names. After calling the <code>ListTriggers</code> operation, you can call this operation to access the data to which you have been granted permissions. This operation supports all IAM permissions, including permission conditions that uses tags.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetTriggersInput}.
    #
    # @option params [Array<String>] :trigger_names
    #   <p>A list of trigger names, which may be the names returned from the <code>ListTriggers</code> operation.</p>
    #
    # @return [Types::BatchGetTriggersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_triggers(
    #     trigger_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetTriggersOutput
    #   resp.data.triggers #=> Array<Trigger>
    #   resp.data.triggers[0] #=> Types::Trigger
    #   resp.data.triggers[0].name #=> String
    #   resp.data.triggers[0].workflow_name #=> String
    #   resp.data.triggers[0].id #=> String
    #   resp.data.triggers[0].type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.triggers[0].state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.triggers[0].description #=> String
    #   resp.data.triggers[0].schedule #=> String
    #   resp.data.triggers[0].actions #=> Array<Action>
    #   resp.data.triggers[0].actions[0] #=> Types::Action
    #   resp.data.triggers[0].actions[0].job_name #=> String
    #   resp.data.triggers[0].actions[0].arguments #=> Hash<String, String>
    #   resp.data.triggers[0].actions[0].arguments['key'] #=> String
    #   resp.data.triggers[0].actions[0].timeout #=> Integer
    #   resp.data.triggers[0].actions[0].security_configuration #=> String
    #   resp.data.triggers[0].actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.triggers[0].actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.triggers[0].actions[0].crawler_name #=> String
    #   resp.data.triggers[0].predicate #=> Types::Predicate
    #   resp.data.triggers[0].predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.triggers[0].predicate.conditions #=> Array<Condition>
    #   resp.data.triggers[0].predicate.conditions[0] #=> Types::Condition
    #   resp.data.triggers[0].predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.triggers[0].predicate.conditions[0].job_name #=> String
    #   resp.data.triggers[0].predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.triggers[0].predicate.conditions[0].crawler_name #=> String
    #   resp.data.triggers[0].predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.triggers[0].event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.triggers[0].event_batching_condition.batch_size #=> Integer
    #   resp.data.triggers[0].event_batching_condition.batch_window #=> Integer
    #   resp.data.triggers_not_found #=> Array<String>
    #   resp.data.triggers_not_found[0] #=> String
    #
    def batch_get_triggers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetTriggersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetTriggersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetTriggers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetTriggers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetTriggers,
        stubs: @stubs,
        params_class: Params::BatchGetTriggersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_triggers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of resource metadata for a given list of workflow names. After calling the <code>ListWorkflows</code> operation, you can call this operation to access the data to which you have been granted permissions. This operation supports all IAM permissions, including permission conditions that uses tags.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchGetWorkflowsInput}.
    #
    # @option params [Array<String>] :names
    #   <p>A list of workflow names, which may be the names returned from the <code>ListWorkflows</code> operation.</p>
    #
    # @option params [Boolean] :include_graph
    #   <p>Specifies whether to include a graph when returning the workflow resource metadata.</p>
    #
    # @return [Types::BatchGetWorkflowsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_get_workflows(
    #     names: [
    #       'member'
    #     ], # required
    #     include_graph: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchGetWorkflowsOutput
    #   resp.data.workflows #=> Array<Workflow>
    #   resp.data.workflows[0] #=> Types::Workflow
    #   resp.data.workflows[0].name #=> String
    #   resp.data.workflows[0].description #=> String
    #   resp.data.workflows[0].default_run_properties #=> Hash<String, String>
    #   resp.data.workflows[0].default_run_properties['key'] #=> String
    #   resp.data.workflows[0].created_on #=> Time
    #   resp.data.workflows[0].last_modified_on #=> Time
    #   resp.data.workflows[0].last_run #=> Types::WorkflowRun
    #   resp.data.workflows[0].last_run.name #=> String
    #   resp.data.workflows[0].last_run.workflow_run_id #=> String
    #   resp.data.workflows[0].last_run.previous_run_id #=> String
    #   resp.data.workflows[0].last_run.workflow_run_properties #=> Hash<String, String>
    #   resp.data.workflows[0].last_run.started_on #=> Time
    #   resp.data.workflows[0].last_run.completed_on #=> Time
    #   resp.data.workflows[0].last_run.status #=> String, one of ["RUNNING", "COMPLETED", "STOPPING", "STOPPED", "ERROR"]
    #   resp.data.workflows[0].last_run.error_message #=> String
    #   resp.data.workflows[0].last_run.statistics #=> Types::WorkflowRunStatistics
    #   resp.data.workflows[0].last_run.statistics.total_actions #=> Integer
    #   resp.data.workflows[0].last_run.statistics.timeout_actions #=> Integer
    #   resp.data.workflows[0].last_run.statistics.failed_actions #=> Integer
    #   resp.data.workflows[0].last_run.statistics.stopped_actions #=> Integer
    #   resp.data.workflows[0].last_run.statistics.succeeded_actions #=> Integer
    #   resp.data.workflows[0].last_run.statistics.running_actions #=> Integer
    #   resp.data.workflows[0].last_run.graph #=> Types::WorkflowGraph
    #   resp.data.workflows[0].last_run.graph.nodes #=> Array<Node>
    #   resp.data.workflows[0].last_run.graph.nodes[0] #=> Types::Node
    #   resp.data.workflows[0].last_run.graph.nodes[0].type #=> String, one of ["CRAWLER", "JOB", "TRIGGER"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].unique_id #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details #=> Types::TriggerNodeDetails
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger #=> Types::Trigger
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.workflow_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.id #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.description #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.schedule #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions #=> Array<Action>
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0] #=> Types::Action
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].job_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].timeout #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].security_configuration #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.actions[0].crawler_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate #=> Types::Predicate
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_window #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details #=> Types::JobNodeDetails
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs #=> Array<JobRun>
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0] #=> Types::JobRun
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].id #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].attempt #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].previous_run_id #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].trigger_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].job_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].started_on #=> Time
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].last_modified_on #=> Time
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].completed_on #=> Time
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].arguments #=> Hash<String, String>
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].error_message #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs #=> Array<Predecessor>
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].job_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].run_id #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].allocated_capacity #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].execution_time #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].timeout #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].max_capacity #=> Float
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].number_of_workers #=> Integer
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].security_configuration #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].log_group_name #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].glue_version #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].job_details.job_runs[0].dpu_seconds #=> Float
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details #=> Types::CrawlerNodeDetails
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls #=> Array<Crawl>
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0] #=> Types::Crawl
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].started_on #=> Time
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].completed_on #=> Time
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].error_message #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].log_group #=> String
    #   resp.data.workflows[0].last_run.graph.nodes[0].crawler_details.crawls[0].log_stream #=> String
    #   resp.data.workflows[0].last_run.graph.edges #=> Array<Edge>
    #   resp.data.workflows[0].last_run.graph.edges[0] #=> Types::Edge
    #   resp.data.workflows[0].last_run.graph.edges[0].source_id #=> String
    #   resp.data.workflows[0].last_run.graph.edges[0].destination_id #=> String
    #   resp.data.workflows[0].last_run.starting_event_batch_condition #=> Types::StartingEventBatchCondition
    #   resp.data.workflows[0].last_run.starting_event_batch_condition.batch_size #=> Integer
    #   resp.data.workflows[0].last_run.starting_event_batch_condition.batch_window #=> Integer
    #   resp.data.workflows[0].graph #=> Types::WorkflowGraph
    #   resp.data.workflows[0].max_concurrent_runs #=> Integer
    #   resp.data.workflows[0].blueprint_details #=> Types::BlueprintDetails
    #   resp.data.workflows[0].blueprint_details.blueprint_name #=> String
    #   resp.data.workflows[0].blueprint_details.run_id #=> String
    #   resp.data.missing_workflows #=> Array<String>
    #   resp.data.missing_workflows[0] #=> String
    #
    def batch_get_workflows(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchGetWorkflowsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchGetWorkflowsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchGetWorkflows
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchGetWorkflows
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchGetWorkflows,
        stubs: @stubs,
        params_class: Params::BatchGetWorkflowsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_get_workflows
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Stops one or more job runs for a specified job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchStopJobRunInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition for which to stop job runs.</p>
    #
    # @option params [Array<String>] :job_run_ids
    #   <p>A list of the <code>JobRunIds</code> that should be stopped for that job
    #         definition.</p>
    #
    # @return [Types::BatchStopJobRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_stop_job_run(
    #     job_name: 'JobName', # required
    #     job_run_ids: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchStopJobRunOutput
    #   resp.data.successful_submissions #=> Array<BatchStopJobRunSuccessfulSubmission>
    #   resp.data.successful_submissions[0] #=> Types::BatchStopJobRunSuccessfulSubmission
    #   resp.data.successful_submissions[0].job_name #=> String
    #   resp.data.successful_submissions[0].job_run_id #=> String
    #   resp.data.errors #=> Array<BatchStopJobRunError>
    #   resp.data.errors[0] #=> Types::BatchStopJobRunError
    #   resp.data.errors[0].job_name #=> String
    #   resp.data.errors[0].job_run_id #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_stop_job_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchStopJobRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchStopJobRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchStopJobRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchStopJobRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchStopJobRun,
        stubs: @stubs,
        params_class: Params::BatchStopJobRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_stop_job_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates one or more partitions in a batch operation.</p>
    #
    # @param [Hash] params
    #   See {Types::BatchUpdatePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the catalog in which the partition is to be updated. Currently, this should be
    #         the Amazon Web Services account ID.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the metadata database in which the partition is
    #         to be updated.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the metadata table in which the partition is to be updated.</p>
    #
    # @option params [Array<BatchUpdatePartitionRequestEntry>] :entries
    #   <p>A list of up to 100 <code>BatchUpdatePartitionRequestEntry</code> objects to update.</p>
    #
    # @return [Types::BatchUpdatePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.batch_update_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     entries: [
    #       {
    #         partition_value_list: [
    #           'member'
    #         ], # required
    #         partition_input: {
    #           values: [
    #             'member'
    #           ],
    #           last_access_time: Time.now,
    #           storage_descriptor: {
    #             columns: [
    #               {
    #                 name: 'Name', # required
    #                 type: 'Type',
    #                 comment: 'Comment',
    #                 parameters: {
    #                   'key' => 'value'
    #                 }
    #               }
    #             ],
    #             location: 'Location',
    #             additional_locations: [
    #               'member'
    #             ],
    #             input_format: 'InputFormat',
    #             output_format: 'OutputFormat',
    #             compressed: false,
    #             number_of_buckets: 1,
    #             serde_info: {
    #               name: 'Name',
    #               serialization_library: 'SerializationLibrary',
    #             },
    #             bucket_columns: [
    #               'member'
    #             ],
    #             sort_columns: [
    #               {
    #                 column: 'Column', # required
    #                 sort_order: 1 # required
    #               }
    #             ],
    #             skewed_info: {
    #               skewed_column_values: [
    #                 'member'
    #               ],
    #               skewed_column_value_location_maps: {
    #                 'key' => 'value'
    #               }
    #             },
    #             stored_as_sub_directories: false,
    #             schema_reference: {
    #               schema_id: {
    #                 schema_arn: 'SchemaArn',
    #                 schema_name: 'SchemaName',
    #                 registry_name: 'RegistryName'
    #               },
    #               schema_version_id: 'SchemaVersionId',
    #               schema_version_number: 1
    #             }
    #           },
    #           last_analyzed_time: Time.now
    #         } # required
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::BatchUpdatePartitionOutput
    #   resp.data.errors #=> Array<BatchUpdatePartitionFailureEntry>
    #   resp.data.errors[0] #=> Types::BatchUpdatePartitionFailureEntry
    #   resp.data.errors[0].partition_value_list #=> Array<String>
    #   resp.data.errors[0].partition_value_list[0] #=> String
    #   resp.data.errors[0].error_detail #=> Types::ErrorDetail
    #   resp.data.errors[0].error_detail.error_code #=> String
    #   resp.data.errors[0].error_detail.error_message #=> String
    #
    def batch_update_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::BatchUpdatePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::BatchUpdatePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::BatchUpdatePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::BatchUpdatePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::BatchUpdatePartition,
        stubs: @stubs,
        params_class: Params::BatchUpdatePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :batch_update_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Cancels (stops) a task run. Machine learning task runs are asynchronous tasks that Glue runs on your behalf as part of various machine learning workflows. You can cancel a
    #       machine learning task run at any time by calling <code>CancelMLTaskRun</code> with a task
    #       run's parent transform's <code>TransformID</code> and the task run's <code>TaskRunId</code>. </p>
    #
    # @param [Hash] params
    #   See {Types::CancelMLTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :task_run_id
    #   <p>A unique identifier for the task run.</p>
    #
    # @return [Types::CancelMLTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.cancel_ml_task_run(
    #     transform_id: 'TransformId', # required
    #     task_run_id: 'TaskRunId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CancelMLTaskRunOutput
    #   resp.data.transform_id #=> String
    #   resp.data.task_run_id #=> String
    #   resp.data.status #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #
    def cancel_ml_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CancelMLTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CancelMLTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CancelMLTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::CancelMLTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CancelMLTaskRun,
        stubs: @stubs,
        params_class: Params::CancelMLTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :cancel_ml_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Cancels the statement..</p>
    #
    # @param [Hash] params
    #   See {Types::CancelStatementInput}.
    #
    # @option params [String] :session_id
    #   <p>The Session ID of the statement to be cancelled.</p>
    #
    # @option params [Integer] :id
    #   <p>The ID of the statement to be cancelled.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request to cancel the statement.</p>
    #
    # @return [Types::CancelStatementOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.cancel_statement(
    #     session_id: 'SessionId', # required
    #     id: 1, # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CancelStatementOutput
    #
    def cancel_statement(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CancelStatementInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CancelStatementInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CancelStatement
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::CancelStatement
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CancelStatement,
        stubs: @stubs,
        params_class: Params::CancelStatementOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :cancel_statement
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Validates the supplied schema. This call has no side effects, it simply validates using the supplied schema using <code>DataFormat</code> as the format. Since it does not take a schema set name, no compatibility checks are performed.</p>
    #
    # @param [Hash] params
    #   See {Types::CheckSchemaVersionValidityInput}.
    #
    # @option params [String] :data_format
    #   <p>The data format of the schema definition. Currently <code>AVRO</code>, <code>JSON</code> and <code>PROTOBUF</code> are supported.</p>
    #
    # @option params [String] :schema_definition
    #   <p>The definition of the schema that has to be validated.</p>
    #
    # @return [Types::CheckSchemaVersionValidityOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.check_schema_version_validity(
    #     data_format: 'AVRO', # required - accepts ["AVRO", "JSON", "PROTOBUF"]
    #     schema_definition: 'SchemaDefinition' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CheckSchemaVersionValidityOutput
    #   resp.data.valid #=> Boolean
    #   resp.data.error #=> String
    #
    def check_schema_version_validity(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CheckSchemaVersionValidityInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CheckSchemaVersionValidityInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CheckSchemaVersionValidity
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::CheckSchemaVersionValidity
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CheckSchemaVersionValidity,
        stubs: @stubs,
        params_class: Params::CheckSchemaVersionValidityOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :check_schema_version_validity
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Registers a blueprint with Glue.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateBlueprintInput}.
    #
    # @option params [String] :name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [String] :description
    #   <p>A description of the blueprint.</p>
    #
    # @option params [String] :blueprint_location
    #   <p>Specifies a path in Amazon S3 where the blueprint is published.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to be applied to this blueprint.</p>
    #
    # @return [Types::CreateBlueprintOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_blueprint(
    #     name: 'Name', # required
    #     description: 'Description',
    #     blueprint_location: 'BlueprintLocation', # required
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateBlueprintOutput
    #   resp.data.name #=> String
    #
    def create_blueprint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateBlueprintInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateBlueprintInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateBlueprint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateBlueprint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateBlueprint,
        stubs: @stubs,
        params_class: Params::CreateBlueprintOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_blueprint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a classifier in the user's account. This can be a <code>GrokClassifier</code>, an
    #         <code>XMLClassifier</code>, a <code>JsonClassifier</code>, or a <code>CsvClassifier</code>,
    #       depending on which field of the request is present.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateClassifierInput}.
    #
    # @option params [CreateGrokClassifierRequest] :grok_classifier
    #   <p>A <code>GrokClassifier</code> object specifying the classifier
    #         to create.</p>
    #
    # @option params [CreateXMLClassifierRequest] :xml_classifier
    #   <p>An <code>XMLClassifier</code> object specifying the classifier
    #         to create.</p>
    #
    # @option params [CreateJsonClassifierRequest] :json_classifier
    #   <p>A <code>JsonClassifier</code> object specifying the classifier
    #         to create.</p>
    #
    # @option params [CreateCsvClassifierRequest] :csv_classifier
    #   <p>A <code>CsvClassifier</code> object specifying the classifier
    #         to create.</p>
    #
    # @return [Types::CreateClassifierOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_classifier(
    #     grok_classifier: {
    #       classification: 'Classification', # required
    #       name: 'Name', # required
    #       grok_pattern: 'GrokPattern', # required
    #       custom_patterns: 'CustomPatterns'
    #     },
    #     xml_classifier: {
    #       classification: 'Classification', # required
    #       name: 'Name', # required
    #       row_tag: 'RowTag'
    #     },
    #     json_classifier: {
    #       name: 'Name', # required
    #       json_path: 'JsonPath' # required
    #     },
    #     csv_classifier: {
    #       name: 'Name', # required
    #       delimiter: 'Delimiter',
    #       quote_symbol: 'QuoteSymbol',
    #       contains_header: 'UNKNOWN', # accepts ["UNKNOWN", "PRESENT", "ABSENT"]
    #       header: [
    #         'member'
    #       ],
    #       disable_value_trimming: false,
    #       allow_single_column: false
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateClassifierOutput
    #
    def create_classifier(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateClassifierInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateClassifierInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateClassifier
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::CreateClassifier
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateClassifier,
        stubs: @stubs,
        params_class: Params::CreateClassifierOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_classifier
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a connection definition in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateConnectionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which to create the connection. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [ConnectionInput] :connection_input
    #   <p>A <code>ConnectionInput</code> object defining the connection
    #         to create.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags you assign to the connection.</p>
    #
    # @return [Types::CreateConnectionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_connection(
    #     catalog_id: 'CatalogId',
    #     connection_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       connection_type: 'JDBC', # required - accepts ["JDBC", "SFTP", "MONGODB", "KAFKA", "NETWORK", "MARKETPLACE", "CUSTOM"]
    #       match_criteria: [
    #         'member'
    #       ],
    #       connection_properties: {
    #         'key' => 'value'
    #       }, # required
    #       physical_connection_requirements: {
    #         subnet_id: 'SubnetId',
    #         security_group_id_list: [
    #           'member'
    #         ],
    #         availability_zone: 'AvailabilityZone'
    #       }
    #     }, # required
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateConnectionOutput
    #
    def create_connection(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateConnectionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateConnectionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateConnection
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateConnection
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateConnection,
        stubs: @stubs,
        params_class: Params::CreateConnectionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_connection
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new crawler with specified targets, role, configuration, and optional schedule.
    #       At least one crawl target must be specified, in the <code>s3Targets</code> field, the
    #         <code>jdbcTargets</code> field, or the <code>DynamoDBTargets</code> field.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>Name of the new crawler.</p>
    #
    # @option params [String] :role
    #   <p>The IAM role or Amazon Resource Name (ARN) of an IAM role used by the new crawler to
    #         access customer resources.</p>
    #
    # @option params [String] :database_name
    #   <p>The Glue database where results are written, such as:
    #           <code>arn:aws:daylight:us-east-1::database/sometable/*</code>.</p>
    #
    # @option params [String] :description
    #   <p>A description of the new crawler.</p>
    #
    # @option params [CrawlerTargets] :targets
    #   <p>A list of collection of targets to crawl.</p>
    #
    # @option params [String] :schedule
    #   <p>A <code>cron</code> expression used to specify the schedule (see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html">Time-Based Schedules for Jobs and Crawlers</a>. For example, to run
    #         something every day at 12:15 UTC, you would specify:
    #         <code>cron(15 12 * * ? *)</code>.</p>
    #
    # @option params [Array<String>] :classifiers
    #   <p>A list of custom classifiers that the user has registered. By default, all built-in
    #         classifiers are included in a crawl, but these custom classifiers always override the default
    #         classifiers for a given classification.</p>
    #
    # @option params [String] :table_prefix
    #   <p>The table prefix used for catalog tables that are created.</p>
    #
    # @option params [SchemaChangePolicy] :schema_change_policy
    #   <p>The policy for the crawler's update and deletion behavior.</p>
    #
    # @option params [RecrawlPolicy] :recrawl_policy
    #   <p>A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.</p>
    #
    # @option params [LineageConfiguration] :lineage_configuration
    #   <p>Specifies data lineage configuration settings for the crawler.</p>
    #
    # @option params [LakeFormationConfiguration] :lake_formation_configuration
    #   <p>Specifies AWS Lake Formation configuration settings for the crawler.</p>
    #
    # @option params [String] :configuration
    #   <p>Crawler configuration information. This versioned JSON
    #         string allows users to specify aspects of a crawler's behavior.
    #         For more information, see <a href="https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html">Configuring a Crawler</a>.</p>
    #
    # @option params [String] :crawler_security_configuration
    #   <p>The name of the <code>SecurityConfiguration</code> structure to be used by this
    #         crawler.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to use with this crawler request. You may use tags to limit access to the
    #               crawler. For more information about tags in Glue, see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a> in the developer
    #               guide.</p>
    #
    # @return [Types::CreateCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_crawler(
    #     name: 'Name', # required
    #     role: 'Role', # required
    #     database_name: 'DatabaseName',
    #     description: 'Description',
    #     targets: {
    #       s3_targets: [
    #         {
    #           path: 'Path',
    #           exclusions: [
    #             'member'
    #           ],
    #           connection_name: 'ConnectionName',
    #           sample_size: 1,
    #           event_queue_arn: 'EventQueueArn',
    #           dlq_event_queue_arn: 'DlqEventQueueArn'
    #         }
    #       ],
    #       jdbc_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           path: 'Path',
    #         }
    #       ],
    #       mongo_db_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           path: 'Path',
    #           scan_all: false
    #         }
    #       ],
    #       dynamo_db_targets: [
    #         {
    #           path: 'Path',
    #           scan_all: false,
    #           scan_rate: 1.0
    #         }
    #       ],
    #       catalog_targets: [
    #         {
    #           database_name: 'DatabaseName', # required
    #           tables: [
    #             'member'
    #           ], # required
    #           connection_name: 'ConnectionName'
    #         }
    #       ],
    #       delta_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           write_manifest: false
    #         }
    #       ]
    #     }, # required
    #     schedule: 'Schedule',
    #     classifiers: [
    #       'member'
    #     ],
    #     table_prefix: 'TablePrefix',
    #     schema_change_policy: {
    #       update_behavior: 'LOG', # accepts ["LOG", "UPDATE_IN_DATABASE"]
    #       delete_behavior: 'LOG' # accepts ["LOG", "DELETE_FROM_DATABASE", "DEPRECATE_IN_DATABASE"]
    #     },
    #     recrawl_policy: {
    #       recrawl_behavior: 'CRAWL_EVERYTHING' # accepts ["CRAWL_EVERYTHING", "CRAWL_NEW_FOLDERS_ONLY", "CRAWL_EVENT_MODE"]
    #     },
    #     lineage_configuration: {
    #       crawler_lineage_settings: 'ENABLE' # accepts ["ENABLE", "DISABLE"]
    #     },
    #     lake_formation_configuration: {
    #       use_lake_formation_credentials: false,
    #       account_id: 'AccountId'
    #     },
    #     configuration: 'Configuration',
    #     crawler_security_configuration: 'CrawlerSecurityConfiguration',
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateCrawlerOutput
    #
    def create_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateCrawler,
        stubs: @stubs,
        params_class: Params::CreateCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a custom pattern that is used to detect sensitive data across the columns and rows of your structured data.</p>
    # 	
    # 	        <p>Each custom pattern you create specifies a regular expression and an optional list of context words. If no context words are passed only a regular expression is checked.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateCustomEntityTypeInput}.
    #
    # @option params [String] :name
    #   <p>A name for the custom pattern that allows it to be retrieved or deleted later. This name must be unique per Amazon Web Services account.</p>
    #
    # @option params [String] :regex_string
    #   <p>A regular expression string that is used for detecting sensitive data in a custom pattern.</p>
    #
    # @option params [Array<String>] :context_words
    #   <p>A list of context words. If none of these context words are found within the vicinity of the regular expression the data will not be detected as sensitive data.</p>
    #   	
    #   	        <p>If no context words are passed only a regular expression is checked.</p>
    #
    # @return [Types::CreateCustomEntityTypeOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_custom_entity_type(
    #     name: 'Name', # required
    #     regex_string: 'RegexString', # required
    #     context_words: [
    #       'member'
    #     ]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateCustomEntityTypeOutput
    #   resp.data.name #=> String
    #
    def create_custom_entity_type(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateCustomEntityTypeInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateCustomEntityTypeInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateCustomEntityType
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateCustomEntityType
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateCustomEntityType,
        stubs: @stubs,
        params_class: Params::CreateCustomEntityTypeOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_custom_entity_type
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new database in a Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateDatabaseInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which to create the database. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [DatabaseInput] :database_input
    #   <p>The metadata for the database.</p>
    #
    # @return [Types::CreateDatabaseOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_database(
    #     catalog_id: 'CatalogId',
    #     database_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       location_uri: 'LocationUri',
    #       parameters: {
    #         'key' => 'value'
    #       },
    #       create_table_default_permissions: [
    #         {
    #           principal: {
    #             data_lake_principal_identifier: 'DataLakePrincipalIdentifier'
    #           },
    #           permissions: [
    #             'ALL' # accepts ["ALL", "SELECT", "ALTER", "DROP", "DELETE", "INSERT", "CREATE_DATABASE", "CREATE_TABLE", "DATA_LOCATION_ACCESS"]
    #           ]
    #         }
    #       ],
    #       target_database: {
    #         catalog_id: 'CatalogId',
    #         database_name: 'DatabaseName'
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateDatabaseOutput
    #
    def create_database(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateDatabaseInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateDatabaseInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateDatabase
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateDatabase
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateDatabase,
        stubs: @stubs,
        params_class: Params::CreateDatabaseOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_database
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new development endpoint.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateDevEndpointInput}.
    #
    # @option params [String] :endpoint_name
    #   <p>The name to be assigned to the new <code>DevEndpoint</code>.</p>
    #
    # @option params [String] :role_arn
    #   <p>The IAM role for the <code>DevEndpoint</code>.</p>
    #
    # @option params [Array<String>] :security_group_ids
    #   <p>Security group IDs for the security groups to be used by the new
    #         <code>DevEndpoint</code>.</p>
    #
    # @option params [String] :subnet_id
    #   <p>The subnet ID for the new <code>DevEndpoint</code> to use.</p>
    #
    # @option params [String] :public_key
    #   <p>The public key to be used by this <code>DevEndpoint</code> for authentication. This
    #         attribute is provided for backward compatibility because the recommended attribute to use is
    #         public keys.</p>
    #
    # @option params [Array<String>] :public_keys
    #   <p>A list of public keys to be used by the development endpoints for authentication. The use
    #         of this attribute is preferred over a single public key because the public keys allow you to
    #         have a different private key per client.</p>
    #
    #            <note>
    #               <p>If you previously created an endpoint with a public key, you must remove that key to be able
    #           to set a list of public keys. Call the <code>UpdateDevEndpoint</code> API with the public
    #           key content in the <code>deletePublicKeys</code> attribute, and the list of new keys in the
    #             <code>addPublicKeys</code> attribute.</p>
    #            </note>
    #
    # @option params [Integer] :number_of_nodes
    #   <p>The number of Glue Data Processing Units (DPUs) to allocate to this
    #           <code>DevEndpoint</code>.</p>
    #
    # @option params [String] :worker_type
    #   <p>The type of predefined worker that is allocated to the development endpoint. Accepts a value of Standard, G.1X, or G.2X.</p>
    #   	        <ul>
    #               <li>
    #                  <p>For the <code>Standard</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 50GB disk, and 2 executors per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.1X</code> worker type, each worker maps to 1 DPU (4 vCPU, 16 GB of memory, 64 GB disk), and provides 1 executor per worker. We recommend this worker type for memory-intensive jobs.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.2X</code> worker type, each worker maps to 2 DPU (8 vCPU, 32 GB of memory, 128 GB disk), and provides 1 executor per worker. We recommend this worker type for memory-intensive jobs.</p>
    #               </li>
    #            </ul>
    #   	        <p>Known issue: when a development endpoint is created with the <code>G.2X</code>
    #               <code>WorkerType</code> configuration, the Spark drivers for the development endpoint will run on 4 vCPU, 16 GB of memory, and a 64 GB disk. </p>
    #
    # @option params [String] :glue_version
    #   <p>Glue version determines the versions of Apache Spark and Python that Glue supports. The Python version indicates the version supported for running your ETL scripts on development endpoints. </p>
    #
    #            <p>For more information about the available Glue versions and corresponding Spark and Python versions, see <a href="https://docs.aws.amazon.com/glue/latest/dg/add-job.html">Glue version</a> in the developer guide.</p>
    #   	
    #   	        <p>Development endpoints that are created without specifying a Glue version default to Glue 0.9.</p>
    #   	
    #   	        <p>You can specify a version of Python support for development endpoints by using the <code>Arguments</code> parameter in the <code>CreateDevEndpoint</code> or <code>UpdateDevEndpoint</code> APIs. If no arguments are provided, the version defaults to Python 2.</p>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers of a defined <code>workerType</code> that are allocated to the development endpoint.</p>
    #   	
    #   	        <p>The maximum number of workers you can define are 299 for <code>G.1X</code>, and 149 for <code>G.2X</code>. </p>
    #
    # @option params [String] :extra_python_libs_s3_path
    #   <p>The paths to one or more Python libraries in an Amazon S3 bucket that should be loaded in
    #         your <code>DevEndpoint</code>. Multiple values must be complete paths separated by a
    #         comma.</p>
    #            <note>
    #               <p>You can only use pure Python libraries with a <code>DevEndpoint</code>. Libraries that rely on
    #           C extensions, such as the <a href="http://pandas.pydata.org/">pandas</a> Python data
    #           analysis library, are not yet supported.</p>
    #            </note>
    #
    # @option params [String] :extra_jars_s3_path
    #   <p>The path to one or more Java <code>.jar</code> files in an S3 bucket that should be loaded
    #         in your <code>DevEndpoint</code>.</p>
    #
    # @option params [String] :security_configuration
    #   <p>The name of the <code>SecurityConfiguration</code> structure to be used with this
    #           <code>DevEndpoint</code>.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to use with this DevEndpoint. You may use tags to limit access to the DevEndpoint. For more information about tags in Glue, see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a> in the developer guide.</p>
    #
    # @option params [Hash<String, String>] :arguments
    #   <p>A map of arguments used to configure the <code>DevEndpoint</code>.</p>
    #
    # @return [Types::CreateDevEndpointOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_dev_endpoint(
    #     endpoint_name: 'EndpointName', # required
    #     role_arn: 'RoleArn', # required
    #     security_group_ids: [
    #       'member'
    #     ],
    #     subnet_id: 'SubnetId',
    #     public_key: 'PublicKey',
    #     public_keys: [
    #       'member'
    #     ],
    #     number_of_nodes: 1,
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     glue_version: 'GlueVersion',
    #     number_of_workers: 1,
    #     extra_python_libs_s3_path: 'ExtraPythonLibsS3Path',
    #     extra_jars_s3_path: 'ExtraJarsS3Path',
    #     security_configuration: 'SecurityConfiguration',
    #     tags: {
    #       'key' => 'value'
    #     },
    #     arguments: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateDevEndpointOutput
    #   resp.data.endpoint_name #=> String
    #   resp.data.status #=> String
    #   resp.data.security_group_ids #=> Array<String>
    #   resp.data.security_group_ids[0] #=> String
    #   resp.data.subnet_id #=> String
    #   resp.data.role_arn #=> String
    #   resp.data.yarn_endpoint_address #=> String
    #   resp.data.zeppelin_remote_spark_interpreter_port #=> Integer
    #   resp.data.number_of_nodes #=> Integer
    #   resp.data.worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.glue_version #=> String
    #   resp.data.number_of_workers #=> Integer
    #   resp.data.availability_zone #=> String
    #   resp.data.vpc_id #=> String
    #   resp.data.extra_python_libs_s3_path #=> String
    #   resp.data.extra_jars_s3_path #=> String
    #   resp.data.failure_reason #=> String
    #   resp.data.security_configuration #=> String
    #   resp.data.created_timestamp #=> Time
    #   resp.data.arguments #=> Hash<String, String>
    #   resp.data.arguments['key'] #=> String
    #
    def create_dev_endpoint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateDevEndpointInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateDevEndpointInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateDevEndpoint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::ValidationException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateDevEndpoint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateDevEndpoint,
        stubs: @stubs,
        params_class: Params::CreateDevEndpointOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_dev_endpoint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateJobInput}.
    #
    # @option params [String] :name
    #   <p>The name you assign to this job definition. It must be unique in your account.</p>
    #
    # @option params [String] :description
    #   <p>Description of the job being defined.</p>
    #
    # @option params [String] :log_uri
    #   <p>This field is reserved for future use.</p>
    #
    # @option params [String] :role
    #   <p>The name or Amazon Resource Name (ARN) of the IAM role associated with this job.</p>
    #
    # @option params [ExecutionProperty] :execution_property
    #   <p>An <code>ExecutionProperty</code> specifying the maximum number of concurrent runs allowed
    #         for this job.</p>
    #
    # @option params [JobCommand] :command
    #   <p>The <code>JobCommand</code> that runs this job.</p>
    #
    # @option params [Hash<String, String>] :default_arguments
    #   <p>The default arguments for this job.</p>
    #            <p>You can specify arguments here that your own job-execution script
    #         consumes, as well as arguments that Glue itself consumes.</p>
    #            <p>For information about how to specify and consume your own Job arguments, see the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-calling.html">Calling Glue APIs in Python</a> topic in the developer guide.</p>
    #            <p>For information about the key-value pairs that Glue consumes to set up your job, see the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html">Special Parameters Used by Glue</a> topic in the developer guide.</p>
    #
    # @option params [Hash<String, String>] :non_overridable_arguments
    #   <p>Non-overridable arguments for this job, specified as name-value pairs.</p>
    #
    # @option params [ConnectionsList] :connections
    #   <p>The connections used for this job.</p>
    #
    # @option params [Integer] :max_retries
    #   <p>The maximum number of times to retry this job if it fails.</p>
    #
    # @option params [Integer] :allocated_capacity
    #   <p>This parameter is deprecated. Use <code>MaxCapacity</code> instead.</p>
    #   	
    #            <p>The number of Glue data processing units (DPUs) to allocate to this Job. You can
    #         allocate from 2 to 100 DPUs; the default is 10. A DPU is a relative measure of processing
    #         power that consists of 4 vCPUs of compute capacity and 16 GB of memory. For more information,
    #         see the <a href="https://aws.amazon.com/glue/pricing/">Glue pricing
    #         page</a>.</p>
    #
    # @option params [Integer] :timeout
    #   <p>The job timeout in minutes.  This is the maximum time that a job run
    #         can consume resources before it is terminated and enters <code>TIMEOUT</code>
    #         status. The default is 2,880 minutes (48 hours).</p>
    #
    # @option params [Float] :max_capacity
    #   <p>For Glue version 1.0 or earlier jobs, using the standard worker type, the number of Glue data processing units (DPUs) that can be allocated when this job runs. A DPU is a relative measure
    #          of processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory.
    #          For more information, see the <a href="https://aws.amazon.com/glue/pricing/">Glue
    #            pricing page</a>.</p>
    #   		
    #   	        <p>Do not set <code>Max Capacity</code> if using <code>WorkerType</code> and <code>NumberOfWorkers</code>.</p>
    #
    #            <p>The value that can be allocated for <code>MaxCapacity</code> depends on whether you are
    #         running a Python shell job or an Apache Spark ETL job:</p>
    #            <ul>
    #               <li>
    #                  <p>When you specify a Python shell job (<code>JobCommand.Name</code>="pythonshell"), you can
    #             allocate either 0.0625 or 1 DPU. The default is 0.0625 DPU.</p>
    #               </li>
    #               <li>
    #                  <p>When you specify an Apache Spark ETL job (<code>JobCommand.Name</code>="glueetl") or Apache
    #            Spark streaming ETL job (<code>JobCommand.Name</code>="gluestreaming"), you can allocate from 2 to 100 DPUs.
    #            The default is 10 DPUs. This job type cannot have a fractional DPU allocation.</p>
    #               </li>
    #            </ul>
    #            <p>For Glue version 2.0 jobs, you cannot instead specify a <code>Maximum capacity</code>. Instead, you should specify a <code>Worker type</code> and the <code>Number of workers</code>.</p>
    #
    # @option params [String] :security_configuration
    #   <p>The name of the <code>SecurityConfiguration</code> structure to be used with this
    #         job.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to use with this job. You may use tags to limit access to the job. For more information about tags in Glue, see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a> in the developer guide.</p>
    #
    # @option params [NotificationProperty] :notification_property
    #   <p>Specifies configuration properties of a job notification.</p>
    #
    # @option params [String] :glue_version
    #   <p>Glue version determines the versions of Apache Spark and Python that Glue supports. The Python version indicates the version supported for jobs of type Spark. </p>
    #
    #            <p>For more information about the available Glue versions and corresponding Spark and Python versions, see <a href="https://docs.aws.amazon.com/glue/latest/dg/add-job.html">Glue version</a> in the developer guide.</p>
    #   	
    #   	        <p>Jobs that are created without specifying a Glue version default to Glue 0.9.</p>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers of a defined <code>workerType</code> that are allocated when a job runs.</p>
    #   	
    #   	        <p>The maximum number of workers you can define are 299 for <code>G.1X</code>, and 149 for <code>G.2X</code>. </p>
    #
    # @option params [String] :worker_type
    #   <p>The type of predefined worker that is allocated when a job runs. Accepts a value of Standard, G.1X, or G.2X.</p>
    #   	        <ul>
    #               <li>
    #                  <p>For the <code>Standard</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 50GB disk, and 2 executors per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.1X</code> worker type, each worker maps to 1 DPU (4 vCPU, 16 GB of memory, 64 GB disk), and provides 1 executor per worker. We recommend this worker type for memory-intensive jobs.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.2X</code> worker type, each worker maps to 2 DPU (8 vCPU, 32 GB of memory, 128 GB disk), and provides 1 executor per worker. We recommend this worker type for memory-intensive jobs.</p>
    #               </li>
    #            </ul>
    #
    # @option params [Hash<String, CodeGenConfigurationNode>] :code_gen_configuration_nodes
    #   <p>The representation of a directed acyclic graph on which both the Glue Studio visual component and Glue Studio code generation is based.</p>
    #
    # @return [Types::CreateJobOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_job(
    #     name: 'Name', # required
    #     description: 'Description',
    #     log_uri: 'LogUri',
    #     role: 'Role', # required
    #     execution_property: {
    #       max_concurrent_runs: 1
    #     },
    #     command: {
    #       name: 'Name',
    #       script_location: 'ScriptLocation',
    #       python_version: 'PythonVersion'
    #     }, # required
    #     default_arguments: {
    #       'key' => 'value'
    #     },
    #     connections: {
    #       connections: [
    #         'member'
    #       ]
    #     },
    #     max_retries: 1,
    #     allocated_capacity: 1,
    #     timeout: 1,
    #     max_capacity: 1.0,
    #     security_configuration: 'SecurityConfiguration',
    #     tags: {
    #       'key' => 'value'
    #     },
    #     notification_property: {
    #       notify_delay_after: 1
    #     },
    #     glue_version: 'GlueVersion',
    #     number_of_workers: 1,
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     code_gen_configuration_nodes: {
    #       'key' => {
    #         athena_connector_source: {
    #           name: 'Name', # required
    #           connection_name: 'ConnectionName', # required
    #           connector_name: 'ConnectorName', # required
    #           connection_type: 'ConnectionType', # required
    #           connection_table: 'ConnectionTable',
    #           schema_name: 'SchemaName', # required
    #           output_schemas: [
    #             {
    #               columns: [
    #                 {
    #                   name: 'Name', # required
    #                   type: 'Type'
    #                 }
    #               ]
    #             }
    #           ]
    #         },
    #         jdbc_connector_source: {
    #           name: 'Name', # required
    #           connection_name: 'ConnectionName', # required
    #           connector_name: 'ConnectorName', # required
    #           connection_type: 'ConnectionType', # required
    #           additional_options: {
    #             filter_predicate: 'FilterPredicate',
    #             partition_column: 'PartitionColumn',
    #             lower_bound: 1,
    #             upper_bound: 1,
    #             num_partitions: 1,
    #             job_bookmark_keys: [
    #               'member'
    #             ],
    #             job_bookmark_keys_sort_order: 'JobBookmarkKeysSortOrder',
    #             data_type_mapping: {
    #               'key' => 'DATE' # accepts ["DATE", "STRING", "TIMESTAMP", "INT", "FLOAT", "LONG", "BIGDECIMAL", "BYTE", "SHORT", "DOUBLE"]
    #             }
    #           },
    #           connection_table: 'ConnectionTable',
    #           query: 'Query',
    #         },
    #         spark_connector_source: {
    #           name: 'Name', # required
    #           connection_name: 'ConnectionName', # required
    #           connector_name: 'ConnectorName', # required
    #           connection_type: 'ConnectionType', # required
    #           additional_options: {
    #             'key' => 'value'
    #           },
    #         },
    #         catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         redshift_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table', # required
    #           redshift_tmp_dir: 'RedshiftTmpDir',
    #           tmp_dir_iam_role: 'TmpDirIAMRole'
    #         },
    #         s3_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table', # required
    #           partition_predicate: 'PartitionPredicate',
    #           additional_options: {
    #             bounded_size: 1,
    #             bounded_files: 1
    #           }
    #         },
    #         s3_csv_source: {
    #           name: 'Name', # required
    #           compression_type: 'gzip', # accepts ["gzip", "bzip2"]
    #           group_size: 'GroupSize',
    #           group_files: 'GroupFiles',
    #           recurse: false,
    #           max_band: 1,
    #           max_files_in_band: 1,
    #           additional_options: {
    #             bounded_size: 1,
    #             bounded_files: 1,
    #             enable_sample_path: false,
    #             sample_path: 'SamplePath'
    #           },
    #           separator: 'comma', # required - accepts ["comma", "ctrla", "pipe", "semicolon", "tab"]
    #           escaper: 'Escaper',
    #           quote_char: 'quote', # required - accepts ["quote", "quillemet", "single_quote", "disabled"]
    #           multiline: false,
    #           with_header: false,
    #           write_header: false,
    #           skip_first: false,
    #           optimize_performance: false,
    #         },
    #         s3_json_source: {
    #           name: 'Name', # required
    #           compression_type: 'gzip', # accepts ["gzip", "bzip2"]
    #           group_size: 'GroupSize',
    #           group_files: 'GroupFiles',
    #           recurse: false,
    #           max_band: 1,
    #           max_files_in_band: 1,
    #           json_path: 'JsonPath',
    #           multiline: false,
    #         },
    #         s3_parquet_source: {
    #           name: 'Name', # required
    #           compression_type: 'snappy', # accepts ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #           group_size: 'GroupSize',
    #           group_files: 'GroupFiles',
    #           recurse: false,
    #           max_band: 1,
    #           max_files_in_band: 1,
    #         },
    #         relational_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         dynamo_db_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         jdbc_connector_target: {
    #           name: 'Name', # required
    #           inputs: [
    #             'member'
    #           ], # required
    #           connection_name: 'ConnectionName', # required
    #           connection_table: 'ConnectionTable', # required
    #           connector_name: 'ConnectorName', # required
    #           connection_type: 'ConnectionType', # required
    #         },
    #         spark_connector_target: {
    #           name: 'Name', # required
    #           connection_name: 'ConnectionName', # required
    #           connector_name: 'ConnectorName', # required
    #           connection_type: 'ConnectionType', # required
    #         },
    #         catalog_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         redshift_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table', # required
    #           redshift_tmp_dir: 'RedshiftTmpDir',
    #           tmp_dir_iam_role: 'TmpDirIAMRole',
    #           upsert_redshift_options: {
    #             table_location: 'TableLocation',
    #             connection_name: 'ConnectionName',
    #             upsert_keys: [
    #               'member'
    #             ]
    #           }
    #         },
    #         s3_catalog_target: {
    #           name: 'Name', # required
    #           table: 'Table', # required
    #           database: 'Database', # required
    #           schema_change_policy: {
    #             enable_update_catalog: false,
    #             update_behavior: 'UPDATE_IN_DATABASE' # accepts ["UPDATE_IN_DATABASE", "LOG"]
    #           }
    #         },
    #         s3_glue_parquet_target: {
    #           name: 'Name', # required
    #           path: 'Path', # required
    #           compression: 'snappy', # accepts ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #           schema_change_policy: {
    #             enable_update_catalog: false,
    #             update_behavior: 'UPDATE_IN_DATABASE', # accepts ["UPDATE_IN_DATABASE", "LOG"]
    #             table: 'Table',
    #             database: 'Database'
    #           }
    #         },
    #         s3_direct_target: {
    #           name: 'Name', # required
    #           path: 'Path', # required
    #           compression: 'Compression',
    #           format: 'json', # required - accepts ["json", "csv", "avro", "orc", "parquet"]
    #         },
    #         apply_mapping: {
    #           name: 'Name', # required
    #           mapping: [
    #             {
    #               to_key: 'ToKey',
    #               from_type: 'FromType',
    #               to_type: 'ToType',
    #               dropped: false,
    #             }
    #           ] # required
    #         },
    #         select_fields: {
    #           name: 'Name', # required
    #         },
    #         drop_fields: {
    #           name: 'Name', # required
    #         },
    #         rename_field: {
    #           name: 'Name', # required
    #         },
    #         spigot: {
    #           name: 'Name', # required
    #           path: 'Path', # required
    #           topk: 1,
    #           prob: 1.0
    #         },
    #         join: {
    #           name: 'Name', # required
    #           inputs: [
    #             'member'
    #           ], # required
    #           join_type: 'equijoin', # required - accepts ["equijoin", "left", "right", "outer", "leftsemi", "leftanti"]
    #           columns: [
    #             {
    #               from: 'From', # required
    #             }
    #           ] # required
    #         },
    #         split_fields: {
    #           name: 'Name', # required
    #         },
    #         select_from_collection: {
    #           name: 'Name', # required
    #           index: 1 # required
    #         },
    #         fill_missing_values: {
    #           name: 'Name', # required
    #           imputed_path: 'ImputedPath', # required
    #           filled_path: 'FilledPath'
    #         },
    #         filter: {
    #           name: 'Name', # required
    #           logical_operator: 'AND', # required - accepts ["AND", "OR"]
    #           filters: [
    #             {
    #               operation: 'EQ', # required - accepts ["EQ", "LT", "GT", "LTE", "GTE", "REGEX", "ISNULL"]
    #               negated: false,
    #               values: [
    #                 {
    #                   type: 'COLUMNEXTRACTED', # required - accepts ["COLUMNEXTRACTED", "CONSTANT"]
    #                 }
    #               ] # required
    #             }
    #           ] # required
    #         },
    #         custom_code: {
    #           name: 'Name', # required
    #           inputs: [
    #             'member'
    #           ], # required
    #           code: 'Code', # required
    #           class_name: 'ClassName', # required
    #         },
    #         spark_sql: {
    #           name: 'Name', # required
    #           sql_query: 'SqlQuery', # required
    #           sql_aliases: [
    #             {
    #               from: 'From', # required
    #               alias: 'Alias' # required
    #             }
    #           ], # required
    #         },
    #         direct_kinesis_source: {
    #           name: 'Name', # required
    #           window_size: 1,
    #           detect_schema: false,
    #           streaming_options: {
    #             endpoint_url: 'EndpointUrl',
    #             stream_name: 'StreamName',
    #             classification: 'Classification',
    #             delimiter: 'Delimiter',
    #             starting_position: 'latest', # accepts ["latest", "trim_horizon", "earliest"]
    #             max_fetch_time_in_ms: 1,
    #             max_fetch_records_per_shard: 1,
    #             max_record_per_read: 1,
    #             add_idle_time_between_reads: false,
    #             idle_time_between_reads_in_ms: 1,
    #             describe_shard_interval: 1,
    #             num_retries: 1,
    #             retry_interval_ms: 1,
    #             max_retry_interval_ms: 1,
    #             avoid_empty_batches: false,
    #             stream_arn: 'StreamArn',
    #             role_arn: 'RoleArn',
    #             role_session_name: 'RoleSessionName'
    #           },
    #           data_preview_options: {
    #             polling_time: 1,
    #             record_polling_limit: 1
    #           }
    #         },
    #         direct_kafka_source: {
    #           name: 'Name', # required
    #           streaming_options: {
    #             bootstrap_servers: 'BootstrapServers',
    #             security_protocol: 'SecurityProtocol',
    #             connection_name: 'ConnectionName',
    #             topic_name: 'TopicName',
    #             assign: 'Assign',
    #             subscribe_pattern: 'SubscribePattern',
    #             classification: 'Classification',
    #             delimiter: 'Delimiter',
    #             starting_offsets: 'StartingOffsets',
    #             ending_offsets: 'EndingOffsets',
    #             poll_timeout_ms: 1,
    #             num_retries: 1,
    #             retry_interval_ms: 1,
    #             max_offsets_per_trigger: 1,
    #             min_partitions: 1
    #           },
    #           window_size: 1,
    #           detect_schema: false,
    #         },
    #         catalog_kinesis_source: {
    #           name: 'Name', # required
    #           window_size: 1,
    #           detect_schema: false,
    #           table: 'Table', # required
    #           database: 'Database', # required
    #         },
    #         catalog_kafka_source: {
    #           name: 'Name', # required
    #           window_size: 1,
    #           detect_schema: false,
    #           table: 'Table', # required
    #           database: 'Database', # required
    #         },
    #         drop_null_fields: {
    #           name: 'Name', # required
    #           null_check_box_list: {
    #             is_empty: false,
    #             is_null_string: false,
    #             is_neg_one: false
    #           },
    #           null_text_list: [
    #             {
    #               value: 'Value', # required
    #               datatype: {
    #                 id: 'Id', # required
    #                 label: 'Label' # required
    #               } # required
    #             }
    #           ]
    #         },
    #         merge: {
    #           name: 'Name', # required
    #           source: 'Source', # required
    #         },
    #         union: {
    #           name: 'Name', # required
    #           union_type: 'ALL' # required - accepts ["ALL", "DISTINCT"]
    #         },
    #         pii_detection: {
    #           name: 'Name', # required
    #           pii_type: 'RowAudit', # required - accepts ["RowAudit", "RowMasking", "ColumnAudit", "ColumnMasking"]
    #           output_column_name: 'OutputColumnName',
    #           sample_fraction: 1.0,
    #           threshold_fraction: 1.0,
    #           mask_value: 'MaskValue'
    #         },
    #         aggregate: {
    #           name: 'Name', # required
    #           aggs: [
    #             {
    #               agg_func: 'avg' # required - accepts ["avg", "countDistinct", "count", "first", "last", "kurtosis", "max", "min", "skewness", "stddev_samp", "stddev_pop", "sum", "sumDistinct", "var_samp", "var_pop"]
    #             }
    #           ] # required
    #         },
    #         drop_duplicates: {
    #           name: 'Name', # required
    #           columns: [
    #             [
    #               'member'
    #             ]
    #           ]
    #         },
    #         governed_catalog_target: {
    #           name: 'Name', # required
    #           table: 'Table', # required
    #           database: 'Database', # required
    #         },
    #         governed_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table', # required
    #           partition_predicate: 'PartitionPredicate',
    #         },
    #         microsoft_sql_server_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         my_sql_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         oracle_sql_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         postgre_sql_catalog_source: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         microsoft_sql_server_catalog_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         my_sql_catalog_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         oracle_sql_catalog_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         },
    #         postgre_sql_catalog_target: {
    #           name: 'Name', # required
    #           database: 'Database', # required
    #           table: 'Table' # required
    #         }
    #       }
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateJobOutput
    #   resp.data.name #=> String
    #
    def create_job(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateJobInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateJobInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateJob
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateJob
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateJob,
        stubs: @stubs,
        params_class: Params::CreateJobOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_job
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates an Glue machine learning transform. This operation creates the transform and
    #       all the necessary parameters to train it.</p>
    # 	
    # 	        <p>Call this operation as the first step in the process of using a machine learning transform
    #       (such as the <code>FindMatches</code> transform) for deduplicating data. You can provide an
    #       optional <code>Description</code>, in addition to the parameters that you want to use for your
    #       algorithm.</p>
    #
    #          <p>You must also specify certain parameters for the tasks that Glue runs on your
    #       behalf as part of learning from your data and creating a high-quality machine learning
    #       transform. These parameters include <code>Role</code>, and optionally,
    #         <code>AllocatedCapacity</code>, <code>Timeout</code>, and <code>MaxRetries</code>. For more
    #       information, see <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-jobs-job.html">Jobs</a>.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateMLTransformInput}.
    #
    # @option params [String] :name
    #   <p>The unique name that you give the transform when you create it.</p>
    #
    # @option params [String] :description
    #   <p>A description of the machine learning transform that is being defined. The default is an
    #         empty string.</p>
    #
    # @option params [Array<GlueTable>] :input_record_tables
    #   <p>A list of Glue table definitions used by the transform.</p>
    #
    # @option params [TransformParameters] :parameters
    #   <p>The algorithmic parameters that are specific to the transform type used. Conditionally
    #         dependent on the transform type.</p>
    #
    # @option params [String] :role
    #   <p>The name or Amazon Resource Name (ARN) of the IAM role with the required permissions. The required permissions include both Glue service role permissions to Glue resources, and Amazon S3 permissions required by the transform. </p>
    #
    #   		       <ul>
    #               <li>
    #                  <p>This role needs Glue service role permissions to allow access to resources in Glue. See <a href="https://docs.aws.amazon.com/glue/latest/dg/attach-policy-iam-user.html">Attach a Policy to IAM Users That Access Glue</a>.</p>
    #               </li>
    #               <li>
    #                  <p>This role needs permission to your Amazon Simple Storage Service (Amazon S3) sources, targets, temporary directory, scripts, and any libraries used by the task run for this transform.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :glue_version
    #   <p>This value determines which version of Glue this machine learning transform is compatible with. Glue 1.0 is recommended for most customers. If the value is not set, the Glue compatibility defaults to Glue 0.9.  For more information, see <a href="https://docs.aws.amazon.com/glue/latest/dg/release-notes.html#release-notes-versions">Glue Versions</a> in the developer guide.</p>
    #
    # @option params [Float] :max_capacity
    #   <p>The number of Glue data processing units (DPUs) that are allocated to task runs for this transform. You can allocate from 2 to 100 DPUs; the default is 10. A DPU is a relative measure of
    #         processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory. For more
    #         information, see the <a href="https://aws.amazon.com/glue/pricing/">Glue pricing
    #           page</a>. </p>
    #   		
    #   			      <p>
    #               <code>MaxCapacity</code> is a mutually exclusive option with <code>NumberOfWorkers</code> and <code>WorkerType</code>.</p>
    #           <ul>
    #               <li>
    #                  <p>If either <code>NumberOfWorkers</code> or <code>WorkerType</code> is set, then <code>MaxCapacity</code> cannot be set.</p>
    #               </li>
    #               <li>
    #                  <p>If <code>MaxCapacity</code> is set then neither <code>NumberOfWorkers</code> or <code>WorkerType</code> can be set.</p>
    #               </li>
    #               <li>
    #                  <p>If <code>WorkerType</code> is set, then <code>NumberOfWorkers</code> is required (and vice versa).</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>MaxCapacity</code> and <code>NumberOfWorkers</code> must both be at least 1.</p>
    #               </li>
    #            </ul>
    #   		
    #   	        <p>When the <code>WorkerType</code> field is set to a value other than <code>Standard</code>, the <code>MaxCapacity</code> field is set automatically and becomes read-only.</p>	
    #
    #   		
    #            <p>When the <code>WorkerType</code> field is set to a value other than <code>Standard</code>, the <code>MaxCapacity</code> field is set automatically and becomes read-only.</p>
    #
    # @option params [String] :worker_type
    #   <p>The type of predefined worker that is allocated when this task runs. Accepts a value of Standard, G.1X, or G.2X.</p>
    #   	        <ul>
    #               <li>
    #                  <p>For the <code>Standard</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 50GB disk, and 2 executors per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.1X</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 64GB disk, and 1 executor per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.2X</code> worker type, each worker provides 8 vCPU, 32 GB of memory and a 128GB disk, and 1 executor per worker.</p>
    #               </li>
    #            </ul>
    #   	
    #   	        <p>
    #               <code>MaxCapacity</code> is a mutually exclusive option with <code>NumberOfWorkers</code> and <code>WorkerType</code>.</p>
    #           <ul>
    #               <li>
    #                  <p>If either <code>NumberOfWorkers</code> or <code>WorkerType</code> is set, then <code>MaxCapacity</code> cannot be set.</p>
    #               </li>
    #               <li>
    #                  <p>If <code>MaxCapacity</code> is set then neither <code>NumberOfWorkers</code> or <code>WorkerType</code> can be set.</p>
    #               </li>
    #               <li>
    #                  <p>If <code>WorkerType</code> is set, then <code>NumberOfWorkers</code> is required (and vice versa).</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>MaxCapacity</code> and <code>NumberOfWorkers</code> must both be at least 1.</p>
    #               </li>
    #            </ul>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers of a defined <code>workerType</code> that are allocated when this task runs.</p>
    #   	
    #   		       <p>If <code>WorkerType</code> is set, then <code>NumberOfWorkers</code> is required (and vice versa).</p>
    #
    # @option params [Integer] :timeout
    #   <p>The timeout of the task run for this transform in minutes. This is the maximum time that a task run for this transform can consume resources before it is terminated and enters <code>TIMEOUT</code> status. The default is 2,880 minutes (48 hours).</p>
    #
    # @option params [Integer] :max_retries
    #   <p>The maximum number of times to retry a task for this transform after a task run fails.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to use with this machine learning transform. You may use tags to limit access to the machine learning transform. For more information about tags in Glue, see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a> in the developer guide.</p>
    #
    # @option params [TransformEncryption] :transform_encryption
    #   <p>The encryption-at-rest settings of the transform that apply to accessing user data. Machine learning transforms can access user data encrypted in Amazon S3 using KMS.</p>
    #
    # @return [Types::CreateMLTransformOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_ml_transform(
    #     name: 'Name', # required
    #     description: 'Description',
    #     input_record_tables: [
    #       {
    #         database_name: 'DatabaseName', # required
    #         table_name: 'TableName', # required
    #         catalog_id: 'CatalogId',
    #         connection_name: 'ConnectionName'
    #       }
    #     ], # required
    #     parameters: {
    #       transform_type: 'FIND_MATCHES', # required - accepts ["FIND_MATCHES"]
    #       find_matches_parameters: {
    #         primary_key_column_name: 'PrimaryKeyColumnName',
    #         precision_recall_tradeoff: 1.0,
    #         accuracy_cost_tradeoff: 1.0,
    #         enforce_provided_labels: false
    #       }
    #     }, # required
    #     role: 'Role', # required
    #     glue_version: 'GlueVersion',
    #     max_capacity: 1.0,
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     number_of_workers: 1,
    #     timeout: 1,
    #     max_retries: 1,
    #     tags: {
    #       'key' => 'value'
    #     },
    #     transform_encryption: {
    #       ml_user_data_encryption: {
    #         ml_user_data_encryption_mode: 'DISABLED', # required - accepts ["DISABLED", "SSE-KMS"]
    #         kms_key_id: 'KmsKeyId'
    #       },
    #       task_run_security_configuration_name: 'TaskRunSecurityConfigurationName'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateMLTransformOutput
    #   resp.data.transform_id #=> String
    #
    def create_ml_transform(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateMLTransformInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateMLTransformInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateMLTransform
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateMLTransform
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateMLTransform,
        stubs: @stubs,
        params_class: Params::CreateMLTransformOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_ml_transform
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new partition.</p>
    #
    # @param [Hash] params
    #   See {Types::CreatePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The Amazon Web Services account ID of the catalog in which the partition is to be created.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the metadata database in which the partition is
    #         to be created.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the metadata table in which the partition is to be created.</p>
    #
    # @option params [PartitionInput] :partition_input
    #   <p>A <code>PartitionInput</code> structure defining the partition
    #         to be created.</p>
    #
    # @return [Types::CreatePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_input: {
    #       values: [
    #         'member'
    #       ],
    #       last_access_time: Time.now,
    #       storage_descriptor: {
    #         columns: [
    #           {
    #             name: 'Name', # required
    #             type: 'Type',
    #             comment: 'Comment',
    #             parameters: {
    #               'key' => 'value'
    #             }
    #           }
    #         ],
    #         location: 'Location',
    #         additional_locations: [
    #           'member'
    #         ],
    #         input_format: 'InputFormat',
    #         output_format: 'OutputFormat',
    #         compressed: false,
    #         number_of_buckets: 1,
    #         serde_info: {
    #           name: 'Name',
    #           serialization_library: 'SerializationLibrary',
    #         },
    #         bucket_columns: [
    #           'member'
    #         ],
    #         sort_columns: [
    #           {
    #             column: 'Column', # required
    #             sort_order: 1 # required
    #           }
    #         ],
    #         skewed_info: {
    #           skewed_column_values: [
    #             'member'
    #           ],
    #           skewed_column_value_location_maps: {
    #             'key' => 'value'
    #           }
    #         },
    #         stored_as_sub_directories: false,
    #         schema_reference: {
    #           schema_id: {
    #             schema_arn: 'SchemaArn',
    #             schema_name: 'SchemaName',
    #             registry_name: 'RegistryName'
    #           },
    #           schema_version_id: 'SchemaVersionId',
    #           schema_version_number: 1
    #         }
    #       },
    #       last_analyzed_time: Time.now
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreatePartitionOutput
    #
    def create_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreatePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreatePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreatePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreatePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreatePartition,
        stubs: @stubs,
        params_class: Params::CreatePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a specified partition index in an existing table.</p>
    #
    # @param [Hash] params
    #   See {Types::CreatePartitionIndexInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The catalog ID where the table resides.</p>
    #
    # @option params [String] :database_name
    #   <p>Specifies the name of a database in which you want to create a partition index.</p>
    #
    # @option params [String] :table_name
    #   <p>Specifies the name of a table in which you want to create a partition index.</p>
    #
    # @option params [PartitionIndex] :partition_index
    #   <p>Specifies a <code>PartitionIndex</code> structure to create a partition index in an existing table.</p>
    #
    # @return [Types::CreatePartitionIndexOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_partition_index(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_index: {
    #       keys: [
    #         'member'
    #       ], # required
    #       index_name: 'IndexName' # required
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreatePartitionIndexOutput
    #
    def create_partition_index(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreatePartitionIndexInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreatePartitionIndexInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreatePartitionIndex
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreatePartitionIndex
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreatePartitionIndex,
        stubs: @stubs,
        params_class: Params::CreatePartitionIndexOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_partition_index
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new registry which may be used to hold a collection of schemas.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateRegistryInput}.
    #
    # @option params [String] :registry_name
    #   <p>Name of the registry to be created of max length of 255, and may only contain letters, numbers, hyphen, underscore, dollar sign, or hash mark.  No whitespace.</p>
    #
    # @option params [String] :description
    #   <p>A description of the registry. If description is not provided, there will not be any default value for this.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Amazon Web Services tags that contain a key value pair and may be searched by console, command line, or API.</p>
    #
    # @return [Types::CreateRegistryOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_registry(
    #     registry_name: 'RegistryName', # required
    #     description: 'Description',
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateRegistryOutput
    #   resp.data.registry_arn #=> String
    #   resp.data.registry_name #=> String
    #   resp.data.description #=> String
    #   resp.data.tags #=> Hash<String, String>
    #   resp.data.tags['key'] #=> String
    #
    def create_registry(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateRegistryInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateRegistryInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateRegistry
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException]),
        data_parser: Parsers::CreateRegistry
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateRegistry,
        stubs: @stubs,
        params_class: Params::CreateRegistryOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_registry
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new schema set and registers the schema definition. Returns an error if the schema set already exists without actually registering the version.</p>
    #          <p>When the schema set is created, a version checkpoint will be set to the first version. Compatibility mode "DISABLED" restricts any additional schema versions from being added after the first schema version. For all other compatibility modes, validation of compatibility settings will be applied only from the second version onwards when the <code>RegisterSchemaVersion</code> API is used.</p>
    #          <p>When this API is called without a <code>RegistryId</code>, this will create an entry for a "default-registry" in the registry database tables, if it is not already present.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateSchemaInput}.
    #
    # @option params [RegistryId] :registry_id
    #   <p> This is a wrapper shape to contain the registry identity fields. If this is not provided, the default registry will be used. The ARN format for the same will be: <code>arn:aws:glue:us-east-2:<customer id>:registry/default-registry:random-5-letter-id</code>.</p>
    #
    # @option params [String] :schema_name
    #   <p>Name of the schema to be created of max length of 255, and may only contain letters, numbers, hyphen, underscore, dollar sign, or hash mark. No whitespace.</p>
    #
    # @option params [String] :data_format
    #   <p>The data format of the schema definition. Currently <code>AVRO</code>, <code>JSON</code> and <code>PROTOBUF</code> are supported.</p>
    #
    # @option params [String] :compatibility
    #   <p>The compatibility mode of the schema. The possible values are:</p>
    #   	
    #            <ul>
    #               <li>
    #                  <p>
    #                     <i>NONE</i>: No compatibility mode applies. You can use this choice in development scenarios or if you do not know the compatibility mode that you want to apply to schemas. Any new version added will be accepted without undergoing a compatibility check.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>DISABLED</i>: This compatibility choice prevents versioning for a particular schema. You can use this choice to prevent future versioning of a schema.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>BACKWARD</i>: This compatibility choice is recommended as it allows data receivers to read both the current and one previous schema version. This means that for instance, a new schema version cannot drop data fields or change the type of these fields, so they can't be read by readers using the previous version.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>BACKWARD_ALL</i>: This compatibility choice allows data receivers to read both the current and all previous schema versions. You can use this choice when you need to delete fields or add optional fields, and check compatibility against all previous schema versions. </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>FORWARD</i>: This compatibility choice allows data receivers to read both the current and one next schema version, but not necessarily later versions. You can use this choice when you need to add fields or delete optional fields, but only check compatibility against the last schema version.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>FORWARD_ALL</i>: This compatibility choice allows data receivers to read written by producers of any new registered schema. You can use this choice when you need to add fields or delete optional fields, and check compatibility against all previous schema versions.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>FULL</i>: This compatibility choice allows data receivers to read data written by producers using the previous or next version of the schema, but not necessarily earlier or later versions. You can use this choice when you need to add or remove optional fields, but only check compatibility against the last schema version.</p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <i>FULL_ALL</i>: This compatibility choice allows data receivers to read data written by producers using all previous schema versions. You can use this choice when you need to add or remove optional fields, and check compatibility against all previous schema versions.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :description
    #   <p>An optional description of the schema. If description is not provided, there will not be any automatic default value for this.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Amazon Web Services tags that contain a key value pair and may be searched by console, command line, or API. If specified, follows the Amazon Web Services tags-on-create pattern.</p>
    #
    # @option params [String] :schema_definition
    #   <p>The schema definition using the <code>DataFormat</code> setting for <code>SchemaName</code>.</p>
    #
    # @return [Types::CreateSchemaOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_schema(
    #     registry_id: {
    #       registry_name: 'RegistryName',
    #       registry_arn: 'RegistryArn'
    #     },
    #     schema_name: 'SchemaName', # required
    #     data_format: 'AVRO', # required - accepts ["AVRO", "JSON", "PROTOBUF"]
    #     compatibility: 'NONE', # accepts ["NONE", "DISABLED", "BACKWARD", "BACKWARD_ALL", "FORWARD", "FORWARD_ALL", "FULL", "FULL_ALL"]
    #     description: 'Description',
    #     tags: {
    #       'key' => 'value'
    #     },
    #     schema_definition: 'SchemaDefinition'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateSchemaOutput
    #   resp.data.registry_name #=> String
    #   resp.data.registry_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.schema_arn #=> String
    #   resp.data.description #=> String
    #   resp.data.data_format #=> String, one of ["AVRO", "JSON", "PROTOBUF"]
    #   resp.data.compatibility #=> String, one of ["NONE", "DISABLED", "BACKWARD", "BACKWARD_ALL", "FORWARD", "FORWARD_ALL", "FULL", "FULL_ALL"]
    #   resp.data.schema_checkpoint #=> Integer
    #   resp.data.latest_schema_version #=> Integer
    #   resp.data.next_schema_version #=> Integer
    #   resp.data.schema_status #=> String, one of ["AVAILABLE", "PENDING", "DELETING"]
    #   resp.data.tags #=> Hash<String, String>
    #   resp.data.tags['key'] #=> String
    #   resp.data.schema_version_id #=> String
    #   resp.data.schema_version_status #=> String, one of ["AVAILABLE", "PENDING", "FAILURE", "DELETING"]
    #
    def create_schema(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateSchemaInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateSchemaInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateSchema
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException]),
        data_parser: Parsers::CreateSchema
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateSchema,
        stubs: @stubs,
        params_class: Params::CreateSchemaOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_schema
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Transforms a directed acyclic graph (DAG) into code.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateScriptInput}.
    #
    # @option params [Array<CodeGenNode>] :dag_nodes
    #   <p>A list of the nodes in the DAG.</p>
    #
    # @option params [Array<CodeGenEdge>] :dag_edges
    #   <p>A list of the edges in the DAG.</p>
    #
    # @option params [String] :language
    #   <p>The programming language of the resulting code from the DAG.</p>
    #
    # @return [Types::CreateScriptOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_script(
    #     dag_nodes: [
    #       {
    #         id: 'Id', # required
    #         node_type: 'NodeType', # required
    #         args: [
    #           {
    #             name: 'Name', # required
    #             value: 'Value', # required
    #             param: false
    #           }
    #         ], # required
    #         line_number: 1
    #       }
    #     ],
    #     dag_edges: [
    #       {
    #         source: 'Source', # required
    #         target: 'Target', # required
    #         target_parameter: 'TargetParameter'
    #       }
    #     ],
    #     language: 'PYTHON' # accepts ["PYTHON", "SCALA"]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateScriptOutput
    #   resp.data.python_script #=> String
    #   resp.data.scala_code #=> String
    #
    def create_script(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateScriptInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateScriptInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateScript
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::CreateScript
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateScript,
        stubs: @stubs,
        params_class: Params::CreateScriptOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_script
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new security configuration. A security configuration is a set of security properties that can be used by Glue. You can use a security configuration to encrypt data at rest. For information about using security configurations in Glue, see <a href="https://docs.aws.amazon.com/glue/latest/dg/encryption-security-configuration.html">Encrypting Data Written by Crawlers, Jobs, and Development Endpoints</a>.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateSecurityConfigurationInput}.
    #
    # @option params [String] :name
    #   <p>The name for the new security configuration.</p>
    #
    # @option params [EncryptionConfiguration] :encryption_configuration
    #   <p>The encryption configuration for the new security configuration.</p>
    #
    # @return [Types::CreateSecurityConfigurationOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_security_configuration(
    #     name: 'Name', # required
    #     encryption_configuration: {
    #       s3_encryption: [
    #         {
    #           s3_encryption_mode: 'DISABLED', # accepts ["DISABLED", "SSE-KMS", "SSE-S3"]
    #           kms_key_arn: 'KmsKeyArn'
    #         }
    #       ],
    #       cloud_watch_encryption: {
    #         cloud_watch_encryption_mode: 'DISABLED', # accepts ["DISABLED", "SSE-KMS"]
    #         kms_key_arn: 'KmsKeyArn'
    #       },
    #       job_bookmarks_encryption: {
    #         job_bookmarks_encryption_mode: 'DISABLED', # accepts ["DISABLED", "CSE-KMS"]
    #         kms_key_arn: 'KmsKeyArn'
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateSecurityConfigurationOutput
    #   resp.data.name #=> String
    #   resp.data.created_timestamp #=> Time
    #
    def create_security_configuration(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateSecurityConfigurationInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateSecurityConfigurationInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateSecurityConfiguration
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateSecurityConfiguration
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateSecurityConfiguration,
        stubs: @stubs,
        params_class: Params::CreateSecurityConfigurationOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_security_configuration
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new session.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateSessionInput}.
    #
    # @option params [String] :id
    #   <p>The ID of the session request. </p>
    #
    # @option params [String] :description
    #   <p>The description of the session. </p>
    #
    # @option params [String] :role
    #   <p>The IAM Role ARN </p>
    #
    # @option params [SessionCommand] :command
    #   <p>The <code>SessionCommand</code> that runs the job. </p>
    #
    # @option params [Integer] :timeout
    #   <p>The number of seconds before request times out. </p>
    #
    # @option params [Integer] :idle_timeout
    #   <p>The number of seconds when idle before request times out. </p>
    #
    # @option params [Hash<String, String>] :default_arguments
    #   <p>A map array of key-value pairs. Max is 75 pairs. </p>
    #
    # @option params [ConnectionsList] :connections
    #   <p>The number of connections to use for the session. </p>
    #
    # @option params [Float] :max_capacity
    #   <p>The number of AWS Glue data processing units (DPUs) that can be allocated when the job runs.
    #         A DPU is a relative measure of processing power that consists of 4 vCPUs of compute capacity and 16 GB memory. </p>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers to use for the session. </p>
    #
    # @option params [String] :worker_type
    #   <p>The Worker Type. Can be one of G.1X, G.2X, Standard </p>
    #
    # @option params [String] :security_configuration
    #   <p>The name of the SecurityConfiguration structure to be used with the session </p>
    #
    # @option params [String] :glue_version
    #   <p>The Glue version determines the versions of Apache Spark and Python that AWS Glue supports.
    #         The GlueVersion must be greater than 2.0. </p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The map of key value pairs (tags) belonging to the session.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request. </p>
    #
    # @return [Types::CreateSessionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_session(
    #     id: 'Id', # required
    #     description: 'Description',
    #     role: 'Role', # required
    #     command: {
    #       name: 'Name',
    #       python_version: 'PythonVersion'
    #     }, # required
    #     timeout: 1,
    #     idle_timeout: 1,
    #     default_arguments: {
    #       'key' => 'value'
    #     },
    #     connections: {
    #       connections: [
    #         'member'
    #       ]
    #     },
    #     max_capacity: 1.0,
    #     number_of_workers: 1,
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     security_configuration: 'SecurityConfiguration',
    #     glue_version: 'GlueVersion',
    #     tags: {
    #       'key' => 'value'
    #     },
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateSessionOutput
    #   resp.data.session #=> Types::Session
    #   resp.data.session.id #=> String
    #   resp.data.session.created_on #=> Time
    #   resp.data.session.status #=> String, one of ["PROVISIONING", "READY", "FAILED", "TIMEOUT", "STOPPING", "STOPPED"]
    #   resp.data.session.error_message #=> String
    #   resp.data.session.description #=> String
    #   resp.data.session.role #=> String
    #   resp.data.session.command #=> Types::SessionCommand
    #   resp.data.session.command.name #=> String
    #   resp.data.session.command.python_version #=> String
    #   resp.data.session.default_arguments #=> Hash<String, String>
    #   resp.data.session.default_arguments['key'] #=> String
    #   resp.data.session.connections #=> Types::ConnectionsList
    #   resp.data.session.connections.connections #=> Array<String>
    #   resp.data.session.connections.connections[0] #=> String
    #   resp.data.session.progress #=> Float
    #   resp.data.session.max_capacity #=> Float
    #   resp.data.session.security_configuration #=> String
    #   resp.data.session.glue_version #=> String
    #
    def create_session(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateSessionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateSessionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateSession
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::ValidationException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateSession
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateSession,
        stubs: @stubs,
        params_class: Params::CreateSessionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_session
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new table definition in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which to create the <code>Table</code>.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The catalog database in which to create the new table. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [TableInput] :table_input
    #   <p>The <code>TableInput</code> object that defines the metadata table
    #         to create in the catalog.</p>
    #
    # @option params [Array<PartitionIndex>] :partition_indexes
    #   <p>A list of partition indexes, <code>PartitionIndex</code> structures, to create in the table.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The ID of the transaction.</p>
    #
    # @return [Types::CreateTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       owner: 'Owner',
    #       last_access_time: Time.now,
    #       last_analyzed_time: Time.now,
    #       retention: 1,
    #       storage_descriptor: {
    #         columns: [
    #           {
    #             name: 'Name', # required
    #             type: 'Type',
    #             comment: 'Comment',
    #             parameters: {
    #               'key' => 'value'
    #             }
    #           }
    #         ],
    #         location: 'Location',
    #         additional_locations: [
    #           'member'
    #         ],
    #         input_format: 'InputFormat',
    #         output_format: 'OutputFormat',
    #         compressed: false,
    #         number_of_buckets: 1,
    #         serde_info: {
    #           name: 'Name',
    #           serialization_library: 'SerializationLibrary',
    #         },
    #         bucket_columns: [
    #           'member'
    #         ],
    #         sort_columns: [
    #           {
    #             column: 'Column', # required
    #             sort_order: 1 # required
    #           }
    #         ],
    #         skewed_info: {
    #           skewed_column_values: [
    #             'member'
    #           ],
    #           skewed_column_value_location_maps: {
    #             'key' => 'value'
    #           }
    #         },
    #         stored_as_sub_directories: false,
    #         schema_reference: {
    #           schema_id: {
    #             schema_arn: 'SchemaArn',
    #             schema_name: 'SchemaName',
    #             registry_name: 'RegistryName'
    #           },
    #           schema_version_id: 'SchemaVersionId',
    #           schema_version_number: 1
    #         }
    #       },
    #       view_original_text: 'ViewOriginalText',
    #       view_expanded_text: 'ViewExpandedText',
    #       table_type: 'TableType',
    #       target_table: {
    #         catalog_id: 'CatalogId',
    #         database_name: 'DatabaseName',
    #         name: 'Name'
    #       }
    #     }, # required
    #     partition_indexes: [
    #       {
    #         keys: [
    #           'member'
    #         ], # required
    #         index_name: 'IndexName' # required
    #       }
    #     ],
    #     transaction_id: 'TransactionId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateTableOutput
    #
    def create_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateTable,
        stubs: @stubs,
        params_class: Params::CreateTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new trigger.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger.</p>
    #
    # @option params [String] :workflow_name
    #   <p>The name of the workflow associated with the trigger.</p>
    #
    # @option params [String] :type
    #   <p>The type of the new trigger.</p>
    #
    # @option params [String] :schedule
    #   <p>A <code>cron</code> expression used to specify the schedule (see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html">Time-Based Schedules for Jobs and Crawlers</a>. For example, to run
    #         something every day at 12:15 UTC, you would specify:
    #         <code>cron(15 12 * * ? *)</code>.</p>
    #            <p>This field is required when the trigger type is SCHEDULED.</p>
    #
    # @option params [Predicate] :predicate
    #   <p>A predicate to specify when the new trigger should fire.</p>
    #            <p>This field is required when the trigger type is <code>CONDITIONAL</code>.</p>
    #
    # @option params [Array<Action>] :actions
    #   <p>The actions initiated by this trigger when it fires.</p>
    #
    # @option params [String] :description
    #   <p>A description of the new trigger.</p>
    #
    # @option params [Boolean] :start_on_creation
    #   <p>Set to <code>true</code> to start <code>SCHEDULED</code> and <code>CONDITIONAL</code>
    #         triggers when created. True is not supported for <code>ON_DEMAND</code> triggers.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to use with this trigger. You may use tags to limit access to the trigger.
    #         For more information about tags in Glue, see
    #         <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a> in the developer guide. </p>
    #
    # @option params [EventBatchingCondition] :event_batching_condition
    #   <p>Batch condition that must be met (specified number of events received or batch time window expired)
    #         before EventBridge event trigger fires.</p>
    #
    # @return [Types::CreateTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_trigger(
    #     name: 'Name', # required
    #     workflow_name: 'WorkflowName',
    #     type: 'SCHEDULED', # required - accepts ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #     schedule: 'Schedule',
    #     predicate: {
    #       logical: 'AND', # accepts ["AND", "ANY"]
    #       conditions: [
    #         {
    #           logical_operator: 'EQUALS', # accepts ["EQUALS"]
    #           job_name: 'JobName',
    #           state: 'STARTING', # accepts ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #           crawler_name: 'CrawlerName',
    #           crawl_state: 'RUNNING' # accepts ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #         }
    #       ]
    #     },
    #     actions: [
    #       {
    #         job_name: 'JobName',
    #         arguments: {
    #           'key' => 'value'
    #         },
    #         timeout: 1,
    #         security_configuration: 'SecurityConfiguration',
    #         notification_property: {
    #           notify_delay_after: 1
    #         },
    #         crawler_name: 'CrawlerName'
    #       }
    #     ], # required
    #     description: 'Description',
    #     start_on_creation: false,
    #     tags: {
    #       'key' => 'value'
    #     },
    #     event_batching_condition: {
    #       batch_size: 1, # required
    #       batch_window: 1
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateTriggerOutput
    #   resp.data.name #=> String
    #
    def create_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::IdempotentParameterMismatchException]),
        data_parser: Parsers::CreateTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateTrigger,
        stubs: @stubs,
        params_class: Params::CreateTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new function definition in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateUserDefinedFunctionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which to create the function. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which to create the function.</p>
    #
    # @option params [UserDefinedFunctionInput] :function_input
    #   <p>A <code>FunctionInput</code> object that defines the function
    #         to create in the Data Catalog.</p>
    #
    # @return [Types::CreateUserDefinedFunctionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_user_defined_function(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     function_input: {
    #       function_name: 'FunctionName',
    #       class_name: 'ClassName',
    #       owner_name: 'OwnerName',
    #       owner_type: 'USER', # accepts ["USER", "ROLE", "GROUP"]
    #       resource_uris: [
    #         {
    #           resource_type: 'JAR', # accepts ["JAR", "FILE", "ARCHIVE"]
    #           uri: 'Uri'
    #         }
    #       ]
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateUserDefinedFunctionOutput
    #
    def create_user_defined_function(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateUserDefinedFunctionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateUserDefinedFunctionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateUserDefinedFunction
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateUserDefinedFunction
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateUserDefinedFunction,
        stubs: @stubs,
        params_class: Params::CreateUserDefinedFunctionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_user_defined_function
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates a new workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::CreateWorkflowInput}.
    #
    # @option params [String] :name
    #   <p>The name to be assigned to the workflow. It should be unique within your account.</p>
    #
    # @option params [String] :description
    #   <p>A description of the workflow.</p>
    #
    # @option params [Hash<String, String>] :default_run_properties
    #   <p>A collection of properties to be used as part of each execution of the workflow.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>The tags to be used with this workflow.</p>
    #
    # @option params [Integer] :max_concurrent_runs
    #   <p>You can use this parameter to prevent unwanted multiple updates to data, to control costs, or in some cases, to prevent exceeding the maximum number of concurrent runs of any of the component jobs. If you leave this parameter blank, there is no limit to the number of concurrent workflow runs.</p>
    #
    # @return [Types::CreateWorkflowOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.create_workflow(
    #     name: 'Name', # required
    #     description: 'Description',
    #     default_run_properties: {
    #       'key' => 'value'
    #     },
    #     tags: {
    #       'key' => 'value'
    #     },
    #     max_concurrent_runs: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::CreateWorkflowOutput
    #   resp.data.name #=> String
    #
    def create_workflow(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::CreateWorkflowInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::CreateWorkflowInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::CreateWorkflow
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::CreateWorkflow
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::CreateWorkflow,
        stubs: @stubs,
        params_class: Params::CreateWorkflowOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :create_workflow
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes an existing blueprint.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteBlueprintInput}.
    #
    # @option params [String] :name
    #   <p>The name of the blueprint to delete.</p>
    #
    # @return [Types::DeleteBlueprintOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_blueprint(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteBlueprintOutput
    #   resp.data.name #=> String
    #
    def delete_blueprint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteBlueprintInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteBlueprintInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteBlueprint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteBlueprint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteBlueprint,
        stubs: @stubs,
        params_class: Params::DeleteBlueprintOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_blueprint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes a classifier from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteClassifierInput}.
    #
    # @option params [String] :name
    #   <p>Name of the classifier to remove.</p>
    #
    # @return [Types::DeleteClassifierOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_classifier(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteClassifierOutput
    #
    def delete_classifier(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteClassifierInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteClassifierInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteClassifier
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteClassifier
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteClassifier,
        stubs: @stubs,
        params_class: Params::DeleteClassifierOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_classifier
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Delete the partition column statistics of a column.</p>
    # 	
    # 	        <p>The Identity and Access Management (IAM) permission required for this operation is <code>DeletePartition</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteColumnStatisticsForPartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<String>] :partition_values
    #   <p>A list of partition values identifying the partition.</p>
    #
    # @option params [String] :column_name
    #   <p>Name of the column.</p>
    #
    # @return [Types::DeleteColumnStatisticsForPartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_column_statistics_for_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ], # required
    #     column_name: 'ColumnName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteColumnStatisticsForPartitionOutput
    #
    def delete_column_statistics_for_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteColumnStatisticsForPartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteColumnStatisticsForPartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteColumnStatisticsForPartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteColumnStatisticsForPartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteColumnStatisticsForPartition,
        stubs: @stubs,
        params_class: Params::DeleteColumnStatisticsForPartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_column_statistics_for_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves table statistics of columns.</p>
    # 	
    # 	        <p>The Identity and Access Management (IAM) permission required for this operation is <code>DeleteTable</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteColumnStatisticsForTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [String] :column_name
    #   <p>The name of the column.</p>
    #
    # @return [Types::DeleteColumnStatisticsForTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_column_statistics_for_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     column_name: 'ColumnName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteColumnStatisticsForTableOutput
    #
    def delete_column_statistics_for_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteColumnStatisticsForTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteColumnStatisticsForTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteColumnStatisticsForTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteColumnStatisticsForTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteColumnStatisticsForTable,
        stubs: @stubs,
        params_class: Params::DeleteColumnStatisticsForTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_column_statistics_for_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a connection from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteConnectionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the connection resides. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :connection_name
    #   <p>The name of the connection to delete.</p>
    #
    # @return [Types::DeleteConnectionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_connection(
    #     catalog_id: 'CatalogId',
    #     connection_name: 'ConnectionName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteConnectionOutput
    #
    def delete_connection(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteConnectionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteConnectionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteConnection
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteConnection
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteConnection,
        stubs: @stubs,
        params_class: Params::DeleteConnectionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_connection
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes a specified crawler from the Glue Data Catalog, unless the crawler state is
    #         <code>RUNNING</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the crawler to remove.</p>
    #
    # @return [Types::DeleteCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_crawler(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteCrawlerOutput
    #
    def delete_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::SchedulerTransitioningException, Errors::EntityNotFoundException, Errors::OperationTimeoutException, Errors::CrawlerRunningException]),
        data_parser: Parsers::DeleteCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteCrawler,
        stubs: @stubs,
        params_class: Params::DeleteCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a custom pattern by specifying its name.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteCustomEntityTypeInput}.
    #
    # @option params [String] :name
    #   <p>The name of the custom pattern that you want to delete.</p>
    #
    # @return [Types::DeleteCustomEntityTypeOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_custom_entity_type(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteCustomEntityTypeOutput
    #   resp.data.name #=> String
    #
    def delete_custom_entity_type(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteCustomEntityTypeInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteCustomEntityTypeInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteCustomEntityType
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::DeleteCustomEntityType
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteCustomEntityType,
        stubs: @stubs,
        params_class: Params::DeleteCustomEntityTypeOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_custom_entity_type
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes a specified database from a Data Catalog.</p>
    #          <note>
    #             <p>After completing this operation, you no longer have access to the tables (and all table
    #         versions and partitions that might belong to the tables) and the user-defined functions in
    #         the deleted database. Glue deletes these "orphaned" resources asynchronously in a timely
    #         manner, at the discretion of the service.</p>
    #             <p>To ensure the immediate deletion of all related resources, before calling
    #           <code>DeleteDatabase</code>, use <code>DeleteTableVersion</code> or
    #           <code>BatchDeleteTableVersion</code>, <code>DeletePartition</code> or
    #           <code>BatchDeletePartition</code>, <code>DeleteUserDefinedFunction</code>, and
    #           <code>DeleteTable</code> or <code>BatchDeleteTable</code>, to delete any resources that
    #         belong to the database.</p>
    #          </note>
    #
    # @param [Hash] params
    #   See {Types::DeleteDatabaseInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the database resides. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :name
    #   <p>The name of the database to delete. For Hive compatibility, this must be all
    #         lowercase.</p>
    #
    # @return [Types::DeleteDatabaseOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_database(
    #     catalog_id: 'CatalogId',
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteDatabaseOutput
    #
    def delete_database(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteDatabaseInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteDatabaseInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteDatabase
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteDatabase
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteDatabase,
        stubs: @stubs,
        params_class: Params::DeleteDatabaseOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_database
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified development endpoint.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteDevEndpointInput}.
    #
    # @option params [String] :endpoint_name
    #   <p>The name of the <code>DevEndpoint</code>.</p>
    #
    # @return [Types::DeleteDevEndpointOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_dev_endpoint(
    #     endpoint_name: 'EndpointName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteDevEndpointOutput
    #
    def delete_dev_endpoint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteDevEndpointInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteDevEndpointInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteDevEndpoint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteDevEndpoint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteDevEndpoint,
        stubs: @stubs,
        params_class: Params::DeleteDevEndpointOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_dev_endpoint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified job definition. If the job definition
    #       is not found, no exception is thrown.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteJobInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition to delete.</p>
    #
    # @return [Types::DeleteJobOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_job(
    #     job_name: 'JobName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteJobOutput
    #   resp.data.job_name #=> String
    #
    def delete_job(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteJobInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteJobInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteJob
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteJob
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteJob,
        stubs: @stubs,
        params_class: Params::DeleteJobOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_job
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes an Glue machine learning transform. Machine learning transforms are a special
    #       type of transform that use machine learning to learn the details of the transformation to be
    #       performed by learning from examples provided by humans. These transformations are then saved
    #       by Glue. If you no longer need a transform, you can delete it by calling
    #         <code>DeleteMLTransforms</code>. However, any Glue jobs that still reference the deleted
    #       transform will no longer succeed.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteMLTransformInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the transform to delete.</p>
    #
    # @return [Types::DeleteMLTransformOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_ml_transform(
    #     transform_id: 'TransformId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteMLTransformOutput
    #   resp.data.transform_id #=> String
    #
    def delete_ml_transform(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteMLTransformInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteMLTransformInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteMLTransform
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteMLTransform
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteMLTransform,
        stubs: @stubs,
        params_class: Params::DeleteMLTransformOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_ml_transform
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified partition.</p>
    #
    # @param [Hash] params
    #   See {Types::DeletePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partition to be deleted resides. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the table in question
    #         resides.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table that contains the partition to be deleted.</p>
    #
    # @option params [Array<String>] :partition_values
    #   <p>The values that define the partition.</p>
    #
    # @return [Types::DeletePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeletePartitionOutput
    #
    def delete_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeletePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeletePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeletePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeletePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeletePartition,
        stubs: @stubs,
        params_class: Params::DeletePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified partition index from an existing table.</p>
    #
    # @param [Hash] params
    #   See {Types::DeletePartitionIndexInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The catalog ID where the table resides.</p>
    #
    # @option params [String] :database_name
    #   <p>Specifies the name of a database from which you want to delete a partition index.</p>
    #
    # @option params [String] :table_name
    #   <p>Specifies the name of a table from which you want to delete a partition index.</p>
    #
    # @option params [String] :index_name
    #   <p>The name of the partition index to be deleted.</p>
    #
    # @return [Types::DeletePartitionIndexOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_partition_index(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     index_name: 'IndexName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeletePartitionIndexOutput
    #
    def delete_partition_index(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeletePartitionIndexInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeletePartitionIndexInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeletePartitionIndex
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::ConflictException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeletePartitionIndex
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeletePartitionIndex,
        stubs: @stubs,
        params_class: Params::DeletePartitionIndexOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_partition_index
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Delete the entire registry including schema and all of its versions. To get the status of the delete operation, you can call the <code>GetRegistry</code> API after the asynchronous call. Deleting a registry will deactivate all online operations for the registry such as the <code>UpdateRegistry</code>, <code>CreateSchema</code>, <code>UpdateSchema</code>, and <code>RegisterSchemaVersion</code> APIs. </p>
    #
    # @param [Hash] params
    #   See {Types::DeleteRegistryInput}.
    #
    # @option params [RegistryId] :registry_id
    #   <p>This is a wrapper structure that may contain the registry name and Amazon Resource Name (ARN).</p>
    #
    # @return [Types::DeleteRegistryOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_registry(
    #     registry_id: {
    #       registry_name: 'RegistryName',
    #       registry_arn: 'RegistryArn'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteRegistryOutput
    #   resp.data.registry_name #=> String
    #   resp.data.registry_arn #=> String
    #   resp.data.status #=> String, one of ["AVAILABLE", "DELETING"]
    #
    def delete_registry(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteRegistryInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteRegistryInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteRegistry
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::DeleteRegistry
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteRegistry,
        stubs: @stubs,
        params_class: Params::DeleteRegistryOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_registry
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified policy.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteResourcePolicyInput}.
    #
    # @option params [String] :policy_hash_condition
    #   <p>The hash value returned when this policy was set.</p>
    #
    # @option params [String] :resource_arn
    #   <p>The ARN of the Glue resource for the resource policy to be deleted.</p>
    #
    # @return [Types::DeleteResourcePolicyOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_resource_policy(
    #     policy_hash_condition: 'PolicyHashCondition',
    #     resource_arn: 'ResourceArn'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteResourcePolicyOutput
    #
    def delete_resource_policy(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteResourcePolicyInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteResourcePolicyInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteResourcePolicy
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConditionCheckFailureException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteResourcePolicy
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteResourcePolicy,
        stubs: @stubs,
        params_class: Params::DeleteResourcePolicyOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_resource_policy
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes the entire schema set, including the schema set and all of its versions. To get the status of the delete operation, you can call <code>GetSchema</code> API after the asynchronous call. Deleting a registry will deactivate all online operations for the schema, such as the <code>GetSchemaByDefinition</code>, and <code>RegisterSchemaVersion</code> APIs.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteSchemaInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure that may contain the schema name and Amazon Resource Name (ARN).</p>
    #
    # @return [Types::DeleteSchemaOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_schema(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteSchemaOutput
    #   resp.data.schema_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.status #=> String, one of ["AVAILABLE", "PENDING", "DELETING"]
    #
    def delete_schema(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteSchemaInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteSchemaInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteSchema
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::DeleteSchema
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteSchema,
        stubs: @stubs,
        params_class: Params::DeleteSchemaOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_schema
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Remove versions from the specified schema. A version number or range may be supplied. If the compatibility mode forbids deleting of a version that is necessary, such as BACKWARDS_FULL, an error is returned.  Calling the <code>GetSchemaVersions</code> API after this call will list the status of the deleted versions.</p>
    #          <p>When the range of version numbers contain check pointed version, the API will return a 409 conflict and will not proceed with the deletion. You have to remove the checkpoint first using the <code>DeleteSchemaCheckpoint</code> API before using this API.</p>
    #          <p>You cannot use the <code>DeleteSchemaVersions</code> API to delete the first schema version in the schema set. The first schema version can only be deleted by the <code>DeleteSchema</code> API. This operation will also delete the attached <code>SchemaVersionMetadata</code> under the schema versions. Hard deletes will be enforced on the database.</p>
    #          <p>If the compatibility mode forbids deleting of a version that is necessary, such as BACKWARDS_FULL, an error is returned.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteSchemaVersionsInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure that may contain the schema name and Amazon Resource Name (ARN).</p>
    #
    # @option params [String] :versions
    #   <p>A version range may be supplied which may be of the format:</p>
    #   	        <ul>
    #               <li>
    #                  <p>a single version number, 5</p>
    #               </li>
    #               <li>
    #                  <p>a range, 5-8 : deletes versions 5, 6, 7, 8</p>
    #               </li>
    #            </ul>
    #
    # @return [Types::DeleteSchemaVersionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_schema_versions(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     versions: 'Versions' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteSchemaVersionsOutput
    #   resp.data.schema_version_errors #=> Array<SchemaVersionErrorItem>
    #   resp.data.schema_version_errors[0] #=> Types::SchemaVersionErrorItem
    #   resp.data.schema_version_errors[0].version_number #=> Integer
    #   resp.data.schema_version_errors[0].error_details #=> Types::ErrorDetails
    #   resp.data.schema_version_errors[0].error_details.error_code #=> String
    #   resp.data.schema_version_errors[0].error_details.error_message #=> String
    #
    def delete_schema_versions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteSchemaVersionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteSchemaVersionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteSchemaVersions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::DeleteSchemaVersions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteSchemaVersions,
        stubs: @stubs,
        params_class: Params::DeleteSchemaVersionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_schema_versions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified security configuration.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteSecurityConfigurationInput}.
    #
    # @option params [String] :name
    #   <p>The name of the security configuration to delete.</p>
    #
    # @return [Types::DeleteSecurityConfigurationOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_security_configuration(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteSecurityConfigurationOutput
    #
    def delete_security_configuration(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteSecurityConfigurationInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteSecurityConfigurationInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteSecurityConfiguration
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteSecurityConfiguration
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteSecurityConfiguration,
        stubs: @stubs,
        params_class: Params::DeleteSecurityConfigurationOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_security_configuration
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes the session.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteSessionInput}.
    #
    # @option params [String] :id
    #   <p>The ID of the session to be deleted.</p>
    #
    # @option params [String] :request_origin
    #   <p>The name of the origin of the delete session request.</p>
    #
    # @return [Types::DeleteSessionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_session(
    #     id: 'Id', # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteSessionOutput
    #   resp.data.id #=> String
    #
    def delete_session(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteSessionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteSessionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteSession
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::DeleteSession
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteSession,
        stubs: @stubs,
        params_class: Params::DeleteSessionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_session
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes a table definition from the Data Catalog.</p>
    #          <note>
    #             <p>After completing this operation, you no longer have access to the table versions and
    #         partitions that belong to the deleted table. Glue deletes these "orphaned" resources
    #         asynchronously in a timely manner, at the discretion of the service.</p>
    #             <p>To ensure the immediate deletion of all related resources, before calling
    #           <code>DeleteTable</code>, use <code>DeleteTableVersion</code> or
    #           <code>BatchDeleteTableVersion</code>, and <code>DeletePartition</code> or
    #           <code>BatchDeletePartition</code>, to delete any resources that belong to the
    #         table.</p>
    #          </note>
    #
    # @param [Hash] params
    #   See {Types::DeleteTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the table resides. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :name
    #   <p>The name of the table to be deleted. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to delete the table contents.</p>
    #
    # @return [Types::DeleteTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     name: 'Name', # required
    #     transaction_id: 'TransactionId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteTableOutput
    #
    def delete_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteTable,
        stubs: @stubs,
        params_class: Params::DeleteTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified version of a table.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteTableVersionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the tables reside. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The database in the catalog in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table. For Hive compatibility,
    #         this name is entirely lowercase.</p>
    #
    # @option params [String] :version_id
    #   <p>The ID of the table version to be deleted. A <code>VersionID</code> is a string representation of an integer. Each version is incremented by 1.</p>
    #
    # @return [Types::DeleteTableVersionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_table_version(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     version_id: 'VersionId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteTableVersionOutput
    #
    def delete_table_version(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteTableVersionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteTableVersionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteTableVersion
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteTableVersion
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteTableVersion,
        stubs: @stubs,
        params_class: Params::DeleteTableVersionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_table_version
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a specified trigger. If the trigger is not found, no
    #       exception is thrown.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger to delete.</p>
    #
    # @return [Types::DeleteTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_trigger(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteTriggerOutput
    #   resp.data.name #=> String
    #
    def delete_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteTrigger,
        stubs: @stubs,
        params_class: Params::DeleteTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes an existing function definition from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteUserDefinedFunctionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the function to be deleted is
    #         located. If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the function is located.</p>
    #
    # @option params [String] :function_name
    #   <p>The name of the function definition to be deleted.</p>
    #
    # @return [Types::DeleteUserDefinedFunctionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_user_defined_function(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     function_name: 'FunctionName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteUserDefinedFunctionOutput
    #
    def delete_user_defined_function(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteUserDefinedFunctionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteUserDefinedFunctionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteUserDefinedFunction
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteUserDefinedFunction
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteUserDefinedFunction,
        stubs: @stubs,
        params_class: Params::DeleteUserDefinedFunctionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_user_defined_function
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Deletes a workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::DeleteWorkflowInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow to be deleted.</p>
    #
    # @return [Types::DeleteWorkflowOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.delete_workflow(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::DeleteWorkflowOutput
    #   resp.data.name #=> String
    #
    def delete_workflow(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::DeleteWorkflowInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::DeleteWorkflowInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::DeleteWorkflow
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::DeleteWorkflow
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::DeleteWorkflow,
        stubs: @stubs,
        params_class: Params::DeleteWorkflowOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :delete_workflow
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the details of a blueprint.</p>
    #
    # @param [Hash] params
    #   See {Types::GetBlueprintInput}.
    #
    # @option params [String] :name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [Boolean] :include_blueprint
    #   <p>Specifies whether or not to include the blueprint in the response.</p>
    #
    # @option params [Boolean] :include_parameter_spec
    #   <p>Specifies whether or not to include the parameter specification.</p>
    #
    # @return [Types::GetBlueprintOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_blueprint(
    #     name: 'Name', # required
    #     include_blueprint: false,
    #     include_parameter_spec: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetBlueprintOutput
    #   resp.data.blueprint #=> Types::Blueprint
    #   resp.data.blueprint.name #=> String
    #   resp.data.blueprint.description #=> String
    #   resp.data.blueprint.created_on #=> Time
    #   resp.data.blueprint.last_modified_on #=> Time
    #   resp.data.blueprint.parameter_spec #=> String
    #   resp.data.blueprint.blueprint_location #=> String
    #   resp.data.blueprint.blueprint_service_location #=> String
    #   resp.data.blueprint.status #=> String, one of ["CREATING", "ACTIVE", "UPDATING", "FAILED"]
    #   resp.data.blueprint.error_message #=> String
    #   resp.data.blueprint.last_active_definition #=> Types::LastActiveDefinition
    #   resp.data.blueprint.last_active_definition.description #=> String
    #   resp.data.blueprint.last_active_definition.last_modified_on #=> Time
    #   resp.data.blueprint.last_active_definition.parameter_spec #=> String
    #   resp.data.blueprint.last_active_definition.blueprint_location #=> String
    #   resp.data.blueprint.last_active_definition.blueprint_service_location #=> String
    #
    def get_blueprint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetBlueprintInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetBlueprintInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetBlueprint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetBlueprint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetBlueprint,
        stubs: @stubs,
        params_class: Params::GetBlueprintOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_blueprint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the details of a blueprint run.</p>
    #
    # @param [Hash] params
    #   See {Types::GetBlueprintRunInput}.
    #
    # @option params [String] :blueprint_name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [String] :run_id
    #   <p>The run ID for the blueprint run you want to retrieve.</p>
    #
    # @return [Types::GetBlueprintRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_blueprint_run(
    #     blueprint_name: 'BlueprintName', # required
    #     run_id: 'RunId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetBlueprintRunOutput
    #   resp.data.blueprint_run #=> Types::BlueprintRun
    #   resp.data.blueprint_run.blueprint_name #=> String
    #   resp.data.blueprint_run.run_id #=> String
    #   resp.data.blueprint_run.workflow_name #=> String
    #   resp.data.blueprint_run.state #=> String, one of ["RUNNING", "SUCCEEDED", "FAILED", "ROLLING_BACK"]
    #   resp.data.blueprint_run.started_on #=> Time
    #   resp.data.blueprint_run.completed_on #=> Time
    #   resp.data.blueprint_run.error_message #=> String
    #   resp.data.blueprint_run.rollback_error_message #=> String
    #   resp.data.blueprint_run.parameters #=> String
    #   resp.data.blueprint_run.role_arn #=> String
    #
    def get_blueprint_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetBlueprintRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetBlueprintRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetBlueprintRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetBlueprintRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetBlueprintRun,
        stubs: @stubs,
        params_class: Params::GetBlueprintRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_blueprint_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the details of blueprint runs for a specified blueprint.</p>
    #
    # @param [Hash] params
    #   See {Types::GetBlueprintRunsInput}.
    #
    # @option params [String] :blueprint_name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @return [Types::GetBlueprintRunsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_blueprint_runs(
    #     blueprint_name: 'BlueprintName', # required
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetBlueprintRunsOutput
    #   resp.data.blueprint_runs #=> Array<BlueprintRun>
    #   resp.data.blueprint_runs[0] #=> Types::BlueprintRun
    #   resp.data.blueprint_runs[0].blueprint_name #=> String
    #   resp.data.blueprint_runs[0].run_id #=> String
    #   resp.data.blueprint_runs[0].workflow_name #=> String
    #   resp.data.blueprint_runs[0].state #=> String, one of ["RUNNING", "SUCCEEDED", "FAILED", "ROLLING_BACK"]
    #   resp.data.blueprint_runs[0].started_on #=> Time
    #   resp.data.blueprint_runs[0].completed_on #=> Time
    #   resp.data.blueprint_runs[0].error_message #=> String
    #   resp.data.blueprint_runs[0].rollback_error_message #=> String
    #   resp.data.blueprint_runs[0].parameters #=> String
    #   resp.data.blueprint_runs[0].role_arn #=> String
    #   resp.data.next_token #=> String
    #
    def get_blueprint_runs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetBlueprintRunsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetBlueprintRunsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetBlueprintRuns
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetBlueprintRuns
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetBlueprintRuns,
        stubs: @stubs,
        params_class: Params::GetBlueprintRunsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_blueprint_runs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the status of a migration operation.</p>
    #
    # @param [Hash] params
    #   See {Types::GetCatalogImportStatusInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the catalog to migrate. Currently, this should be the Amazon Web Services account ID.</p>
    #
    # @return [Types::GetCatalogImportStatusOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_catalog_import_status(
    #     catalog_id: 'CatalogId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetCatalogImportStatusOutput
    #   resp.data.import_status #=> Types::CatalogImportStatus
    #   resp.data.import_status.import_completed #=> Boolean
    #   resp.data.import_status.import_time #=> Time
    #   resp.data.import_status.imported_by #=> String
    #
    def get_catalog_import_status(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetCatalogImportStatusInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetCatalogImportStatusInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetCatalogImportStatus
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetCatalogImportStatus
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetCatalogImportStatus,
        stubs: @stubs,
        params_class: Params::GetCatalogImportStatusOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_catalog_import_status
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieve a classifier by name.</p>
    #
    # @param [Hash] params
    #   See {Types::GetClassifierInput}.
    #
    # @option params [String] :name
    #   <p>Name of the classifier to retrieve.</p>
    #
    # @return [Types::GetClassifierOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_classifier(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetClassifierOutput
    #   resp.data.classifier #=> Types::Classifier
    #   resp.data.classifier.grok_classifier #=> Types::GrokClassifier
    #   resp.data.classifier.grok_classifier.name #=> String
    #   resp.data.classifier.grok_classifier.classification #=> String
    #   resp.data.classifier.grok_classifier.creation_time #=> Time
    #   resp.data.classifier.grok_classifier.last_updated #=> Time
    #   resp.data.classifier.grok_classifier.version #=> Integer
    #   resp.data.classifier.grok_classifier.grok_pattern #=> String
    #   resp.data.classifier.grok_classifier.custom_patterns #=> String
    #   resp.data.classifier.xml_classifier #=> Types::XMLClassifier
    #   resp.data.classifier.xml_classifier.name #=> String
    #   resp.data.classifier.xml_classifier.classification #=> String
    #   resp.data.classifier.xml_classifier.creation_time #=> Time
    #   resp.data.classifier.xml_classifier.last_updated #=> Time
    #   resp.data.classifier.xml_classifier.version #=> Integer
    #   resp.data.classifier.xml_classifier.row_tag #=> String
    #   resp.data.classifier.json_classifier #=> Types::JsonClassifier
    #   resp.data.classifier.json_classifier.name #=> String
    #   resp.data.classifier.json_classifier.creation_time #=> Time
    #   resp.data.classifier.json_classifier.last_updated #=> Time
    #   resp.data.classifier.json_classifier.version #=> Integer
    #   resp.data.classifier.json_classifier.json_path #=> String
    #   resp.data.classifier.csv_classifier #=> Types::CsvClassifier
    #   resp.data.classifier.csv_classifier.name #=> String
    #   resp.data.classifier.csv_classifier.creation_time #=> Time
    #   resp.data.classifier.csv_classifier.last_updated #=> Time
    #   resp.data.classifier.csv_classifier.version #=> Integer
    #   resp.data.classifier.csv_classifier.delimiter #=> String
    #   resp.data.classifier.csv_classifier.quote_symbol #=> String
    #   resp.data.classifier.csv_classifier.contains_header #=> String, one of ["UNKNOWN", "PRESENT", "ABSENT"]
    #   resp.data.classifier.csv_classifier.header #=> Array<String>
    #   resp.data.classifier.csv_classifier.header[0] #=> String
    #   resp.data.classifier.csv_classifier.disable_value_trimming #=> Boolean
    #   resp.data.classifier.csv_classifier.allow_single_column #=> Boolean
    #
    def get_classifier(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetClassifierInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetClassifierInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetClassifier
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetClassifier
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetClassifier,
        stubs: @stubs,
        params_class: Params::GetClassifierOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_classifier
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Lists all classifier objects in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetClassifiersInput}.
    #
    # @option params [Integer] :max_results
    #   <p>The size of the list to return (optional).</p>
    #
    # @option params [String] :next_token
    #   <p>An optional continuation token.</p>
    #
    # @return [Types::GetClassifiersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_classifiers(
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetClassifiersOutput
    #   resp.data.classifiers #=> Array<Classifier>
    #   resp.data.classifiers[0] #=> Types::Classifier
    #   resp.data.classifiers[0].grok_classifier #=> Types::GrokClassifier
    #   resp.data.classifiers[0].grok_classifier.name #=> String
    #   resp.data.classifiers[0].grok_classifier.classification #=> String
    #   resp.data.classifiers[0].grok_classifier.creation_time #=> Time
    #   resp.data.classifiers[0].grok_classifier.last_updated #=> Time
    #   resp.data.classifiers[0].grok_classifier.version #=> Integer
    #   resp.data.classifiers[0].grok_classifier.grok_pattern #=> String
    #   resp.data.classifiers[0].grok_classifier.custom_patterns #=> String
    #   resp.data.classifiers[0].xml_classifier #=> Types::XMLClassifier
    #   resp.data.classifiers[0].xml_classifier.name #=> String
    #   resp.data.classifiers[0].xml_classifier.classification #=> String
    #   resp.data.classifiers[0].xml_classifier.creation_time #=> Time
    #   resp.data.classifiers[0].xml_classifier.last_updated #=> Time
    #   resp.data.classifiers[0].xml_classifier.version #=> Integer
    #   resp.data.classifiers[0].xml_classifier.row_tag #=> String
    #   resp.data.classifiers[0].json_classifier #=> Types::JsonClassifier
    #   resp.data.classifiers[0].json_classifier.name #=> String
    #   resp.data.classifiers[0].json_classifier.creation_time #=> Time
    #   resp.data.classifiers[0].json_classifier.last_updated #=> Time
    #   resp.data.classifiers[0].json_classifier.version #=> Integer
    #   resp.data.classifiers[0].json_classifier.json_path #=> String
    #   resp.data.classifiers[0].csv_classifier #=> Types::CsvClassifier
    #   resp.data.classifiers[0].csv_classifier.name #=> String
    #   resp.data.classifiers[0].csv_classifier.creation_time #=> Time
    #   resp.data.classifiers[0].csv_classifier.last_updated #=> Time
    #   resp.data.classifiers[0].csv_classifier.version #=> Integer
    #   resp.data.classifiers[0].csv_classifier.delimiter #=> String
    #   resp.data.classifiers[0].csv_classifier.quote_symbol #=> String
    #   resp.data.classifiers[0].csv_classifier.contains_header #=> String, one of ["UNKNOWN", "PRESENT", "ABSENT"]
    #   resp.data.classifiers[0].csv_classifier.header #=> Array<String>
    #   resp.data.classifiers[0].csv_classifier.header[0] #=> String
    #   resp.data.classifiers[0].csv_classifier.disable_value_trimming #=> Boolean
    #   resp.data.classifiers[0].csv_classifier.allow_single_column #=> Boolean
    #   resp.data.next_token #=> String
    #
    def get_classifiers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetClassifiersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetClassifiersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetClassifiers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::OperationTimeoutException]),
        data_parser: Parsers::GetClassifiers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetClassifiers,
        stubs: @stubs,
        params_class: Params::GetClassifiersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_classifiers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves partition statistics of columns.</p>
    # 	
    # 	        <p>The Identity and Access Management (IAM) permission required for this operation is <code>GetPartition</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetColumnStatisticsForPartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<String>] :partition_values
    #   <p>A list of partition values identifying the partition.</p>
    #
    # @option params [Array<String>] :column_names
    #   <p>A list of the column names.</p>
    #
    # @return [Types::GetColumnStatisticsForPartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_column_statistics_for_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ], # required
    #     column_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetColumnStatisticsForPartitionOutput
    #   resp.data.column_statistics_list #=> Array<ColumnStatistics>
    #   resp.data.column_statistics_list[0] #=> Types::ColumnStatistics
    #   resp.data.column_statistics_list[0].column_name #=> String
    #   resp.data.column_statistics_list[0].column_type #=> String
    #   resp.data.column_statistics_list[0].analyzed_time #=> Time
    #   resp.data.column_statistics_list[0].statistics_data #=> Types::ColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.type #=> String, one of ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data #=> Types::BooleanColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_trues #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_falses #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data #=> Types::DateColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.minimum_value #=> Time
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.maximum_value #=> Time
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data #=> Types::DecimalColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value #=> Types::DecimalNumber
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value.unscaled_value #=> String
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value.scale #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.maximum_value #=> Types::DecimalNumber
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data #=> Types::DoubleColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.minimum_value #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.maximum_value #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data #=> Types::LongColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.minimum_value #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.maximum_value #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data #=> Types::StringColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.maximum_length #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.average_length #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data #=> Types::BinaryColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.maximum_length #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.average_length #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors #=> Array<ColumnError>
    #   resp.data.errors[0] #=> Types::ColumnError
    #   resp.data.errors[0].column_name #=> String
    #   resp.data.errors[0].error #=> Types::ErrorDetail
    #   resp.data.errors[0].error.error_code #=> String
    #   resp.data.errors[0].error.error_message #=> String
    #
    def get_column_statistics_for_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetColumnStatisticsForPartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetColumnStatisticsForPartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetColumnStatisticsForPartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetColumnStatisticsForPartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetColumnStatisticsForPartition,
        stubs: @stubs,
        params_class: Params::GetColumnStatisticsForPartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_column_statistics_for_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves table statistics of columns.</p>
    # 	
    #          <p>The Identity and Access Management (IAM) permission required for this operation is <code>GetTable</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetColumnStatisticsForTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<String>] :column_names
    #   <p>A list of the column names.</p>
    #
    # @return [Types::GetColumnStatisticsForTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_column_statistics_for_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     column_names: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetColumnStatisticsForTableOutput
    #   resp.data.column_statistics_list #=> Array<ColumnStatistics>
    #   resp.data.column_statistics_list[0] #=> Types::ColumnStatistics
    #   resp.data.column_statistics_list[0].column_name #=> String
    #   resp.data.column_statistics_list[0].column_type #=> String
    #   resp.data.column_statistics_list[0].analyzed_time #=> Time
    #   resp.data.column_statistics_list[0].statistics_data #=> Types::ColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.type #=> String, one of ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data #=> Types::BooleanColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_trues #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_falses #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.boolean_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data #=> Types::DateColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.minimum_value #=> Time
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.maximum_value #=> Time
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.date_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data #=> Types::DecimalColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value #=> Types::DecimalNumber
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value.unscaled_value #=> String
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.minimum_value.scale #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.maximum_value #=> Types::DecimalNumber
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.decimal_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data #=> Types::DoubleColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.minimum_value #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.maximum_value #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.double_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data #=> Types::LongColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.minimum_value #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.maximum_value #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.long_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data #=> Types::StringColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.maximum_length #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.average_length #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.string_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data #=> Types::BinaryColumnStatisticsData
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.maximum_length #=> Integer
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.average_length #=> Float
    #   resp.data.column_statistics_list[0].statistics_data.binary_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors #=> Array<ColumnError>
    #   resp.data.errors[0] #=> Types::ColumnError
    #   resp.data.errors[0].column_name #=> String
    #   resp.data.errors[0].error #=> Types::ErrorDetail
    #   resp.data.errors[0].error.error_code #=> String
    #   resp.data.errors[0].error.error_message #=> String
    #
    def get_column_statistics_for_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetColumnStatisticsForTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetColumnStatisticsForTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetColumnStatisticsForTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetColumnStatisticsForTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetColumnStatisticsForTable,
        stubs: @stubs,
        params_class: Params::GetColumnStatisticsForTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_column_statistics_for_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a connection definition from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetConnectionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the connection resides. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :name
    #   <p>The name of the connection definition to retrieve.</p>
    #
    # @option params [Boolean] :hide_password
    #   <p>Allows you to retrieve the connection metadata without returning the password. For
    #         instance, the AWS Glue console uses this flag to retrieve the connection, and does not display
    #         the password. Set this parameter when the caller might not have permission to use the KMS
    #         key to decrypt the password, but it does have permission to access the rest of the connection
    #         properties.</p>
    #
    # @return [Types::GetConnectionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_connection(
    #     catalog_id: 'CatalogId',
    #     name: 'Name', # required
    #     hide_password: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetConnectionOutput
    #   resp.data.connection #=> Types::Connection
    #   resp.data.connection.name #=> String
    #   resp.data.connection.description #=> String
    #   resp.data.connection.connection_type #=> String, one of ["JDBC", "SFTP", "MONGODB", "KAFKA", "NETWORK", "MARKETPLACE", "CUSTOM"]
    #   resp.data.connection.match_criteria #=> Array<String>
    #   resp.data.connection.match_criteria[0] #=> String
    #   resp.data.connection.connection_properties #=> Hash<String, String>
    #   resp.data.connection.connection_properties['key'] #=> String
    #   resp.data.connection.physical_connection_requirements #=> Types::PhysicalConnectionRequirements
    #   resp.data.connection.physical_connection_requirements.subnet_id #=> String
    #   resp.data.connection.physical_connection_requirements.security_group_id_list #=> Array<String>
    #   resp.data.connection.physical_connection_requirements.security_group_id_list[0] #=> String
    #   resp.data.connection.physical_connection_requirements.availability_zone #=> String
    #   resp.data.connection.creation_time #=> Time
    #   resp.data.connection.last_updated_time #=> Time
    #   resp.data.connection.last_updated_by #=> String
    #
    def get_connection(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetConnectionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetConnectionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetConnection
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetConnection
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetConnection,
        stubs: @stubs,
        params_class: Params::GetConnectionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_connection
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a list of connection definitions from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetConnectionsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the connections reside. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [GetConnectionsFilter] :filter
    #   <p>A filter that controls which connections are returned.</p>
    #
    # @option params [Boolean] :hide_password
    #   <p>Allows you to retrieve the connection metadata without returning the password. For
    #         instance, the AWS Glue console uses this flag to retrieve the connection, and does not display
    #         the password. Set this parameter when the caller might not have permission to use the KMS
    #         key to decrypt the password, but it does have permission to access the rest of the connection
    #         properties.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of connections to return in one response.</p>
    #
    # @return [Types::GetConnectionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_connections(
    #     catalog_id: 'CatalogId',
    #     filter: {
    #       match_criteria: [
    #         'member'
    #       ],
    #       connection_type: 'JDBC' # accepts ["JDBC", "SFTP", "MONGODB", "KAFKA", "NETWORK", "MARKETPLACE", "CUSTOM"]
    #     },
    #     hide_password: false,
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetConnectionsOutput
    #   resp.data.connection_list #=> Array<Connection>
    #   resp.data.connection_list[0] #=> Types::Connection
    #   resp.data.connection_list[0].name #=> String
    #   resp.data.connection_list[0].description #=> String
    #   resp.data.connection_list[0].connection_type #=> String, one of ["JDBC", "SFTP", "MONGODB", "KAFKA", "NETWORK", "MARKETPLACE", "CUSTOM"]
    #   resp.data.connection_list[0].match_criteria #=> Array<String>
    #   resp.data.connection_list[0].match_criteria[0] #=> String
    #   resp.data.connection_list[0].connection_properties #=> Hash<String, String>
    #   resp.data.connection_list[0].connection_properties['key'] #=> String
    #   resp.data.connection_list[0].physical_connection_requirements #=> Types::PhysicalConnectionRequirements
    #   resp.data.connection_list[0].physical_connection_requirements.subnet_id #=> String
    #   resp.data.connection_list[0].physical_connection_requirements.security_group_id_list #=> Array<String>
    #   resp.data.connection_list[0].physical_connection_requirements.security_group_id_list[0] #=> String
    #   resp.data.connection_list[0].physical_connection_requirements.availability_zone #=> String
    #   resp.data.connection_list[0].creation_time #=> Time
    #   resp.data.connection_list[0].last_updated_time #=> Time
    #   resp.data.connection_list[0].last_updated_by #=> String
    #   resp.data.next_token #=> String
    #
    def get_connections(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetConnectionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetConnectionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetConnections
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetConnections
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetConnections,
        stubs: @stubs,
        params_class: Params::GetConnectionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_connections
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves metadata for a specified crawler.</p>
    #
    # @param [Hash] params
    #   See {Types::GetCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the crawler to retrieve metadata for.</p>
    #
    # @return [Types::GetCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_crawler(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetCrawlerOutput
    #   resp.data.crawler #=> Types::Crawler
    #   resp.data.crawler.name #=> String
    #   resp.data.crawler.role #=> String
    #   resp.data.crawler.targets #=> Types::CrawlerTargets
    #   resp.data.crawler.targets.s3_targets #=> Array<S3Target>
    #   resp.data.crawler.targets.s3_targets[0] #=> Types::S3Target
    #   resp.data.crawler.targets.s3_targets[0].path #=> String
    #   resp.data.crawler.targets.s3_targets[0].exclusions #=> Array<String>
    #   resp.data.crawler.targets.s3_targets[0].exclusions[0] #=> String
    #   resp.data.crawler.targets.s3_targets[0].connection_name #=> String
    #   resp.data.crawler.targets.s3_targets[0].sample_size #=> Integer
    #   resp.data.crawler.targets.s3_targets[0].event_queue_arn #=> String
    #   resp.data.crawler.targets.s3_targets[0].dlq_event_queue_arn #=> String
    #   resp.data.crawler.targets.jdbc_targets #=> Array<JdbcTarget>
    #   resp.data.crawler.targets.jdbc_targets[0] #=> Types::JdbcTarget
    #   resp.data.crawler.targets.jdbc_targets[0].connection_name #=> String
    #   resp.data.crawler.targets.jdbc_targets[0].path #=> String
    #   resp.data.crawler.targets.jdbc_targets[0].exclusions #=> Array<String>
    #   resp.data.crawler.targets.mongo_db_targets #=> Array<MongoDBTarget>
    #   resp.data.crawler.targets.mongo_db_targets[0] #=> Types::MongoDBTarget
    #   resp.data.crawler.targets.mongo_db_targets[0].connection_name #=> String
    #   resp.data.crawler.targets.mongo_db_targets[0].path #=> String
    #   resp.data.crawler.targets.mongo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawler.targets.dynamo_db_targets #=> Array<DynamoDBTarget>
    #   resp.data.crawler.targets.dynamo_db_targets[0] #=> Types::DynamoDBTarget
    #   resp.data.crawler.targets.dynamo_db_targets[0].path #=> String
    #   resp.data.crawler.targets.dynamo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawler.targets.dynamo_db_targets[0].scan_rate #=> Float
    #   resp.data.crawler.targets.catalog_targets #=> Array<CatalogTarget>
    #   resp.data.crawler.targets.catalog_targets[0] #=> Types::CatalogTarget
    #   resp.data.crawler.targets.catalog_targets[0].database_name #=> String
    #   resp.data.crawler.targets.catalog_targets[0].tables #=> Array<String>
    #   resp.data.crawler.targets.catalog_targets[0].tables[0] #=> String
    #   resp.data.crawler.targets.catalog_targets[0].connection_name #=> String
    #   resp.data.crawler.targets.delta_targets #=> Array<DeltaTarget>
    #   resp.data.crawler.targets.delta_targets[0] #=> Types::DeltaTarget
    #   resp.data.crawler.targets.delta_targets[0].delta_tables #=> Array<String>
    #   resp.data.crawler.targets.delta_targets[0].connection_name #=> String
    #   resp.data.crawler.targets.delta_targets[0].write_manifest #=> Boolean
    #   resp.data.crawler.database_name #=> String
    #   resp.data.crawler.description #=> String
    #   resp.data.crawler.classifiers #=> Array<String>
    #   resp.data.crawler.classifiers[0] #=> String
    #   resp.data.crawler.recrawl_policy #=> Types::RecrawlPolicy
    #   resp.data.crawler.recrawl_policy.recrawl_behavior #=> String, one of ["CRAWL_EVERYTHING", "CRAWL_NEW_FOLDERS_ONLY", "CRAWL_EVENT_MODE"]
    #   resp.data.crawler.schema_change_policy #=> Types::SchemaChangePolicy
    #   resp.data.crawler.schema_change_policy.update_behavior #=> String, one of ["LOG", "UPDATE_IN_DATABASE"]
    #   resp.data.crawler.schema_change_policy.delete_behavior #=> String, one of ["LOG", "DELETE_FROM_DATABASE", "DEPRECATE_IN_DATABASE"]
    #   resp.data.crawler.lineage_configuration #=> Types::LineageConfiguration
    #   resp.data.crawler.lineage_configuration.crawler_lineage_settings #=> String, one of ["ENABLE", "DISABLE"]
    #   resp.data.crawler.state #=> String, one of ["READY", "RUNNING", "STOPPING"]
    #   resp.data.crawler.table_prefix #=> String
    #   resp.data.crawler.schedule #=> Types::Schedule
    #   resp.data.crawler.schedule.schedule_expression #=> String
    #   resp.data.crawler.schedule.state #=> String, one of ["SCHEDULED", "NOT_SCHEDULED", "TRANSITIONING"]
    #   resp.data.crawler.crawl_elapsed_time #=> Integer
    #   resp.data.crawler.creation_time #=> Time
    #   resp.data.crawler.last_updated #=> Time
    #   resp.data.crawler.last_crawl #=> Types::LastCrawlInfo
    #   resp.data.crawler.last_crawl.status #=> String, one of ["SUCCEEDED", "CANCELLED", "FAILED"]
    #   resp.data.crawler.last_crawl.error_message #=> String
    #   resp.data.crawler.last_crawl.log_group #=> String
    #   resp.data.crawler.last_crawl.log_stream #=> String
    #   resp.data.crawler.last_crawl.message_prefix #=> String
    #   resp.data.crawler.last_crawl.start_time #=> Time
    #   resp.data.crawler.version #=> Integer
    #   resp.data.crawler.configuration #=> String
    #   resp.data.crawler.crawler_security_configuration #=> String
    #   resp.data.crawler.lake_formation_configuration #=> Types::LakeFormationConfiguration
    #   resp.data.crawler.lake_formation_configuration.use_lake_formation_credentials #=> Boolean
    #   resp.data.crawler.lake_formation_configuration.account_id #=> String
    #
    def get_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetCrawler,
        stubs: @stubs,
        params_class: Params::GetCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves metrics about specified crawlers.</p>
    #
    # @param [Hash] params
    #   See {Types::GetCrawlerMetricsInput}.
    #
    # @option params [Array<String>] :crawler_name_list
    #   <p>A list of the names of crawlers about which to retrieve metrics.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::GetCrawlerMetricsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_crawler_metrics(
    #     crawler_name_list: [
    #       'member'
    #     ],
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetCrawlerMetricsOutput
    #   resp.data.crawler_metrics_list #=> Array<CrawlerMetrics>
    #   resp.data.crawler_metrics_list[0] #=> Types::CrawlerMetrics
    #   resp.data.crawler_metrics_list[0].crawler_name #=> String
    #   resp.data.crawler_metrics_list[0].time_left_seconds #=> Float
    #   resp.data.crawler_metrics_list[0].still_estimating #=> Boolean
    #   resp.data.crawler_metrics_list[0].last_runtime_seconds #=> Float
    #   resp.data.crawler_metrics_list[0].median_runtime_seconds #=> Float
    #   resp.data.crawler_metrics_list[0].tables_created #=> Integer
    #   resp.data.crawler_metrics_list[0].tables_updated #=> Integer
    #   resp.data.crawler_metrics_list[0].tables_deleted #=> Integer
    #   resp.data.next_token #=> String
    #
    def get_crawler_metrics(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetCrawlerMetricsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetCrawlerMetricsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetCrawlerMetrics
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::OperationTimeoutException]),
        data_parser: Parsers::GetCrawlerMetrics
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetCrawlerMetrics,
        stubs: @stubs,
        params_class: Params::GetCrawlerMetricsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_crawler_metrics
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves metadata for all crawlers defined in the customer
    #       account.</p>
    #
    # @param [Hash] params
    #   See {Types::GetCrawlersInput}.
    #
    # @option params [Integer] :max_results
    #   <p>The number of crawlers to return on each call.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @return [Types::GetCrawlersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_crawlers(
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetCrawlersOutput
    #   resp.data.crawlers #=> Array<Crawler>
    #   resp.data.crawlers[0] #=> Types::Crawler
    #   resp.data.crawlers[0].name #=> String
    #   resp.data.crawlers[0].role #=> String
    #   resp.data.crawlers[0].targets #=> Types::CrawlerTargets
    #   resp.data.crawlers[0].targets.s3_targets #=> Array<S3Target>
    #   resp.data.crawlers[0].targets.s3_targets[0] #=> Types::S3Target
    #   resp.data.crawlers[0].targets.s3_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].exclusions #=> Array<String>
    #   resp.data.crawlers[0].targets.s3_targets[0].exclusions[0] #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].sample_size #=> Integer
    #   resp.data.crawlers[0].targets.s3_targets[0].event_queue_arn #=> String
    #   resp.data.crawlers[0].targets.s3_targets[0].dlq_event_queue_arn #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets #=> Array<JdbcTarget>
    #   resp.data.crawlers[0].targets.jdbc_targets[0] #=> Types::JdbcTarget
    #   resp.data.crawlers[0].targets.jdbc_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.jdbc_targets[0].exclusions #=> Array<String>
    #   resp.data.crawlers[0].targets.mongo_db_targets #=> Array<MongoDBTarget>
    #   resp.data.crawlers[0].targets.mongo_db_targets[0] #=> Types::MongoDBTarget
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.mongo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawlers[0].targets.dynamo_db_targets #=> Array<DynamoDBTarget>
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0] #=> Types::DynamoDBTarget
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].path #=> String
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].scan_all #=> Boolean
    #   resp.data.crawlers[0].targets.dynamo_db_targets[0].scan_rate #=> Float
    #   resp.data.crawlers[0].targets.catalog_targets #=> Array<CatalogTarget>
    #   resp.data.crawlers[0].targets.catalog_targets[0] #=> Types::CatalogTarget
    #   resp.data.crawlers[0].targets.catalog_targets[0].database_name #=> String
    #   resp.data.crawlers[0].targets.catalog_targets[0].tables #=> Array<String>
    #   resp.data.crawlers[0].targets.catalog_targets[0].tables[0] #=> String
    #   resp.data.crawlers[0].targets.catalog_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.delta_targets #=> Array<DeltaTarget>
    #   resp.data.crawlers[0].targets.delta_targets[0] #=> Types::DeltaTarget
    #   resp.data.crawlers[0].targets.delta_targets[0].delta_tables #=> Array<String>
    #   resp.data.crawlers[0].targets.delta_targets[0].connection_name #=> String
    #   resp.data.crawlers[0].targets.delta_targets[0].write_manifest #=> Boolean
    #   resp.data.crawlers[0].database_name #=> String
    #   resp.data.crawlers[0].description #=> String
    #   resp.data.crawlers[0].classifiers #=> Array<String>
    #   resp.data.crawlers[0].classifiers[0] #=> String
    #   resp.data.crawlers[0].recrawl_policy #=> Types::RecrawlPolicy
    #   resp.data.crawlers[0].recrawl_policy.recrawl_behavior #=> String, one of ["CRAWL_EVERYTHING", "CRAWL_NEW_FOLDERS_ONLY", "CRAWL_EVENT_MODE"]
    #   resp.data.crawlers[0].schema_change_policy #=> Types::SchemaChangePolicy
    #   resp.data.crawlers[0].schema_change_policy.update_behavior #=> String, one of ["LOG", "UPDATE_IN_DATABASE"]
    #   resp.data.crawlers[0].schema_change_policy.delete_behavior #=> String, one of ["LOG", "DELETE_FROM_DATABASE", "DEPRECATE_IN_DATABASE"]
    #   resp.data.crawlers[0].lineage_configuration #=> Types::LineageConfiguration
    #   resp.data.crawlers[0].lineage_configuration.crawler_lineage_settings #=> String, one of ["ENABLE", "DISABLE"]
    #   resp.data.crawlers[0].state #=> String, one of ["READY", "RUNNING", "STOPPING"]
    #   resp.data.crawlers[0].table_prefix #=> String
    #   resp.data.crawlers[0].schedule #=> Types::Schedule
    #   resp.data.crawlers[0].schedule.schedule_expression #=> String
    #   resp.data.crawlers[0].schedule.state #=> String, one of ["SCHEDULED", "NOT_SCHEDULED", "TRANSITIONING"]
    #   resp.data.crawlers[0].crawl_elapsed_time #=> Integer
    #   resp.data.crawlers[0].creation_time #=> Time
    #   resp.data.crawlers[0].last_updated #=> Time
    #   resp.data.crawlers[0].last_crawl #=> Types::LastCrawlInfo
    #   resp.data.crawlers[0].last_crawl.status #=> String, one of ["SUCCEEDED", "CANCELLED", "FAILED"]
    #   resp.data.crawlers[0].last_crawl.error_message #=> String
    #   resp.data.crawlers[0].last_crawl.log_group #=> String
    #   resp.data.crawlers[0].last_crawl.log_stream #=> String
    #   resp.data.crawlers[0].last_crawl.message_prefix #=> String
    #   resp.data.crawlers[0].last_crawl.start_time #=> Time
    #   resp.data.crawlers[0].version #=> Integer
    #   resp.data.crawlers[0].configuration #=> String
    #   resp.data.crawlers[0].crawler_security_configuration #=> String
    #   resp.data.crawlers[0].lake_formation_configuration #=> Types::LakeFormationConfiguration
    #   resp.data.crawlers[0].lake_formation_configuration.use_lake_formation_credentials #=> Boolean
    #   resp.data.crawlers[0].lake_formation_configuration.account_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_crawlers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetCrawlersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetCrawlersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetCrawlers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::OperationTimeoutException]),
        data_parser: Parsers::GetCrawlers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetCrawlers,
        stubs: @stubs,
        params_class: Params::GetCrawlersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_crawlers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the details of a custom pattern by specifying its name.</p>
    #
    # @param [Hash] params
    #   See {Types::GetCustomEntityTypeInput}.
    #
    # @option params [String] :name
    #   <p>The name of the custom pattern that you want to retrieve.</p>
    #
    # @return [Types::GetCustomEntityTypeOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_custom_entity_type(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetCustomEntityTypeOutput
    #   resp.data.name #=> String
    #   resp.data.regex_string #=> String
    #   resp.data.context_words #=> Array<String>
    #   resp.data.context_words[0] #=> String
    #
    def get_custom_entity_type(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetCustomEntityTypeInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetCustomEntityTypeInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetCustomEntityType
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetCustomEntityType
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetCustomEntityType,
        stubs: @stubs,
        params_class: Params::GetCustomEntityTypeOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_custom_entity_type
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the security configuration for a specified catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetDataCatalogEncryptionSettingsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog to retrieve the security configuration for. If none is
    #         provided, the Amazon Web Services account ID is used by default.</p>
    #
    # @return [Types::GetDataCatalogEncryptionSettingsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_data_catalog_encryption_settings(
    #     catalog_id: 'CatalogId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDataCatalogEncryptionSettingsOutput
    #   resp.data.data_catalog_encryption_settings #=> Types::DataCatalogEncryptionSettings
    #   resp.data.data_catalog_encryption_settings.encryption_at_rest #=> Types::EncryptionAtRest
    #   resp.data.data_catalog_encryption_settings.encryption_at_rest.catalog_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS"]
    #   resp.data.data_catalog_encryption_settings.encryption_at_rest.sse_aws_kms_key_id #=> String
    #   resp.data.data_catalog_encryption_settings.connection_password_encryption #=> Types::ConnectionPasswordEncryption
    #   resp.data.data_catalog_encryption_settings.connection_password_encryption.return_connection_password_encrypted #=> Boolean
    #   resp.data.data_catalog_encryption_settings.connection_password_encryption.aws_kms_key_id #=> String
    #
    def get_data_catalog_encryption_settings(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDataCatalogEncryptionSettingsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDataCatalogEncryptionSettingsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDataCatalogEncryptionSettings
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDataCatalogEncryptionSettings
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDataCatalogEncryptionSettings,
        stubs: @stubs,
        params_class: Params::GetDataCatalogEncryptionSettingsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_data_catalog_encryption_settings
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the definition of a specified database.</p>
    #
    # @param [Hash] params
    #   See {Types::GetDatabaseInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the database resides. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :name
    #   <p>The name of the database to retrieve. For Hive compatibility, this
    #         should be all lowercase.</p>
    #
    # @return [Types::GetDatabaseOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_database(
    #     catalog_id: 'CatalogId',
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDatabaseOutput
    #   resp.data.database #=> Types::Database
    #   resp.data.database.name #=> String
    #   resp.data.database.description #=> String
    #   resp.data.database.location_uri #=> String
    #   resp.data.database.parameters #=> Hash<String, String>
    #   resp.data.database.parameters['key'] #=> String
    #   resp.data.database.create_time #=> Time
    #   resp.data.database.create_table_default_permissions #=> Array<PrincipalPermissions>
    #   resp.data.database.create_table_default_permissions[0] #=> Types::PrincipalPermissions
    #   resp.data.database.create_table_default_permissions[0].principal #=> Types::DataLakePrincipal
    #   resp.data.database.create_table_default_permissions[0].principal.data_lake_principal_identifier #=> String
    #   resp.data.database.create_table_default_permissions[0].permissions #=> Array<String>
    #   resp.data.database.create_table_default_permissions[0].permissions[0] #=> String, one of ["ALL", "SELECT", "ALTER", "DROP", "DELETE", "INSERT", "CREATE_DATABASE", "CREATE_TABLE", "DATA_LOCATION_ACCESS"]
    #   resp.data.database.target_database #=> Types::DatabaseIdentifier
    #   resp.data.database.target_database.catalog_id #=> String
    #   resp.data.database.target_database.database_name #=> String
    #   resp.data.database.catalog_id #=> String
    #
    def get_database(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDatabaseInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDatabaseInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDatabase
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDatabase
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDatabase,
        stubs: @stubs,
        params_class: Params::GetDatabaseOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_database
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves all databases defined in a given Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetDatabasesInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog from which to retrieve <code>Databases</code>. If none is
    #         provided, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of databases to return in one response.</p>
    #
    # @option params [String] :resource_share_type
    #   <p>Allows you to specify that you want to list the databases shared with your account. The allowable values are <code>FOREIGN</code> or <code>ALL</code>. </p>
    #   	
    #   	        <ul>
    #               <li>
    #                  <p>If set to <code>FOREIGN</code>, will list the databases shared with your account. </p>
    #               </li>
    #               <li>
    #                  <p>If set to <code>ALL</code>, will list the databases shared with your account, as well as the databases in yor local account. </p>
    #               </li>
    #            </ul>
    #
    # @return [Types::GetDatabasesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_databases(
    #     catalog_id: 'CatalogId',
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     resource_share_type: 'FOREIGN' # accepts ["FOREIGN", "ALL"]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDatabasesOutput
    #   resp.data.database_list #=> Array<Database>
    #   resp.data.database_list[0] #=> Types::Database
    #   resp.data.database_list[0].name #=> String
    #   resp.data.database_list[0].description #=> String
    #   resp.data.database_list[0].location_uri #=> String
    #   resp.data.database_list[0].parameters #=> Hash<String, String>
    #   resp.data.database_list[0].parameters['key'] #=> String
    #   resp.data.database_list[0].create_time #=> Time
    #   resp.data.database_list[0].create_table_default_permissions #=> Array<PrincipalPermissions>
    #   resp.data.database_list[0].create_table_default_permissions[0] #=> Types::PrincipalPermissions
    #   resp.data.database_list[0].create_table_default_permissions[0].principal #=> Types::DataLakePrincipal
    #   resp.data.database_list[0].create_table_default_permissions[0].principal.data_lake_principal_identifier #=> String
    #   resp.data.database_list[0].create_table_default_permissions[0].permissions #=> Array<String>
    #   resp.data.database_list[0].create_table_default_permissions[0].permissions[0] #=> String, one of ["ALL", "SELECT", "ALTER", "DROP", "DELETE", "INSERT", "CREATE_DATABASE", "CREATE_TABLE", "DATA_LOCATION_ACCESS"]
    #   resp.data.database_list[0].target_database #=> Types::DatabaseIdentifier
    #   resp.data.database_list[0].target_database.catalog_id #=> String
    #   resp.data.database_list[0].target_database.database_name #=> String
    #   resp.data.database_list[0].catalog_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_databases(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDatabasesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDatabasesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDatabases
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDatabases
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDatabases,
        stubs: @stubs,
        params_class: Params::GetDatabasesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_databases
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Transforms a Python script into a directed acyclic graph (DAG). </p>
    #
    # @param [Hash] params
    #   See {Types::GetDataflowGraphInput}.
    #
    # @option params [String] :python_script
    #   <p>The Python script to transform.</p>
    #
    # @return [Types::GetDataflowGraphOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_dataflow_graph(
    #     python_script: 'PythonScript'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDataflowGraphOutput
    #   resp.data.dag_nodes #=> Array<CodeGenNode>
    #   resp.data.dag_nodes[0] #=> Types::CodeGenNode
    #   resp.data.dag_nodes[0].id #=> String
    #   resp.data.dag_nodes[0].node_type #=> String
    #   resp.data.dag_nodes[0].args #=> Array<CodeGenNodeArg>
    #   resp.data.dag_nodes[0].args[0] #=> Types::CodeGenNodeArg
    #   resp.data.dag_nodes[0].args[0].name #=> String
    #   resp.data.dag_nodes[0].args[0].value #=> String
    #   resp.data.dag_nodes[0].args[0].param #=> Boolean
    #   resp.data.dag_nodes[0].line_number #=> Integer
    #   resp.data.dag_edges #=> Array<CodeGenEdge>
    #   resp.data.dag_edges[0] #=> Types::CodeGenEdge
    #   resp.data.dag_edges[0].source #=> String
    #   resp.data.dag_edges[0].target #=> String
    #   resp.data.dag_edges[0].target_parameter #=> String
    #
    def get_dataflow_graph(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDataflowGraphInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDataflowGraphInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDataflowGraph
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDataflowGraph
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDataflowGraph,
        stubs: @stubs,
        params_class: Params::GetDataflowGraphOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_dataflow_graph
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves information about a specified development endpoint.</p>
    #          <note>
    #             <p>When you create a development endpoint in a virtual private cloud (VPC), Glue returns only
    #         a private IP address, and the public IP address field is not populated. When you create a
    #         non-VPC development endpoint, Glue returns only a public IP address.</p>
    #          </note>
    #
    # @param [Hash] params
    #   See {Types::GetDevEndpointInput}.
    #
    # @option params [String] :endpoint_name
    #   <p>Name of the <code>DevEndpoint</code> to retrieve information for.</p>
    #
    # @return [Types::GetDevEndpointOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_dev_endpoint(
    #     endpoint_name: 'EndpointName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDevEndpointOutput
    #   resp.data.dev_endpoint #=> Types::DevEndpoint
    #   resp.data.dev_endpoint.endpoint_name #=> String
    #   resp.data.dev_endpoint.role_arn #=> String
    #   resp.data.dev_endpoint.security_group_ids #=> Array<String>
    #   resp.data.dev_endpoint.security_group_ids[0] #=> String
    #   resp.data.dev_endpoint.subnet_id #=> String
    #   resp.data.dev_endpoint.yarn_endpoint_address #=> String
    #   resp.data.dev_endpoint.private_address #=> String
    #   resp.data.dev_endpoint.zeppelin_remote_spark_interpreter_port #=> Integer
    #   resp.data.dev_endpoint.public_address #=> String
    #   resp.data.dev_endpoint.status #=> String
    #   resp.data.dev_endpoint.worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.dev_endpoint.glue_version #=> String
    #   resp.data.dev_endpoint.number_of_workers #=> Integer
    #   resp.data.dev_endpoint.number_of_nodes #=> Integer
    #   resp.data.dev_endpoint.availability_zone #=> String
    #   resp.data.dev_endpoint.vpc_id #=> String
    #   resp.data.dev_endpoint.extra_python_libs_s3_path #=> String
    #   resp.data.dev_endpoint.extra_jars_s3_path #=> String
    #   resp.data.dev_endpoint.failure_reason #=> String
    #   resp.data.dev_endpoint.last_update_status #=> String
    #   resp.data.dev_endpoint.created_timestamp #=> Time
    #   resp.data.dev_endpoint.last_modified_timestamp #=> Time
    #   resp.data.dev_endpoint.public_key #=> String
    #   resp.data.dev_endpoint.public_keys #=> Array<String>
    #   resp.data.dev_endpoint.public_keys[0] #=> String
    #   resp.data.dev_endpoint.security_configuration #=> String
    #   resp.data.dev_endpoint.arguments #=> Hash<String, String>
    #   resp.data.dev_endpoint.arguments['key'] #=> String
    #
    def get_dev_endpoint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDevEndpointInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDevEndpointInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDevEndpoint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDevEndpoint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDevEndpoint,
        stubs: @stubs,
        params_class: Params::GetDevEndpointOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_dev_endpoint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves all the development endpoints in this AWS account.</p>
    #          <note>
    #             <p>When you create a development endpoint in a virtual private cloud (VPC), Glue returns only a private IP address
    #       and the public IP address field is not populated. When you create a non-VPC development
    #       endpoint, Glue returns only a public IP address.</p>
    #          </note>
    #
    # @param [Hash] params
    #   See {Types::GetDevEndpointsInput}.
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of information to return.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::GetDevEndpointsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_dev_endpoints(
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetDevEndpointsOutput
    #   resp.data.dev_endpoints #=> Array<DevEndpoint>
    #   resp.data.dev_endpoints[0] #=> Types::DevEndpoint
    #   resp.data.dev_endpoints[0].endpoint_name #=> String
    #   resp.data.dev_endpoints[0].role_arn #=> String
    #   resp.data.dev_endpoints[0].security_group_ids #=> Array<String>
    #   resp.data.dev_endpoints[0].security_group_ids[0] #=> String
    #   resp.data.dev_endpoints[0].subnet_id #=> String
    #   resp.data.dev_endpoints[0].yarn_endpoint_address #=> String
    #   resp.data.dev_endpoints[0].private_address #=> String
    #   resp.data.dev_endpoints[0].zeppelin_remote_spark_interpreter_port #=> Integer
    #   resp.data.dev_endpoints[0].public_address #=> String
    #   resp.data.dev_endpoints[0].status #=> String
    #   resp.data.dev_endpoints[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.dev_endpoints[0].glue_version #=> String
    #   resp.data.dev_endpoints[0].number_of_workers #=> Integer
    #   resp.data.dev_endpoints[0].number_of_nodes #=> Integer
    #   resp.data.dev_endpoints[0].availability_zone #=> String
    #   resp.data.dev_endpoints[0].vpc_id #=> String
    #   resp.data.dev_endpoints[0].extra_python_libs_s3_path #=> String
    #   resp.data.dev_endpoints[0].extra_jars_s3_path #=> String
    #   resp.data.dev_endpoints[0].failure_reason #=> String
    #   resp.data.dev_endpoints[0].last_update_status #=> String
    #   resp.data.dev_endpoints[0].created_timestamp #=> Time
    #   resp.data.dev_endpoints[0].last_modified_timestamp #=> Time
    #   resp.data.dev_endpoints[0].public_key #=> String
    #   resp.data.dev_endpoints[0].public_keys #=> Array<String>
    #   resp.data.dev_endpoints[0].public_keys[0] #=> String
    #   resp.data.dev_endpoints[0].security_configuration #=> String
    #   resp.data.dev_endpoints[0].arguments #=> Hash<String, String>
    #   resp.data.dev_endpoints[0].arguments['key'] #=> String
    #   resp.data.next_token #=> String
    #
    def get_dev_endpoints(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetDevEndpointsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetDevEndpointsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetDevEndpoints
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetDevEndpoints
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetDevEndpoints,
        stubs: @stubs,
        params_class: Params::GetDevEndpointsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_dev_endpoints
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves an existing job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::GetJobInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition to retrieve.</p>
    #
    # @return [Types::GetJobOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_job(
    #     job_name: 'JobName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetJobOutput
    #   resp.data.job #=> Types::Job
    #   resp.data.job.name #=> String
    #   resp.data.job.description #=> String
    #   resp.data.job.log_uri #=> String
    #   resp.data.job.role #=> String
    #   resp.data.job.created_on #=> Time
    #   resp.data.job.last_modified_on #=> Time
    #   resp.data.job.execution_property #=> Types::ExecutionProperty
    #   resp.data.job.execution_property.max_concurrent_runs #=> Integer
    #   resp.data.job.command #=> Types::JobCommand
    #   resp.data.job.command.name #=> String
    #   resp.data.job.command.script_location #=> String
    #   resp.data.job.command.python_version #=> String
    #   resp.data.job.default_arguments #=> Hash<String, String>
    #   resp.data.job.default_arguments['key'] #=> String
    #   resp.data.job.non_overridable_arguments #=> Hash<String, String>
    #   resp.data.job.connections #=> Types::ConnectionsList
    #   resp.data.job.connections.connections #=> Array<String>
    #   resp.data.job.connections.connections[0] #=> String
    #   resp.data.job.max_retries #=> Integer
    #   resp.data.job.allocated_capacity #=> Integer
    #   resp.data.job.timeout #=> Integer
    #   resp.data.job.max_capacity #=> Float
    #   resp.data.job.worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.job.number_of_workers #=> Integer
    #   resp.data.job.security_configuration #=> String
    #   resp.data.job.notification_property #=> Types::NotificationProperty
    #   resp.data.job.notification_property.notify_delay_after #=> Integer
    #   resp.data.job.glue_version #=> String
    #   resp.data.job.code_gen_configuration_nodes #=> Hash<String, CodeGenConfigurationNode>
    #   resp.data.job.code_gen_configuration_nodes['key'] #=> Types::CodeGenConfigurationNode
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source #=> Types::AthenaConnectorSource
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.connector_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.connection_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.connection_table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.schema_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0] #=> Types::GlueSchema
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns #=> Array<GlueStudioSchemaColumn>
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0] #=> Types::GlueStudioSchemaColumn
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source #=> Types::JDBCConnectorSource
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.connector_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.connection_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options #=> Types::JDBCConnectorOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.filter_predicate #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.partition_column #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.lower_bound #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.upper_bound #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.num_partitions #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys[0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys_sort_order #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping #=> Hash<String, String>
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping['key'] #=> String, one of ["DATE", "STRING", "TIMESTAMP", "INT", "FLOAT", "LONG", "BIGDECIMAL", "BYTE", "SHORT", "DOUBLE"]
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.connection_table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.query #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source #=> Types::SparkConnectorSource
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.connector_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.connection_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.additional_options #=> Hash<String, String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.additional_options['key'] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_source #=> Types::CatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source #=> Types::RedshiftSource
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source.redshift_tmp_dir #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_source.tmp_dir_iam_role #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source #=> Types::S3CatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.partition_predicate #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_files #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source #=> Types::S3CsvSource
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.paths #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.exclusions #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.group_size #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.group_files #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.recurse #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.max_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.max_files_in_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_files #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.additional_options.enable_sample_path #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.additional_options.sample_path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.separator #=> String, one of ["comma", "ctrla", "pipe", "semicolon", "tab"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.escaper #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.quote_char #=> String, one of ["quote", "quillemet", "single_quote", "disabled"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.multiline #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.with_header #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.write_header #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.skip_first #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.optimize_performance #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_csv_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source #=> Types::S3JsonSource
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.paths #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.exclusions #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.group_size #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.group_files #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.recurse #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.max_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.max_files_in_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.json_path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.multiline #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_json_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source #=> Types::S3ParquetSource
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.paths #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.compression_type #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.exclusions #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.group_size #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.group_files #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.recurse #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.max_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.max_files_in_band #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_parquet_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].relational_catalog_source #=> Types::RelationalCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].relational_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].relational_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].relational_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].dynamo_db_catalog_source #=> Types::DynamoDBCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].dynamo_db_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].dynamo_db_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].dynamo_db_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target #=> Types::JDBCConnectorTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.inputs[0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.connection_table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.connector_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.connection_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.job.code_gen_configuration_nodes['key'].jdbc_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target #=> Types::SparkConnectorTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.connector_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.connection_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_target #=> Types::BasicCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target #=> Types::RedshiftTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.redshift_tmp_dir #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.tmp_dir_iam_role #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options #=> Types::UpsertRedshiftTargetOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.table_location #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys[0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target #=> Types::S3CatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target #=> Types::S3GlueParquetTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.partition_keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.compression #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target #=> Types::S3DirectTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.partition_keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.compression #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.format #=> String, one of ["json", "csv", "avro", "orc", "parquet"]
    #   resp.data.job.code_gen_configuration_nodes['key'].s3_direct_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping #=> Types::ApplyMapping
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping #=> Array<Mapping>
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0] #=> Types::Mapping
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_key #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_path #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_type #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].dropped #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].apply_mapping.mapping[0].children #=> Array<Mapping>
    #   resp.data.job.code_gen_configuration_nodes['key'].select_fields #=> Types::SelectFields
    #   resp.data.job.code_gen_configuration_nodes['key'].select_fields.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].select_fields.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].select_fields.paths #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_fields #=> Types::DropFields
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_fields.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_fields.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_fields.paths #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].rename_field #=> Types::RenameField
    #   resp.data.job.code_gen_configuration_nodes['key'].rename_field.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].rename_field.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].rename_field.source_path #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].rename_field.target_path #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot #=> Types::Spigot
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot.path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot.topk #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].spigot.prob #=> Float
    #   resp.data.job.code_gen_configuration_nodes['key'].join #=> Types::Join
    #   resp.data.job.code_gen_configuration_nodes['key'].join.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].join.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].join.inputs[0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].join.join_type #=> String, one of ["equijoin", "left", "right", "outer", "leftsemi", "leftanti"]
    #   resp.data.job.code_gen_configuration_nodes['key'].join.columns #=> Array<JoinColumn>
    #   resp.data.job.code_gen_configuration_nodes['key'].join.columns[0] #=> Types::JoinColumn
    #   resp.data.job.code_gen_configuration_nodes['key'].join.columns[0].from #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].join.columns[0].keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].split_fields #=> Types::SplitFields
    #   resp.data.job.code_gen_configuration_nodes['key'].split_fields.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].split_fields.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].split_fields.paths #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].select_from_collection #=> Types::SelectFromCollection
    #   resp.data.job.code_gen_configuration_nodes['key'].select_from_collection.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].select_from_collection.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].select_from_collection.index #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].fill_missing_values #=> Types::FillMissingValues
    #   resp.data.job.code_gen_configuration_nodes['key'].fill_missing_values.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].fill_missing_values.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].fill_missing_values.imputed_path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].fill_missing_values.filled_path #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].filter #=> Types::Filter
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.logical_operator #=> String, one of ["AND", "OR"]
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters #=> Array<FilterExpression>
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0] #=> Types::FilterExpression
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].operation #=> String, one of ["EQ", "LT", "GT", "LTE", "GTE", "REGEX", "ISNULL"]
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].negated #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].values #=> Array<FilterValue>
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].values[0] #=> Types::FilterValue
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].values[0].type #=> String, one of ["COLUMNEXTRACTED", "CONSTANT"]
    #   resp.data.job.code_gen_configuration_nodes['key'].filter.filters[0].values[0].value #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code #=> Types::CustomCode
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.inputs[0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.code #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.class_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].custom_code.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql #=> Types::SparkSQL
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.sql_query #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.sql_aliases #=> Array<SqlAlias>
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0] #=> Types::SqlAlias
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].from #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].alias #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].spark_sql.output_schemas #=> Array<GlueSchema>
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source #=> Types::DirectKinesisSource
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.window_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.detect_schema #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.endpoint_url #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.classification #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.delimiter #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.starting_position #=> String, one of ["latest", "trim_horizon", "earliest"]
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_time_in_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_records_per_shard #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_record_per_read #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.add_idle_time_between_reads #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.idle_time_between_reads_in_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.describe_shard_interval #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.num_retries #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_retry_interval_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.avoid_empty_batches #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_arn #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_arn #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_session_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.polling_time #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.record_polling_limit #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source #=> Types::DirectKafkaSource
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.bootstrap_servers #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.security_protocol #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.connection_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.topic_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.assign #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.subscribe_pattern #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.classification #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.delimiter #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.starting_offsets #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.ending_offsets #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.poll_timeout_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.num_retries #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.max_offsets_per_trigger #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.min_partitions #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.window_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.detect_schema #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].direct_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source #=> Types::CatalogKinesisSource
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.window_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.detect_schema #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source #=> Types::CatalogKafkaSource
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.window_size #=> Integer
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.detect_schema #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].catalog_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields #=> Types::DropNullFields
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list #=> Types::NullCheckBoxList
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_empty #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_null_string #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_neg_one #=> Boolean
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list #=> Array<NullValueField>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0] #=> Types::NullValueField
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].value #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype #=> Types::Datatype
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.id #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.label #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].merge #=> Types::Merge
    #   resp.data.job.code_gen_configuration_nodes['key'].merge.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].merge.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].merge.source #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].merge.primary_keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].union #=> Types::Union
    #   resp.data.job.code_gen_configuration_nodes['key'].union.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].union.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].union.union_type #=> String, one of ["ALL", "DISTINCT"]
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection #=> Types::PIIDetection
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.pii_type #=> String, one of ["RowAudit", "RowMasking", "ColumnAudit", "ColumnMasking"]
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.entity_types_to_detect #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.output_column_name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.sample_fraction #=> Float
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.threshold_fraction #=> Float
    #   resp.data.job.code_gen_configuration_nodes['key'].pii_detection.mask_value #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate #=> Types::Aggregate
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.groups #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.aggs #=> Array<AggregateOperation>
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.aggs[0] #=> Types::AggregateOperation
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.aggs[0].column #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].aggregate.aggs[0].agg_func #=> String, one of ["avg", "countDistinct", "count", "first", "last", "kurtosis", "max", "min", "skewness", "stddev_samp", "stddev_pop", "sum", "sumDistinct", "var_samp", "var_pop"]
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates #=> Types::DropDuplicates
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates.columns #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates.columns[0] #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].drop_duplicates.columns[0][0] #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target #=> Types::GovernedCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source #=> Types::GovernedCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source.partition_predicate #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].governed_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source #=> Types::MicrosoftSQLServerCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_source #=> Types::MySQLCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_source #=> Types::OracleSQLCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_source #=> Types::PostgreSQLCatalogSource
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_source.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_source.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_source.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target #=> Types::MicrosoftSQLServerCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_target #=> Types::MySQLCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].my_sql_catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_target #=> Types::OracleSQLCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].oracle_sql_catalog_target.table #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_target #=> Types::PostgreSQLCatalogTarget
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_target.name #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_target.database #=> String
    #   resp.data.job.code_gen_configuration_nodes['key'].postgre_sql_catalog_target.table #=> String
    #
    def get_job(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetJobInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetJobInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetJob
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetJob
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetJob,
        stubs: @stubs,
        params_class: Params::GetJobOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_job
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns information on a job bookmark entry.</p>
    #
    # @param [Hash] params
    #   See {Types::GetJobBookmarkInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job in question.</p>
    #
    # @option params [String] :run_id
    #   <p>The unique run identifier associated with this job run.</p>
    #
    # @return [Types::GetJobBookmarkOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_job_bookmark(
    #     job_name: 'JobName', # required
    #     run_id: 'RunId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetJobBookmarkOutput
    #   resp.data.job_bookmark_entry #=> Types::JobBookmarkEntry
    #   resp.data.job_bookmark_entry.job_name #=> String
    #   resp.data.job_bookmark_entry.version #=> Integer
    #   resp.data.job_bookmark_entry.run #=> Integer
    #   resp.data.job_bookmark_entry.attempt #=> Integer
    #   resp.data.job_bookmark_entry.previous_run_id #=> String
    #   resp.data.job_bookmark_entry.run_id #=> String
    #   resp.data.job_bookmark_entry.job_bookmark #=> String
    #
    def get_job_bookmark(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetJobBookmarkInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetJobBookmarkInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetJobBookmark
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::ValidationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetJobBookmark
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetJobBookmark,
        stubs: @stubs,
        params_class: Params::GetJobBookmarkOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_job_bookmark
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the metadata for a given job run.</p>
    #
    # @param [Hash] params
    #   See {Types::GetJobRunInput}.
    #
    # @option params [String] :job_name
    #   <p>Name of the job definition being run.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the job run.</p>
    #
    # @option params [Boolean] :predecessors_included
    #   <p>True if a list of predecessor runs should be returned.</p>
    #
    # @return [Types::GetJobRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_job_run(
    #     job_name: 'JobName', # required
    #     run_id: 'RunId', # required
    #     predecessors_included: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetJobRunOutput
    #   resp.data.job_run #=> Types::JobRun
    #   resp.data.job_run.id #=> String
    #   resp.data.job_run.attempt #=> Integer
    #   resp.data.job_run.previous_run_id #=> String
    #   resp.data.job_run.trigger_name #=> String
    #   resp.data.job_run.job_name #=> String
    #   resp.data.job_run.started_on #=> Time
    #   resp.data.job_run.last_modified_on #=> Time
    #   resp.data.job_run.completed_on #=> Time
    #   resp.data.job_run.job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.job_run.arguments #=> Hash<String, String>
    #   resp.data.job_run.arguments['key'] #=> String
    #   resp.data.job_run.error_message #=> String
    #   resp.data.job_run.predecessor_runs #=> Array<Predecessor>
    #   resp.data.job_run.predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.job_run.predecessor_runs[0].job_name #=> String
    #   resp.data.job_run.predecessor_runs[0].run_id #=> String
    #   resp.data.job_run.allocated_capacity #=> Integer
    #   resp.data.job_run.execution_time #=> Integer
    #   resp.data.job_run.timeout #=> Integer
    #   resp.data.job_run.max_capacity #=> Float
    #   resp.data.job_run.worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.job_run.number_of_workers #=> Integer
    #   resp.data.job_run.security_configuration #=> String
    #   resp.data.job_run.log_group_name #=> String
    #   resp.data.job_run.notification_property #=> Types::NotificationProperty
    #   resp.data.job_run.notification_property.notify_delay_after #=> Integer
    #   resp.data.job_run.glue_version #=> String
    #   resp.data.job_run.dpu_seconds #=> Float
    #
    def get_job_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetJobRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetJobRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetJobRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetJobRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetJobRun,
        stubs: @stubs,
        params_class: Params::GetJobRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_job_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves metadata for all runs of a given job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::GetJobRunsInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition for which to retrieve all job runs.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of the response.</p>
    #
    # @return [Types::GetJobRunsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_job_runs(
    #     job_name: 'JobName', # required
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetJobRunsOutput
    #   resp.data.job_runs #=> Array<JobRun>
    #   resp.data.job_runs[0] #=> Types::JobRun
    #   resp.data.job_runs[0].id #=> String
    #   resp.data.job_runs[0].attempt #=> Integer
    #   resp.data.job_runs[0].previous_run_id #=> String
    #   resp.data.job_runs[0].trigger_name #=> String
    #   resp.data.job_runs[0].job_name #=> String
    #   resp.data.job_runs[0].started_on #=> Time
    #   resp.data.job_runs[0].last_modified_on #=> Time
    #   resp.data.job_runs[0].completed_on #=> Time
    #   resp.data.job_runs[0].job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.job_runs[0].arguments #=> Hash<String, String>
    #   resp.data.job_runs[0].arguments['key'] #=> String
    #   resp.data.job_runs[0].error_message #=> String
    #   resp.data.job_runs[0].predecessor_runs #=> Array<Predecessor>
    #   resp.data.job_runs[0].predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.job_runs[0].predecessor_runs[0].job_name #=> String
    #   resp.data.job_runs[0].predecessor_runs[0].run_id #=> String
    #   resp.data.job_runs[0].allocated_capacity #=> Integer
    #   resp.data.job_runs[0].execution_time #=> Integer
    #   resp.data.job_runs[0].timeout #=> Integer
    #   resp.data.job_runs[0].max_capacity #=> Float
    #   resp.data.job_runs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.job_runs[0].number_of_workers #=> Integer
    #   resp.data.job_runs[0].security_configuration #=> String
    #   resp.data.job_runs[0].log_group_name #=> String
    #   resp.data.job_runs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.job_runs[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.job_runs[0].glue_version #=> String
    #   resp.data.job_runs[0].dpu_seconds #=> Float
    #   resp.data.next_token #=> String
    #
    def get_job_runs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetJobRunsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetJobRunsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetJobRuns
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetJobRuns
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetJobRuns,
        stubs: @stubs,
        params_class: Params::GetJobRunsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_job_runs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves all current job definitions.</p>
    #
    # @param [Hash] params
    #   See {Types::GetJobsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of the response.</p>
    #
    # @return [Types::GetJobsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_jobs(
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetJobsOutput
    #   resp.data.jobs #=> Array<Job>
    #   resp.data.jobs[0] #=> Types::Job
    #   resp.data.jobs[0].name #=> String
    #   resp.data.jobs[0].description #=> String
    #   resp.data.jobs[0].log_uri #=> String
    #   resp.data.jobs[0].role #=> String
    #   resp.data.jobs[0].created_on #=> Time
    #   resp.data.jobs[0].last_modified_on #=> Time
    #   resp.data.jobs[0].execution_property #=> Types::ExecutionProperty
    #   resp.data.jobs[0].execution_property.max_concurrent_runs #=> Integer
    #   resp.data.jobs[0].command #=> Types::JobCommand
    #   resp.data.jobs[0].command.name #=> String
    #   resp.data.jobs[0].command.script_location #=> String
    #   resp.data.jobs[0].command.python_version #=> String
    #   resp.data.jobs[0].default_arguments #=> Hash<String, String>
    #   resp.data.jobs[0].default_arguments['key'] #=> String
    #   resp.data.jobs[0].non_overridable_arguments #=> Hash<String, String>
    #   resp.data.jobs[0].connections #=> Types::ConnectionsList
    #   resp.data.jobs[0].connections.connections #=> Array<String>
    #   resp.data.jobs[0].connections.connections[0] #=> String
    #   resp.data.jobs[0].max_retries #=> Integer
    #   resp.data.jobs[0].allocated_capacity #=> Integer
    #   resp.data.jobs[0].timeout #=> Integer
    #   resp.data.jobs[0].max_capacity #=> Float
    #   resp.data.jobs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.jobs[0].number_of_workers #=> Integer
    #   resp.data.jobs[0].security_configuration #=> String
    #   resp.data.jobs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.jobs[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.jobs[0].glue_version #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes #=> Hash<String, CodeGenConfigurationNode>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'] #=> Types::CodeGenConfigurationNode
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source #=> Types::AthenaConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.schema_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0] #=> Types::GlueSchema
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns #=> Array<GlueStudioSchemaColumn>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0] #=> Types::GlueStudioSchemaColumn
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].athena_connector_source.output_schemas[0].columns[0].type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source #=> Types::JDBCConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options #=> Types::JDBCConnectorOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.filter_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.partition_column #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.lower_bound #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.upper_bound #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.num_partitions #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.job_bookmark_keys_sort_order #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.additional_options.data_type_mapping['key'] #=> String, one of ["DATE", "STRING", "TIMESTAMP", "INT", "FLOAT", "LONG", "BIGDECIMAL", "BYTE", "SHORT", "DOUBLE"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.query #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source #=> Types::SparkConnectorSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.additional_options['key'] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source #=> Types::CatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source #=> Types::RedshiftSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.redshift_tmp_dir #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_source.tmp_dir_iam_role #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source #=> Types::S3CatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.partition_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_source.additional_options.bounded_files #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source #=> Types::S3CsvSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.bounded_files #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.enable_sample_path #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.additional_options.sample_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.separator #=> String, one of ["comma", "ctrla", "pipe", "semicolon", "tab"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.escaper #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.quote_char #=> String, one of ["quote", "quillemet", "single_quote", "disabled"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.multiline #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.with_header #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.write_header #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.skip_first #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.optimize_performance #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_csv_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source #=> Types::S3JsonSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.compression_type #=> String, one of ["gzip", "bzip2"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.json_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.multiline #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_json_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source #=> Types::S3ParquetSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.paths #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.compression_type #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.exclusions #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.group_size #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.group_files #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.recurse #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.max_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.max_files_in_band #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.additional_options #=> Types::S3DirectSourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_parquet_source.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source #=> Types::RelationalCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].relational_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source #=> Types::DynamoDBCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].dynamo_db_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target #=> Types::JDBCConnectorTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].jdbc_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target #=> Types::SparkConnectorTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connector_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.connection_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.additional_options #=> Hash<String, String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_connector_target.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target #=> Types::BasicCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target #=> Types::RedshiftTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.redshift_tmp_dir #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.tmp_dir_iam_role #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options #=> Types::UpsertRedshiftTargetOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.table_location #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].redshift_target.upsert_redshift_options.upsert_keys[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target #=> Types::S3CatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_catalog_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target #=> Types::S3GlueParquetTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.compression #=> String, one of ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.enable_update_catalog #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.update_behavior #=> String, one of ["UPDATE_IN_DATABASE", "LOG"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_glue_parquet_target.schema_change_policy.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target #=> Types::S3DirectTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.compression #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.format #=> String, one of ["json", "csv", "avro", "orc", "parquet"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].s3_direct_target.schema_change_policy #=> Types::DirectSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping #=> Types::ApplyMapping
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping #=> Array<Mapping>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0] #=> Types::Mapping
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_key #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].from_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].to_type #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].dropped #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].apply_mapping.mapping[0].children #=> Array<Mapping>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields #=> Types::SelectFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields #=> Types::DropFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field #=> Types::RenameField
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.source_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].rename_field.target_path #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot #=> Types::Spigot
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.topk #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spigot.prob #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join #=> Types::Join
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.join_type #=> String, one of ["equijoin", "left", "right", "outer", "leftsemi", "leftanti"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns #=> Array<JoinColumn>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0] #=> Types::JoinColumn
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0].from #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].join.columns[0].keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields #=> Types::SplitFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].split_fields.paths #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection #=> Types::SelectFromCollection
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].select_from_collection.index #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values #=> Types::FillMissingValues
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.imputed_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].fill_missing_values.filled_path #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter #=> Types::Filter
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.logical_operator #=> String, one of ["AND", "OR"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters #=> Array<FilterExpression>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0] #=> Types::FilterExpression
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].operation #=> String, one of ["EQ", "LT", "GT", "LTE", "GTE", "REGEX", "ISNULL"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].negated #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values #=> Array<FilterValue>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0] #=> Types::FilterValue
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0].type #=> String, one of ["COLUMNEXTRACTED", "CONSTANT"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].filter.filters[0].values[0].value #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code #=> Types::CustomCode
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.inputs[0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.code #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.class_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].custom_code.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql #=> Types::SparkSQL
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_query #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases #=> Array<SqlAlias>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0] #=> Types::SqlAlias
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].from #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.sql_aliases[0].alias #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].spark_sql.output_schemas #=> Array<GlueSchema>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source #=> Types::DirectKinesisSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.endpoint_url #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.classification #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.delimiter #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.starting_position #=> String, one of ["latest", "trim_horizon", "earliest"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_time_in_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_fetch_records_per_shard #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_record_per_read #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.add_idle_time_between_reads #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.idle_time_between_reads_in_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.describe_shard_interval #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.num_retries #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.max_retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.avoid_empty_batches #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.stream_arn #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_arn #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.streaming_options.role_session_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.polling_time #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kinesis_source.data_preview_options.record_polling_limit #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source #=> Types::DirectKafkaSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.bootstrap_servers #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.security_protocol #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.connection_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.topic_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.assign #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.subscribe_pattern #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.classification #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.delimiter #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.starting_offsets #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.ending_offsets #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.poll_timeout_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.num_retries #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.retry_interval_ms #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.max_offsets_per_trigger #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.streaming_options.min_partitions #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].direct_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source #=> Types::CatalogKinesisSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.streaming_options #=> Types::KinesisStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kinesis_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source #=> Types::CatalogKafkaSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.window_size #=> Integer
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.detect_schema #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.streaming_options #=> Types::KafkaStreamingSourceOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].catalog_kafka_source.data_preview_options #=> Types::StreamingDataPreviewOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields #=> Types::DropNullFields
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list #=> Types::NullCheckBoxList
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_empty #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_null_string #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_check_box_list.is_neg_one #=> Boolean
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list #=> Array<NullValueField>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0] #=> Types::NullValueField
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].value #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype #=> Types::Datatype
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.id #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_null_fields.null_text_list[0].datatype.label #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge #=> Types::Merge
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.source #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].merge.primary_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union #=> Types::Union
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].union.union_type #=> String, one of ["ALL", "DISTINCT"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection #=> Types::PIIDetection
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.pii_type #=> String, one of ["RowAudit", "RowMasking", "ColumnAudit", "ColumnMasking"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.entity_types_to_detect #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.output_column_name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.sample_fraction #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.threshold_fraction #=> Float
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].pii_detection.mask_value #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate #=> Types::Aggregate
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.groups #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs #=> Array<AggregateOperation>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0] #=> Types::AggregateOperation
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0].column #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].aggregate.aggs[0].agg_func #=> String, one of ["avg", "countDistinct", "count", "first", "last", "kurtosis", "max", "min", "skewness", "stddev_samp", "stddev_pop", "sum", "sumDistinct", "var_samp", "var_pop"]
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates #=> Types::DropDuplicates
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns[0] #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].drop_duplicates.columns[0][0] #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target #=> Types::GovernedCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.partition_keys #=> Array<Array<String>>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_target.schema_change_policy #=> Types::CatalogSchemaChangePolicy
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source #=> Types::GovernedCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.partition_predicate #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].governed_catalog_source.additional_options #=> Types::S3SourceAdditionalOptions
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source #=> Types::MicrosoftSQLServerCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source #=> Types::MySQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source #=> Types::OracleSQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source #=> Types::PostgreSQLCatalogSource
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_source.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target #=> Types::MicrosoftSQLServerCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].microsoft_sql_server_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target #=> Types::MySQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].my_sql_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target #=> Types::OracleSQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].oracle_sql_catalog_target.table #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target #=> Types::PostgreSQLCatalogTarget
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.name #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.inputs #=> Array<String>
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.database #=> String
    #   resp.data.jobs[0].code_gen_configuration_nodes['key'].postgre_sql_catalog_target.table #=> String
    #   resp.data.next_token #=> String
    #
    def get_jobs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetJobsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetJobsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetJobs
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetJobs
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetJobs,
        stubs: @stubs,
        params_class: Params::GetJobsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_jobs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets details for a specific task run on a machine learning transform. Machine learning
    #       task runs are asynchronous tasks that Glue runs on your behalf as part of various machine
    #       learning workflows. You can check the stats of any task run by calling
    #         <code>GetMLTaskRun</code> with the <code>TaskRunID</code> and its parent transform's
    #         <code>TransformID</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetMLTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :task_run_id
    #   <p>The unique identifier of the task run.</p>
    #
    # @return [Types::GetMLTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_ml_task_run(
    #     transform_id: 'TransformId', # required
    #     task_run_id: 'TaskRunId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetMLTaskRunOutput
    #   resp.data.transform_id #=> String
    #   resp.data.task_run_id #=> String
    #   resp.data.status #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.log_group_name #=> String
    #   resp.data.properties #=> Types::TaskRunProperties
    #   resp.data.properties.task_type #=> String, one of ["EVALUATION", "LABELING_SET_GENERATION", "IMPORT_LABELS", "EXPORT_LABELS", "FIND_MATCHES"]
    #   resp.data.properties.import_labels_task_run_properties #=> Types::ImportLabelsTaskRunProperties
    #   resp.data.properties.import_labels_task_run_properties.input_s3_path #=> String
    #   resp.data.properties.import_labels_task_run_properties.replace #=> Boolean
    #   resp.data.properties.export_labels_task_run_properties #=> Types::ExportLabelsTaskRunProperties
    #   resp.data.properties.export_labels_task_run_properties.output_s3_path #=> String
    #   resp.data.properties.labeling_set_generation_task_run_properties #=> Types::LabelingSetGenerationTaskRunProperties
    #   resp.data.properties.labeling_set_generation_task_run_properties.output_s3_path #=> String
    #   resp.data.properties.find_matches_task_run_properties #=> Types::FindMatchesTaskRunProperties
    #   resp.data.properties.find_matches_task_run_properties.job_id #=> String
    #   resp.data.properties.find_matches_task_run_properties.job_name #=> String
    #   resp.data.properties.find_matches_task_run_properties.job_run_id #=> String
    #   resp.data.error_string #=> String
    #   resp.data.started_on #=> Time
    #   resp.data.last_modified_on #=> Time
    #   resp.data.completed_on #=> Time
    #   resp.data.execution_time #=> Integer
    #
    def get_ml_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetMLTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetMLTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetMLTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetMLTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetMLTaskRun,
        stubs: @stubs,
        params_class: Params::GetMLTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_ml_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets a list of runs for a machine learning transform. Machine learning task runs are
    #       asynchronous tasks that Glue runs on your behalf as part of various machine learning
    #       workflows. You can get a sortable, filterable list of machine learning task runs by calling
    #         <code>GetMLTaskRuns</code> with their parent transform's <code>TransformID</code> and other
    #       optional parameters as documented in this section.</p>
    # 	
    # 	        <p>This operation returns a list of historic runs and must be paginated.</p>
    #
    # @param [Hash] params
    #   See {Types::GetMLTaskRunsInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :next_token
    #   <p>A token for pagination of the results. The default is empty.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of results to return. </p>
    #
    # @option params [TaskRunFilterCriteria] :filter
    #   <p>The filter criteria, in the <code>TaskRunFilterCriteria</code> structure, for the task run.</p>
    #
    # @option params [TaskRunSortCriteria] :sort
    #   <p>The sorting criteria, in the <code>TaskRunSortCriteria</code> structure, for the task run.</p>
    #
    # @return [Types::GetMLTaskRunsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_ml_task_runs(
    #     transform_id: 'TransformId', # required
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     filter: {
    #       task_run_type: 'EVALUATION', # accepts ["EVALUATION", "LABELING_SET_GENERATION", "IMPORT_LABELS", "EXPORT_LABELS", "FIND_MATCHES"]
    #       status: 'STARTING', # accepts ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #       started_before: Time.now,
    #       started_after: Time.now
    #     },
    #     sort: {
    #       column: 'TASK_RUN_TYPE', # required - accepts ["TASK_RUN_TYPE", "STATUS", "STARTED"]
    #       sort_direction: 'DESCENDING' # required - accepts ["DESCENDING", "ASCENDING"]
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetMLTaskRunsOutput
    #   resp.data.task_runs #=> Array<TaskRun>
    #   resp.data.task_runs[0] #=> Types::TaskRun
    #   resp.data.task_runs[0].transform_id #=> String
    #   resp.data.task_runs[0].task_run_id #=> String
    #   resp.data.task_runs[0].status #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.task_runs[0].log_group_name #=> String
    #   resp.data.task_runs[0].properties #=> Types::TaskRunProperties
    #   resp.data.task_runs[0].properties.task_type #=> String, one of ["EVALUATION", "LABELING_SET_GENERATION", "IMPORT_LABELS", "EXPORT_LABELS", "FIND_MATCHES"]
    #   resp.data.task_runs[0].properties.import_labels_task_run_properties #=> Types::ImportLabelsTaskRunProperties
    #   resp.data.task_runs[0].properties.import_labels_task_run_properties.input_s3_path #=> String
    #   resp.data.task_runs[0].properties.import_labels_task_run_properties.replace #=> Boolean
    #   resp.data.task_runs[0].properties.export_labels_task_run_properties #=> Types::ExportLabelsTaskRunProperties
    #   resp.data.task_runs[0].properties.export_labels_task_run_properties.output_s3_path #=> String
    #   resp.data.task_runs[0].properties.labeling_set_generation_task_run_properties #=> Types::LabelingSetGenerationTaskRunProperties
    #   resp.data.task_runs[0].properties.labeling_set_generation_task_run_properties.output_s3_path #=> String
    #   resp.data.task_runs[0].properties.find_matches_task_run_properties #=> Types::FindMatchesTaskRunProperties
    #   resp.data.task_runs[0].properties.find_matches_task_run_properties.job_id #=> String
    #   resp.data.task_runs[0].properties.find_matches_task_run_properties.job_name #=> String
    #   resp.data.task_runs[0].properties.find_matches_task_run_properties.job_run_id #=> String
    #   resp.data.task_runs[0].error_string #=> String
    #   resp.data.task_runs[0].started_on #=> Time
    #   resp.data.task_runs[0].last_modified_on #=> Time
    #   resp.data.task_runs[0].completed_on #=> Time
    #   resp.data.task_runs[0].execution_time #=> Integer
    #   resp.data.next_token #=> String
    #
    def get_ml_task_runs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetMLTaskRunsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetMLTaskRunsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetMLTaskRuns
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetMLTaskRuns
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetMLTaskRuns,
        stubs: @stubs,
        params_class: Params::GetMLTaskRunsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_ml_task_runs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets an Glue machine learning transform artifact and all its corresponding metadata.
    #       Machine learning transforms are a special type of transform that use machine learning to learn
    #       the details of the transformation to be performed by learning from examples provided by
    #       humans. These transformations are then saved by Glue. You can retrieve their metadata by
    #       calling <code>GetMLTransform</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetMLTransformInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the transform, generated at the time that the transform was
    #         created.</p>
    #
    # @return [Types::GetMLTransformOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_ml_transform(
    #     transform_id: 'TransformId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetMLTransformOutput
    #   resp.data.transform_id #=> String
    #   resp.data.name #=> String
    #   resp.data.description #=> String
    #   resp.data.status #=> String, one of ["NOT_READY", "READY", "DELETING"]
    #   resp.data.created_on #=> Time
    #   resp.data.last_modified_on #=> Time
    #   resp.data.input_record_tables #=> Array<GlueTable>
    #   resp.data.input_record_tables[0] #=> Types::GlueTable
    #   resp.data.input_record_tables[0].database_name #=> String
    #   resp.data.input_record_tables[0].table_name #=> String
    #   resp.data.input_record_tables[0].catalog_id #=> String
    #   resp.data.input_record_tables[0].connection_name #=> String
    #   resp.data.parameters #=> Types::TransformParameters
    #   resp.data.parameters.transform_type #=> String, one of ["FIND_MATCHES"]
    #   resp.data.parameters.find_matches_parameters #=> Types::FindMatchesParameters
    #   resp.data.parameters.find_matches_parameters.primary_key_column_name #=> String
    #   resp.data.parameters.find_matches_parameters.precision_recall_tradeoff #=> Float
    #   resp.data.parameters.find_matches_parameters.accuracy_cost_tradeoff #=> Float
    #   resp.data.parameters.find_matches_parameters.enforce_provided_labels #=> Boolean
    #   resp.data.evaluation_metrics #=> Types::EvaluationMetrics
    #   resp.data.evaluation_metrics.transform_type #=> String, one of ["FIND_MATCHES"]
    #   resp.data.evaluation_metrics.find_matches_metrics #=> Types::FindMatchesMetrics
    #   resp.data.evaluation_metrics.find_matches_metrics.area_under_pr_curve #=> Float
    #   resp.data.evaluation_metrics.find_matches_metrics.precision #=> Float
    #   resp.data.evaluation_metrics.find_matches_metrics.recall #=> Float
    #   resp.data.evaluation_metrics.find_matches_metrics.f1 #=> Float
    #   resp.data.evaluation_metrics.find_matches_metrics.confusion_matrix #=> Types::ConfusionMatrix
    #   resp.data.evaluation_metrics.find_matches_metrics.confusion_matrix.num_true_positives #=> Integer
    #   resp.data.evaluation_metrics.find_matches_metrics.confusion_matrix.num_false_positives #=> Integer
    #   resp.data.evaluation_metrics.find_matches_metrics.confusion_matrix.num_true_negatives #=> Integer
    #   resp.data.evaluation_metrics.find_matches_metrics.confusion_matrix.num_false_negatives #=> Integer
    #   resp.data.evaluation_metrics.find_matches_metrics.column_importances #=> Array<ColumnImportance>
    #   resp.data.evaluation_metrics.find_matches_metrics.column_importances[0] #=> Types::ColumnImportance
    #   resp.data.evaluation_metrics.find_matches_metrics.column_importances[0].column_name #=> String
    #   resp.data.evaluation_metrics.find_matches_metrics.column_importances[0].importance #=> Float
    #   resp.data.label_count #=> Integer
    #   resp.data.schema #=> Array<SchemaColumn>
    #   resp.data.schema[0] #=> Types::SchemaColumn
    #   resp.data.schema[0].name #=> String
    #   resp.data.schema[0].data_type #=> String
    #   resp.data.role #=> String
    #   resp.data.glue_version #=> String
    #   resp.data.max_capacity #=> Float
    #   resp.data.worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.number_of_workers #=> Integer
    #   resp.data.timeout #=> Integer
    #   resp.data.max_retries #=> Integer
    #   resp.data.transform_encryption #=> Types::TransformEncryption
    #   resp.data.transform_encryption.ml_user_data_encryption #=> Types::MLUserDataEncryption
    #   resp.data.transform_encryption.ml_user_data_encryption.ml_user_data_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS"]
    #   resp.data.transform_encryption.ml_user_data_encryption.kms_key_id #=> String
    #   resp.data.transform_encryption.task_run_security_configuration_name #=> String
    #
    def get_ml_transform(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetMLTransformInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetMLTransformInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetMLTransform
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetMLTransform
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetMLTransform,
        stubs: @stubs,
        params_class: Params::GetMLTransformOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_ml_transform
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets a sortable, filterable list of existing Glue machine learning transforms. Machine
    #       learning transforms are a special type of transform that use machine learning to learn the
    #       details of the transformation to be performed by learning from examples provided by humans.
    #       These transformations are then saved by Glue, and you can retrieve their metadata by
    #       calling <code>GetMLTransforms</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetMLTransformsInput}.
    #
    # @option params [String] :next_token
    #   <p>A paginated token to offset the results.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of results to return.</p>
    #
    # @option params [TransformFilterCriteria] :filter
    #   <p>The filter transformation criteria.</p>
    #
    # @option params [TransformSortCriteria] :sort
    #   <p>The sorting criteria.</p>
    #
    # @return [Types::GetMLTransformsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_ml_transforms(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     filter: {
    #       name: 'Name',
    #       transform_type: 'FIND_MATCHES', # accepts ["FIND_MATCHES"]
    #       status: 'NOT_READY', # accepts ["NOT_READY", "READY", "DELETING"]
    #       glue_version: 'GlueVersion',
    #       created_before: Time.now,
    #       created_after: Time.now,
    #       last_modified_before: Time.now,
    #       last_modified_after: Time.now,
    #       schema: [
    #         {
    #           name: 'Name',
    #           data_type: 'DataType'
    #         }
    #       ]
    #     },
    #     sort: {
    #       column: 'NAME', # required - accepts ["NAME", "TRANSFORM_TYPE", "STATUS", "CREATED", "LAST_MODIFIED"]
    #       sort_direction: 'DESCENDING' # required - accepts ["DESCENDING", "ASCENDING"]
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetMLTransformsOutput
    #   resp.data.transforms #=> Array<MLTransform>
    #   resp.data.transforms[0] #=> Types::MLTransform
    #   resp.data.transforms[0].transform_id #=> String
    #   resp.data.transforms[0].name #=> String
    #   resp.data.transforms[0].description #=> String
    #   resp.data.transforms[0].status #=> String, one of ["NOT_READY", "READY", "DELETING"]
    #   resp.data.transforms[0].created_on #=> Time
    #   resp.data.transforms[0].last_modified_on #=> Time
    #   resp.data.transforms[0].input_record_tables #=> Array<GlueTable>
    #   resp.data.transforms[0].input_record_tables[0] #=> Types::GlueTable
    #   resp.data.transforms[0].input_record_tables[0].database_name #=> String
    #   resp.data.transforms[0].input_record_tables[0].table_name #=> String
    #   resp.data.transforms[0].input_record_tables[0].catalog_id #=> String
    #   resp.data.transforms[0].input_record_tables[0].connection_name #=> String
    #   resp.data.transforms[0].parameters #=> Types::TransformParameters
    #   resp.data.transforms[0].parameters.transform_type #=> String, one of ["FIND_MATCHES"]
    #   resp.data.transforms[0].parameters.find_matches_parameters #=> Types::FindMatchesParameters
    #   resp.data.transforms[0].parameters.find_matches_parameters.primary_key_column_name #=> String
    #   resp.data.transforms[0].parameters.find_matches_parameters.precision_recall_tradeoff #=> Float
    #   resp.data.transforms[0].parameters.find_matches_parameters.accuracy_cost_tradeoff #=> Float
    #   resp.data.transforms[0].parameters.find_matches_parameters.enforce_provided_labels #=> Boolean
    #   resp.data.transforms[0].evaluation_metrics #=> Types::EvaluationMetrics
    #   resp.data.transforms[0].evaluation_metrics.transform_type #=> String, one of ["FIND_MATCHES"]
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics #=> Types::FindMatchesMetrics
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.area_under_pr_curve #=> Float
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.precision #=> Float
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.recall #=> Float
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.f1 #=> Float
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.confusion_matrix #=> Types::ConfusionMatrix
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.confusion_matrix.num_true_positives #=> Integer
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.confusion_matrix.num_false_positives #=> Integer
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.confusion_matrix.num_true_negatives #=> Integer
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.confusion_matrix.num_false_negatives #=> Integer
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.column_importances #=> Array<ColumnImportance>
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.column_importances[0] #=> Types::ColumnImportance
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.column_importances[0].column_name #=> String
    #   resp.data.transforms[0].evaluation_metrics.find_matches_metrics.column_importances[0].importance #=> Float
    #   resp.data.transforms[0].label_count #=> Integer
    #   resp.data.transforms[0].schema #=> Array<SchemaColumn>
    #   resp.data.transforms[0].schema[0] #=> Types::SchemaColumn
    #   resp.data.transforms[0].schema[0].name #=> String
    #   resp.data.transforms[0].schema[0].data_type #=> String
    #   resp.data.transforms[0].role #=> String
    #   resp.data.transforms[0].glue_version #=> String
    #   resp.data.transforms[0].max_capacity #=> Float
    #   resp.data.transforms[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.transforms[0].number_of_workers #=> Integer
    #   resp.data.transforms[0].timeout #=> Integer
    #   resp.data.transforms[0].max_retries #=> Integer
    #   resp.data.transforms[0].transform_encryption #=> Types::TransformEncryption
    #   resp.data.transforms[0].transform_encryption.ml_user_data_encryption #=> Types::MLUserDataEncryption
    #   resp.data.transforms[0].transform_encryption.ml_user_data_encryption.ml_user_data_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS"]
    #   resp.data.transforms[0].transform_encryption.ml_user_data_encryption.kms_key_id #=> String
    #   resp.data.transforms[0].transform_encryption.task_run_security_configuration_name #=> String
    #   resp.data.next_token #=> String
    #
    def get_ml_transforms(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetMLTransformsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetMLTransformsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetMLTransforms
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetMLTransforms
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetMLTransforms,
        stubs: @stubs,
        params_class: Params::GetMLTransformsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_ml_transforms
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates mappings.</p>
    #
    # @param [Hash] params
    #   See {Types::GetMappingInput}.
    #
    # @option params [CatalogEntry] :source
    #   <p>Specifies the source table.</p>
    #
    # @option params [Array<CatalogEntry>] :sinks
    #   <p>A list of target tables.</p>
    #
    # @option params [Location] :location
    #   <p>Parameters for the mapping.</p>
    #
    # @return [Types::GetMappingOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_mapping(
    #     source: {
    #       database_name: 'DatabaseName', # required
    #       table_name: 'TableName' # required
    #     }, # required
    #     location: {
    #       jdbc: [
    #         {
    #           name: 'Name', # required
    #           value: 'Value', # required
    #           param: false
    #         }
    #       ],
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetMappingOutput
    #   resp.data.mapping #=> Array<MappingEntry>
    #   resp.data.mapping[0] #=> Types::MappingEntry
    #   resp.data.mapping[0].source_table #=> String
    #   resp.data.mapping[0].source_path #=> String
    #   resp.data.mapping[0].source_type #=> String
    #   resp.data.mapping[0].target_table #=> String
    #   resp.data.mapping[0].target_path #=> String
    #   resp.data.mapping[0].target_type #=> String
    #
    def get_mapping(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetMappingInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetMappingInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetMapping
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetMapping
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetMapping,
        stubs: @stubs,
        params_class: Params::GetMappingOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_mapping
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves information about a specified partition.</p>
    #
    # @param [Hash] params
    #   See {Types::GetPartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partition in question resides. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partition resides.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partition's table.</p>
    #
    # @option params [Array<String>] :partition_values
    #   <p>The values that define the partition.</p>
    #
    # @return [Types::GetPartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetPartitionOutput
    #   resp.data.partition #=> Types::Partition
    #   resp.data.partition.values #=> Array<String>
    #   resp.data.partition.values[0] #=> String
    #   resp.data.partition.database_name #=> String
    #   resp.data.partition.table_name #=> String
    #   resp.data.partition.creation_time #=> Time
    #   resp.data.partition.last_access_time #=> Time
    #   resp.data.partition.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.partition.storage_descriptor.columns #=> Array<Column>
    #   resp.data.partition.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.partition.storage_descriptor.columns[0].name #=> String
    #   resp.data.partition.storage_descriptor.columns[0].type #=> String
    #   resp.data.partition.storage_descriptor.columns[0].comment #=> String
    #   resp.data.partition.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.partition.storage_descriptor.location #=> String
    #   resp.data.partition.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.partition.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.partition.storage_descriptor.input_format #=> String
    #   resp.data.partition.storage_descriptor.output_format #=> String
    #   resp.data.partition.storage_descriptor.compressed #=> Boolean
    #   resp.data.partition.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.partition.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.partition.storage_descriptor.serde_info.name #=> String
    #   resp.data.partition.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.partition.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.partition.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.partition.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.partition.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.partition.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.partition.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.partition.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.partition.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.partition.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.partition.parameters #=> Hash<String, String>
    #   resp.data.partition.last_analyzed_time #=> Time
    #   resp.data.partition.catalog_id #=> String
    #
    def get_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetPartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetPartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetPartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetPartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetPartition,
        stubs: @stubs,
        params_class: Params::GetPartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the partition indexes associated with a table.</p>
    #
    # @param [Hash] params
    #   See {Types::GetPartitionIndexesInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The catalog ID where the table resides.</p>
    #
    # @option params [String] :database_name
    #   <p>Specifies the name of a database from which you want to retrieve partition indexes.</p>
    #
    # @option params [String] :table_name
    #   <p>Specifies the name of a table for which you want to retrieve the partition indexes.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, included if this is a continuation call.</p>
    #
    # @return [Types::GetPartitionIndexesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_partition_indexes(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetPartitionIndexesOutput
    #   resp.data.partition_index_descriptor_list #=> Array<PartitionIndexDescriptor>
    #   resp.data.partition_index_descriptor_list[0] #=> Types::PartitionIndexDescriptor
    #   resp.data.partition_index_descriptor_list[0].index_name #=> String
    #   resp.data.partition_index_descriptor_list[0].keys #=> Array<KeySchemaElement>
    #   resp.data.partition_index_descriptor_list[0].keys[0] #=> Types::KeySchemaElement
    #   resp.data.partition_index_descriptor_list[0].keys[0].name #=> String
    #   resp.data.partition_index_descriptor_list[0].keys[0].type #=> String
    #   resp.data.partition_index_descriptor_list[0].index_status #=> String, one of ["CREATING", "ACTIVE", "DELETING", "FAILED"]
    #   resp.data.partition_index_descriptor_list[0].backfill_errors #=> Array<BackfillError>
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0] #=> Types::BackfillError
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0].code #=> String, one of ["ENCRYPTED_PARTITION_ERROR", "INTERNAL_ERROR", "INVALID_PARTITION_TYPE_DATA_ERROR", "MISSING_PARTITION_VALUE_ERROR", "UNSUPPORTED_PARTITION_CHARACTER_ERROR"]
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0].partitions #=> Array<PartitionValueList>
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0].partitions[0] #=> Types::PartitionValueList
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0].partitions[0].values #=> Array<String>
    #   resp.data.partition_index_descriptor_list[0].backfill_errors[0].partitions[0].values[0] #=> String
    #   resp.data.next_token #=> String
    #
    def get_partition_indexes(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetPartitionIndexesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetPartitionIndexesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetPartitionIndexes
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::ConflictException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetPartitionIndexes
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetPartitionIndexes,
        stubs: @stubs,
        params_class: Params::GetPartitionIndexesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_partition_indexes
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves information about the partitions in a table.</p>
    #
    # @param [Hash] params
    #   See {Types::GetPartitionsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [String] :expression
    #   <p>An expression that filters the partitions to be returned.</p>
    #            <p>The expression uses SQL syntax similar to the SQL <code>WHERE</code> filter clause. The
    #         SQL statement parser <a href="http://jsqlparser.sourceforge.net/home.php">JSQLParser</a> parses the expression. </p>
    #            <p>
    #               <i>Operators</i>: The following are the operators that you can use in the
    #           <code>Expression</code> API call:</p>
    #            <dl>
    #               <dt>=</dt>
    #               <dd>
    #                  <p>Checks whether the values of the two operands are equal; if yes, then the condition becomes
    #               true.</p>
    #                  <p>Example: Assume 'variable a' holds 10 and 'variable b' holds 20. </p>
    #                  <p>(a = b) is not true.</p>
    #               </dd>
    #               <dt>< ></dt>
    #               <dd>
    #                  <p>Checks whether the values of two operands are equal; if the values are not equal,
    #               then the condition becomes true.</p>
    #                  <p>Example: (a < > b) is true.</p>
    #               </dd>
    #               <dt>></dt>
    #               <dd>
    #                  <p>Checks whether the value of the left operand is greater than the value of the right
    #               operand; if yes, then the condition becomes true.</p>
    #                  <p>Example: (a > b) is not true.</p>
    #               </dd>
    #               <dt><</dt>
    #               <dd>
    #                  <p>Checks whether the value of the left operand is less than the value of the right
    #               operand; if yes, then the condition becomes true.</p>
    #                  <p>Example: (a < b) is true.</p>
    #               </dd>
    #               <dt>>=</dt>
    #               <dd>
    #                  <p>Checks whether the value of the left operand is greater than or equal to the value
    #               of the right operand; if yes, then the condition becomes true.</p>
    #                  <p>Example: (a >= b) is not true.</p>
    #               </dd>
    #               <dt><=</dt>
    #               <dd>
    #                  <p>Checks whether the value of the left operand is less than or equal to the value of
    #               the right operand; if yes, then the condition becomes true.</p>
    #                  <p>Example: (a <= b) is true.</p>
    #               </dd>
    #               <dt>AND, OR, IN, BETWEEN, LIKE, NOT, IS NULL</dt>
    #               <dd>
    #                  <p>Logical operators.</p>
    #               </dd>
    #            </dl>
    #            <p>
    #               <i>Supported Partition Key Types</i>: The following are the supported
    #         partition keys.</p>
    #            <ul>
    #               <li>
    #                  <p>
    #                     <code>string</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>date</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>timestamp</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>int</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>bigint</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>long</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>tinyint</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>smallint</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>
    #                     <code>decimal</code>
    #                  </p>
    #               </li>
    #            </ul>
    #            <p>If an type is encountered that is not valid, an exception is thrown. </p>
    #            <p>The following list shows the valid operators on each type. When you define a crawler, the
    #           <code>partitionKey</code> type is created as a <code>STRING</code>, to be compatible with the catalog
    #         partitions. </p>
    #
    #            <p>
    #               <i>Sample API Call</i>: </p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is not the first call to retrieve
    #         these partitions.</p>
    #
    # @option params [Segment] :segment
    #   <p>The segment of the table's partitions to scan in this request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of partitions to return in a single response.</p>
    #
    # @option params [Boolean] :exclude_column_schema
    #   <p>When true, specifies not returning the partition column schema. Useful when you are interested only in other partition attributes such as partition values or location. This approach avoids the problem of a large response by not returning duplicate data.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to read the partition contents.</p>
    #
    # @option params [Time] :query_as_of_time
    #   <p>The time as of when to read the partition contents. If not set, the most recent transaction commit time will be used. Cannot be specified along with <code>TransactionId</code>.</p>
    #
    # @return [Types::GetPartitionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_partitions(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     expression: 'Expression',
    #     next_token: 'NextToken',
    #     segment: {
    #       segment_number: 1, # required
    #       total_segments: 1 # required
    #     },
    #     max_results: 1,
    #     exclude_column_schema: false,
    #     transaction_id: 'TransactionId',
    #     query_as_of_time: Time.now
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetPartitionsOutput
    #   resp.data.partitions #=> Array<Partition>
    #   resp.data.partitions[0] #=> Types::Partition
    #   resp.data.partitions[0].values #=> Array<String>
    #   resp.data.partitions[0].values[0] #=> String
    #   resp.data.partitions[0].database_name #=> String
    #   resp.data.partitions[0].table_name #=> String
    #   resp.data.partitions[0].creation_time #=> Time
    #   resp.data.partitions[0].last_access_time #=> Time
    #   resp.data.partitions[0].storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.partitions[0].storage_descriptor.columns #=> Array<Column>
    #   resp.data.partitions[0].storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.partitions[0].storage_descriptor.columns[0].name #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].type #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].comment #=> String
    #   resp.data.partitions[0].storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.partitions[0].storage_descriptor.location #=> String
    #   resp.data.partitions[0].storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.additional_locations[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.input_format #=> String
    #   resp.data.partitions[0].storage_descriptor.output_format #=> String
    #   resp.data.partitions[0].storage_descriptor.compressed #=> Boolean
    #   resp.data.partitions[0].storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.partitions[0].storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.partitions[0].storage_descriptor.serde_info.name #=> String
    #   resp.data.partitions[0].storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.partitions[0].storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.partitions[0].storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.partitions[0].storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.partitions[0].storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.partitions[0].storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.partitions[0].storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.partitions[0].storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.partitions[0].parameters #=> Hash<String, String>
    #   resp.data.partitions[0].last_analyzed_time #=> Time
    #   resp.data.partitions[0].catalog_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_partitions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetPartitionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetPartitionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetPartitions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InvalidStateException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetPartitions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetPartitions,
        stubs: @stubs,
        params_class: Params::GetPartitionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_partitions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets code to perform a specified mapping.</p>
    #
    # @param [Hash] params
    #   See {Types::GetPlanInput}.
    #
    # @option params [Array<MappingEntry>] :mapping
    #   <p>The list of mappings from a source table to target tables.</p>
    #
    # @option params [CatalogEntry] :source
    #   <p>The source table.</p>
    #
    # @option params [Array<CatalogEntry>] :sinks
    #   <p>The target tables.</p>
    #
    # @option params [Location] :location
    #   <p>The parameters for the mapping.</p>
    #
    # @option params [String] :language
    #   <p>The programming language of the code to perform the mapping.</p>
    #
    # @option params [Hash<String, String>] :additional_plan_options_map
    #   <p>A map to hold additional optional key-value parameters.</p>
    #   	
    #   	        <p>Currently, these key-value pairs are supported:</p>
    #   	
    #   	        <ul>
    #               <li>
    #                  <p>
    #                     <code>inferSchema</code>  —  Specifies whether to set <code>inferSchema</code> to true or false for the default script generated by an Glue job. For example, to set <code>inferSchema</code> to true, pass the following key value pair:</p>
    #   	              <p>
    #                     <code>--additional-plan-options-map '{"inferSchema":"true"}'</code>
    #                  </p>
    #   	           </li>
    #            </ul>
    #
    # @return [Types::GetPlanOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_plan(
    #     mapping: [
    #       {
    #         source_table: 'SourceTable',
    #         source_path: 'SourcePath',
    #         source_type: 'SourceType',
    #         target_table: 'TargetTable',
    #         target_path: 'TargetPath',
    #         target_type: 'TargetType'
    #       }
    #     ], # required
    #     source: {
    #       database_name: 'DatabaseName', # required
    #       table_name: 'TableName' # required
    #     }, # required
    #     location: {
    #       jdbc: [
    #         {
    #           name: 'Name', # required
    #           value: 'Value', # required
    #           param: false
    #         }
    #       ],
    #     },
    #     language: 'PYTHON', # accepts ["PYTHON", "SCALA"]
    #     additional_plan_options_map: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetPlanOutput
    #   resp.data.python_script #=> String
    #   resp.data.scala_code #=> String
    #
    def get_plan(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetPlanInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetPlanInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetPlan
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetPlan
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetPlan,
        stubs: @stubs,
        params_class: Params::GetPlanOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_plan
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Describes the specified registry in detail.</p>
    #
    # @param [Hash] params
    #   See {Types::GetRegistryInput}.
    #
    # @option params [RegistryId] :registry_id
    #   <p>This is a wrapper structure that may contain the registry name and Amazon Resource Name (ARN).</p>
    #
    # @return [Types::GetRegistryOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_registry(
    #     registry_id: {
    #       registry_name: 'RegistryName',
    #       registry_arn: 'RegistryArn'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetRegistryOutput
    #   resp.data.registry_name #=> String
    #   resp.data.registry_arn #=> String
    #   resp.data.description #=> String
    #   resp.data.status #=> String, one of ["AVAILABLE", "DELETING"]
    #   resp.data.created_time #=> String
    #   resp.data.updated_time #=> String
    #
    def get_registry(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetRegistryInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetRegistryInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetRegistry
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetRegistry
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetRegistry,
        stubs: @stubs,
        params_class: Params::GetRegistryOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_registry
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the resource policies set on individual resources by Resource Access Manager
    #       during cross-account permission grants. Also retrieves the Data Catalog resource
    #       policy.</p>
    #          <p>If you enabled metadata encryption in Data Catalog settings, and you do not have
    #       permission on the KMS key, the operation can't return the Data Catalog resource
    #       policy.</p>
    #
    # @param [Hash] params
    #   See {Types::GetResourcePoliciesInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @return [Types::GetResourcePoliciesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_resource_policies(
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetResourcePoliciesOutput
    #   resp.data.get_resource_policies_response_list #=> Array<GluePolicy>
    #   resp.data.get_resource_policies_response_list[0] #=> Types::GluePolicy
    #   resp.data.get_resource_policies_response_list[0].policy_in_json #=> String
    #   resp.data.get_resource_policies_response_list[0].policy_hash #=> String
    #   resp.data.get_resource_policies_response_list[0].create_time #=> Time
    #   resp.data.get_resource_policies_response_list[0].update_time #=> Time
    #   resp.data.next_token #=> String
    #
    def get_resource_policies(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetResourcePoliciesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetResourcePoliciesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetResourcePolicies
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetResourcePolicies
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetResourcePolicies,
        stubs: @stubs,
        params_class: Params::GetResourcePoliciesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_resource_policies
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a specified resource policy.</p>
    #
    # @param [Hash] params
    #   See {Types::GetResourcePolicyInput}.
    #
    # @option params [String] :resource_arn
    #   <p>The ARN of the Glue resource for which to retrieve the resource policy. If not
    #         supplied, the Data Catalog resource policy is returned. Use <code>GetResourcePolicies</code>
    #         to view all existing resource policies. For more information see <a href="https://docs.aws.amazon.com/glue/latest/dg/glue-specifying-resource-arns.html">Specifying Glue Resource ARNs</a>.
    #       </p>
    #
    # @return [Types::GetResourcePolicyOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_resource_policy(
    #     resource_arn: 'ResourceArn'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetResourcePolicyOutput
    #   resp.data.policy_in_json #=> String
    #   resp.data.policy_hash #=> String
    #   resp.data.create_time #=> Time
    #   resp.data.update_time #=> Time
    #
    def get_resource_policy(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetResourcePolicyInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetResourcePolicyInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetResourcePolicy
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetResourcePolicy
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetResourcePolicy,
        stubs: @stubs,
        params_class: Params::GetResourcePolicyOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_resource_policy
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Describes the specified schema in detail.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSchemaInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @return [Types::GetSchemaOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_schema(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSchemaOutput
    #   resp.data.registry_name #=> String
    #   resp.data.registry_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.schema_arn #=> String
    #   resp.data.description #=> String
    #   resp.data.data_format #=> String, one of ["AVRO", "JSON", "PROTOBUF"]
    #   resp.data.compatibility #=> String, one of ["NONE", "DISABLED", "BACKWARD", "BACKWARD_ALL", "FORWARD", "FORWARD_ALL", "FULL", "FULL_ALL"]
    #   resp.data.schema_checkpoint #=> Integer
    #   resp.data.latest_schema_version #=> Integer
    #   resp.data.next_schema_version #=> Integer
    #   resp.data.schema_status #=> String, one of ["AVAILABLE", "PENDING", "DELETING"]
    #   resp.data.created_time #=> String
    #   resp.data.updated_time #=> String
    #
    def get_schema(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSchemaInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSchemaInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSchema
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetSchema
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSchema,
        stubs: @stubs,
        params_class: Params::GetSchemaOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_schema
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a schema by the <code>SchemaDefinition</code>. The schema definition is sent to the Schema Registry, canonicalized, and hashed. If the hash is matched within the scope of the <code>SchemaName</code> or ARN (or the default registry, if none is supplied), that schema’s metadata is returned. Otherwise, a 404 or NotFound error is returned. Schema versions in <code>Deleted</code> statuses will not be included in the results.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSchemaByDefinitionInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :schema_definition
    #   <p>The definition of the schema for which schema details are required.</p>
    #
    # @return [Types::GetSchemaByDefinitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_schema_by_definition(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     schema_definition: 'SchemaDefinition' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSchemaByDefinitionOutput
    #   resp.data.schema_version_id #=> String
    #   resp.data.schema_arn #=> String
    #   resp.data.data_format #=> String, one of ["AVRO", "JSON", "PROTOBUF"]
    #   resp.data.status #=> String, one of ["AVAILABLE", "PENDING", "FAILURE", "DELETING"]
    #   resp.data.created_time #=> String
    #
    def get_schema_by_definition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSchemaByDefinitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSchemaByDefinitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSchemaByDefinition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetSchemaByDefinition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSchemaByDefinition,
        stubs: @stubs,
        params_class: Params::GetSchemaByDefinitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_schema_by_definition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Get the specified schema by its unique ID assigned when a version of the schema is created or registered. Schema versions in Deleted status will not be included in the results.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSchemaVersionInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :schema_version_id
    #   <p>The <code>SchemaVersionId</code> of the schema version. This field is required for fetching by schema ID. Either this or the <code>SchemaId</code> wrapper has to be provided.</p>
    #
    # @option params [SchemaVersionNumber] :schema_version_number
    #   <p>The version number of the schema.</p>
    #
    # @return [Types::GetSchemaVersionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_schema_version(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     },
    #     schema_version_id: 'SchemaVersionId',
    #     schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSchemaVersionOutput
    #   resp.data.schema_version_id #=> String
    #   resp.data.schema_definition #=> String
    #   resp.data.data_format #=> String, one of ["AVRO", "JSON", "PROTOBUF"]
    #   resp.data.schema_arn #=> String
    #   resp.data.version_number #=> Integer
    #   resp.data.status #=> String, one of ["AVAILABLE", "PENDING", "FAILURE", "DELETING"]
    #   resp.data.created_time #=> String
    #
    def get_schema_version(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSchemaVersionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSchemaVersionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSchemaVersion
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetSchemaVersion
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSchemaVersion,
        stubs: @stubs,
        params_class: Params::GetSchemaVersionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_schema_version
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Fetches the schema version difference in the specified difference type between two stored schema versions in the Schema Registry.</p>
    #          <p>This API allows you to compare two schema versions between two schema definitions under the same schema.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSchemaVersionsDiffInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [SchemaVersionNumber] :first_schema_version_number
    #   <p>The first of the two schema versions to be compared.</p>
    #
    # @option params [SchemaVersionNumber] :second_schema_version_number
    #   <p>The second of the two schema versions to be compared.</p>
    #
    # @option params [String] :schema_diff_type
    #   <p>Refers to <code>SYNTAX_DIFF</code>, which is the currently supported diff type.</p>
    #
    # @return [Types::GetSchemaVersionsDiffOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_schema_versions_diff(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     first_schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     }, # required
    #     schema_diff_type: 'SYNTAX_DIFF' # required - accepts ["SYNTAX_DIFF"]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSchemaVersionsDiffOutput
    #   resp.data.diff #=> String
    #
    def get_schema_versions_diff(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSchemaVersionsDiffInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSchemaVersionsDiffInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSchemaVersionsDiff
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetSchemaVersionsDiff
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSchemaVersionsDiff,
        stubs: @stubs,
        params_class: Params::GetSchemaVersionsDiffOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_schema_versions_diff
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a specified security configuration.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSecurityConfigurationInput}.
    #
    # @option params [String] :name
    #   <p>The name of the security configuration to retrieve.</p>
    #
    # @return [Types::GetSecurityConfigurationOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_security_configuration(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSecurityConfigurationOutput
    #   resp.data.security_configuration #=> Types::SecurityConfiguration
    #   resp.data.security_configuration.name #=> String
    #   resp.data.security_configuration.created_time_stamp #=> Time
    #   resp.data.security_configuration.encryption_configuration #=> Types::EncryptionConfiguration
    #   resp.data.security_configuration.encryption_configuration.s3_encryption #=> Array<S3Encryption>
    #   resp.data.security_configuration.encryption_configuration.s3_encryption[0] #=> Types::S3Encryption
    #   resp.data.security_configuration.encryption_configuration.s3_encryption[0].s3_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS", "SSE-S3"]
    #   resp.data.security_configuration.encryption_configuration.s3_encryption[0].kms_key_arn #=> String
    #   resp.data.security_configuration.encryption_configuration.cloud_watch_encryption #=> Types::CloudWatchEncryption
    #   resp.data.security_configuration.encryption_configuration.cloud_watch_encryption.cloud_watch_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS"]
    #   resp.data.security_configuration.encryption_configuration.cloud_watch_encryption.kms_key_arn #=> String
    #   resp.data.security_configuration.encryption_configuration.job_bookmarks_encryption #=> Types::JobBookmarksEncryption
    #   resp.data.security_configuration.encryption_configuration.job_bookmarks_encryption.job_bookmarks_encryption_mode #=> String, one of ["DISABLED", "CSE-KMS"]
    #   resp.data.security_configuration.encryption_configuration.job_bookmarks_encryption.kms_key_arn #=> String
    #
    def get_security_configuration(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSecurityConfigurationInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSecurityConfigurationInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSecurityConfiguration
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetSecurityConfiguration
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSecurityConfiguration,
        stubs: @stubs,
        params_class: Params::GetSecurityConfigurationOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_security_configuration
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a list of all security configurations.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSecurityConfigurationsInput}.
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of results to return.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::GetSecurityConfigurationsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_security_configurations(
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSecurityConfigurationsOutput
    #   resp.data.security_configurations #=> Array<SecurityConfiguration>
    #   resp.data.security_configurations[0] #=> Types::SecurityConfiguration
    #   resp.data.security_configurations[0].name #=> String
    #   resp.data.security_configurations[0].created_time_stamp #=> Time
    #   resp.data.security_configurations[0].encryption_configuration #=> Types::EncryptionConfiguration
    #   resp.data.security_configurations[0].encryption_configuration.s3_encryption #=> Array<S3Encryption>
    #   resp.data.security_configurations[0].encryption_configuration.s3_encryption[0] #=> Types::S3Encryption
    #   resp.data.security_configurations[0].encryption_configuration.s3_encryption[0].s3_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS", "SSE-S3"]
    #   resp.data.security_configurations[0].encryption_configuration.s3_encryption[0].kms_key_arn #=> String
    #   resp.data.security_configurations[0].encryption_configuration.cloud_watch_encryption #=> Types::CloudWatchEncryption
    #   resp.data.security_configurations[0].encryption_configuration.cloud_watch_encryption.cloud_watch_encryption_mode #=> String, one of ["DISABLED", "SSE-KMS"]
    #   resp.data.security_configurations[0].encryption_configuration.cloud_watch_encryption.kms_key_arn #=> String
    #   resp.data.security_configurations[0].encryption_configuration.job_bookmarks_encryption #=> Types::JobBookmarksEncryption
    #   resp.data.security_configurations[0].encryption_configuration.job_bookmarks_encryption.job_bookmarks_encryption_mode #=> String, one of ["DISABLED", "CSE-KMS"]
    #   resp.data.security_configurations[0].encryption_configuration.job_bookmarks_encryption.kms_key_arn #=> String
    #   resp.data.next_token #=> String
    #
    def get_security_configurations(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSecurityConfigurationsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSecurityConfigurationsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSecurityConfigurations
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetSecurityConfigurations
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSecurityConfigurations,
        stubs: @stubs,
        params_class: Params::GetSecurityConfigurationsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_security_configurations
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the session.</p>
    #
    # @param [Hash] params
    #   See {Types::GetSessionInput}.
    #
    # @option params [String] :id
    #   <p>The ID of the session. </p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request. </p>
    #
    # @return [Types::GetSessionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_session(
    #     id: 'Id', # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetSessionOutput
    #   resp.data.session #=> Types::Session
    #   resp.data.session.id #=> String
    #   resp.data.session.created_on #=> Time
    #   resp.data.session.status #=> String, one of ["PROVISIONING", "READY", "FAILED", "TIMEOUT", "STOPPING", "STOPPED"]
    #   resp.data.session.error_message #=> String
    #   resp.data.session.description #=> String
    #   resp.data.session.role #=> String
    #   resp.data.session.command #=> Types::SessionCommand
    #   resp.data.session.command.name #=> String
    #   resp.data.session.command.python_version #=> String
    #   resp.data.session.default_arguments #=> Hash<String, String>
    #   resp.data.session.default_arguments['key'] #=> String
    #   resp.data.session.connections #=> Types::ConnectionsList
    #   resp.data.session.connections.connections #=> Array<String>
    #   resp.data.session.connections.connections[0] #=> String
    #   resp.data.session.progress #=> Float
    #   resp.data.session.max_capacity #=> Float
    #   resp.data.session.security_configuration #=> String
    #   resp.data.session.glue_version #=> String
    #
    def get_session(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetSessionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetSessionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetSession
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetSession
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetSession,
        stubs: @stubs,
        params_class: Params::GetSessionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_session
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the statement.</p>
    #
    # @param [Hash] params
    #   See {Types::GetStatementInput}.
    #
    # @option params [String] :session_id
    #   <p>The Session ID of the statement.</p>
    #
    # @option params [Integer] :id
    #   <p>The Id of the statement.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request.</p>
    #
    # @return [Types::GetStatementOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_statement(
    #     session_id: 'SessionId', # required
    #     id: 1, # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetStatementOutput
    #   resp.data.statement #=> Types::Statement
    #   resp.data.statement.id #=> Integer
    #   resp.data.statement.code #=> String
    #   resp.data.statement.state #=> String, one of ["WAITING", "RUNNING", "AVAILABLE", "CANCELLING", "CANCELLED", "ERROR"]
    #   resp.data.statement.output #=> Types::StatementOutput
    #   resp.data.statement.output.data #=> Types::StatementOutputData
    #   resp.data.statement.output.data.text_plain #=> String
    #   resp.data.statement.output.execution_count #=> Integer
    #   resp.data.statement.output.status #=> String, one of ["WAITING", "RUNNING", "AVAILABLE", "CANCELLING", "CANCELLED", "ERROR"]
    #   resp.data.statement.output.error_name #=> String
    #   resp.data.statement.output.error_value #=> String
    #   resp.data.statement.output.traceback #=> Array<String>
    #   resp.data.statement.output.traceback[0] #=> String
    #   resp.data.statement.progress #=> Float
    #   resp.data.statement.started_on #=> Integer
    #   resp.data.statement.completed_on #=> Integer
    #
    def get_statement(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetStatementInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetStatementInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetStatement
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::GetStatement
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetStatement,
        stubs: @stubs,
        params_class: Params::GetStatementOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_statement
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the <code>Table</code> definition in a Data Catalog for
    #       a specified table.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the table resides. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the database in the catalog in which the table resides.
    #         For Hive compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :name
    #   <p>The name of the table for which to retrieve the definition. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to read the table contents. </p>
    #
    # @option params [Time] :query_as_of_time
    #   <p>The time as of when to read the table contents. If not set, the most recent transaction commit time will be used. Cannot be specified along with <code>TransactionId</code>.</p>
    #
    # @return [Types::GetTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     name: 'Name', # required
    #     transaction_id: 'TransactionId',
    #     query_as_of_time: Time.now
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTableOutput
    #   resp.data.table #=> Types::Table
    #   resp.data.table.name #=> String
    #   resp.data.table.database_name #=> String
    #   resp.data.table.description #=> String
    #   resp.data.table.owner #=> String
    #   resp.data.table.create_time #=> Time
    #   resp.data.table.update_time #=> Time
    #   resp.data.table.last_access_time #=> Time
    #   resp.data.table.last_analyzed_time #=> Time
    #   resp.data.table.retention #=> Integer
    #   resp.data.table.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table.storage_descriptor.columns #=> Array<Column>
    #   resp.data.table.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table.storage_descriptor.columns[0].name #=> String
    #   resp.data.table.storage_descriptor.columns[0].type #=> String
    #   resp.data.table.storage_descriptor.columns[0].comment #=> String
    #   resp.data.table.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table.storage_descriptor.location #=> String
    #   resp.data.table.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table.storage_descriptor.input_format #=> String
    #   resp.data.table.storage_descriptor.output_format #=> String
    #   resp.data.table.storage_descriptor.compressed #=> Boolean
    #   resp.data.table.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table.storage_descriptor.serde_info.name #=> String
    #   resp.data.table.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table.partition_keys #=> Array<Column>
    #   resp.data.table.view_original_text #=> String
    #   resp.data.table.view_expanded_text #=> String
    #   resp.data.table.table_type #=> String
    #   resp.data.table.parameters #=> Hash<String, String>
    #   resp.data.table.created_by #=> String
    #   resp.data.table.is_registered_with_lake_formation #=> Boolean
    #   resp.data.table.target_table #=> Types::TableIdentifier
    #   resp.data.table.target_table.catalog_id #=> String
    #   resp.data.table.target_table.database_name #=> String
    #   resp.data.table.target_table.name #=> String
    #   resp.data.table.catalog_id #=> String
    #   resp.data.table.version_id #=> String
    #
    def get_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTable,
        stubs: @stubs,
        params_class: Params::GetTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a specified version of a table.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTableVersionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the tables reside. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The database in the catalog in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table. For Hive compatibility,
    #         this name is entirely lowercase.</p>
    #
    # @option params [String] :version_id
    #   <p>The ID value of the table version to be retrieved. A <code>VersionID</code> is a string representation of an integer. Each version is incremented by 1. </p>
    #
    # @return [Types::GetTableVersionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_table_version(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     version_id: 'VersionId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTableVersionOutput
    #   resp.data.table_version #=> Types::TableVersion
    #   resp.data.table_version.table #=> Types::Table
    #   resp.data.table_version.table.name #=> String
    #   resp.data.table_version.table.database_name #=> String
    #   resp.data.table_version.table.description #=> String
    #   resp.data.table_version.table.owner #=> String
    #   resp.data.table_version.table.create_time #=> Time
    #   resp.data.table_version.table.update_time #=> Time
    #   resp.data.table_version.table.last_access_time #=> Time
    #   resp.data.table_version.table.last_analyzed_time #=> Time
    #   resp.data.table_version.table.retention #=> Integer
    #   resp.data.table_version.table.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table_version.table.storage_descriptor.columns #=> Array<Column>
    #   resp.data.table_version.table.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table_version.table.storage_descriptor.columns[0].name #=> String
    #   resp.data.table_version.table.storage_descriptor.columns[0].type #=> String
    #   resp.data.table_version.table.storage_descriptor.columns[0].comment #=> String
    #   resp.data.table_version.table.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table_version.table.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table_version.table.storage_descriptor.location #=> String
    #   resp.data.table_version.table.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table_version.table.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table_version.table.storage_descriptor.input_format #=> String
    #   resp.data.table_version.table.storage_descriptor.output_format #=> String
    #   resp.data.table_version.table.storage_descriptor.compressed #=> Boolean
    #   resp.data.table_version.table.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table_version.table.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table_version.table.storage_descriptor.serde_info.name #=> String
    #   resp.data.table_version.table.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table_version.table.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table_version.table.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table_version.table.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table_version.table.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table_version.table.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table_version.table.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table_version.table.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table_version.table.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table_version.table.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table_version.table.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table_version.table.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table_version.table.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table_version.table.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table_version.table.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table_version.table.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table_version.table.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table_version.table.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table_version.table.partition_keys #=> Array<Column>
    #   resp.data.table_version.table.view_original_text #=> String
    #   resp.data.table_version.table.view_expanded_text #=> String
    #   resp.data.table_version.table.table_type #=> String
    #   resp.data.table_version.table.parameters #=> Hash<String, String>
    #   resp.data.table_version.table.created_by #=> String
    #   resp.data.table_version.table.is_registered_with_lake_formation #=> Boolean
    #   resp.data.table_version.table.target_table #=> Types::TableIdentifier
    #   resp.data.table_version.table.target_table.catalog_id #=> String
    #   resp.data.table_version.table.target_table.database_name #=> String
    #   resp.data.table_version.table.target_table.name #=> String
    #   resp.data.table_version.table.catalog_id #=> String
    #   resp.data.table_version.table.version_id #=> String
    #   resp.data.table_version.version_id #=> String
    #
    def get_table_version(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTableVersionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTableVersionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTableVersion
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTableVersion
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTableVersion,
        stubs: @stubs,
        params_class: Params::GetTableVersionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_table_version
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a list of strings that identify available versions of
    #       a specified table.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTableVersionsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the tables reside. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The database in the catalog in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is not the first call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of table versions to return in one response.</p>
    #
    # @return [Types::GetTableVersionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_table_versions(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTableVersionsOutput
    #   resp.data.table_versions #=> Array<TableVersion>
    #   resp.data.table_versions[0] #=> Types::TableVersion
    #   resp.data.table_versions[0].table #=> Types::Table
    #   resp.data.table_versions[0].table.name #=> String
    #   resp.data.table_versions[0].table.database_name #=> String
    #   resp.data.table_versions[0].table.description #=> String
    #   resp.data.table_versions[0].table.owner #=> String
    #   resp.data.table_versions[0].table.create_time #=> Time
    #   resp.data.table_versions[0].table.update_time #=> Time
    #   resp.data.table_versions[0].table.last_access_time #=> Time
    #   resp.data.table_versions[0].table.last_analyzed_time #=> Time
    #   resp.data.table_versions[0].table.retention #=> Integer
    #   resp.data.table_versions[0].table.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table_versions[0].table.storage_descriptor.columns #=> Array<Column>
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0].name #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0].type #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0].comment #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table_versions[0].table.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.location #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table_versions[0].table.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.input_format #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.output_format #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.compressed #=> Boolean
    #   resp.data.table_versions[0].table.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table_versions[0].table.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table_versions[0].table.storage_descriptor.serde_info.name #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table_versions[0].table.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table_versions[0].table.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table_versions[0].table.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table_versions[0].table.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table_versions[0].table.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table_versions[0].table.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table_versions[0].table.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table_versions[0].table.partition_keys #=> Array<Column>
    #   resp.data.table_versions[0].table.view_original_text #=> String
    #   resp.data.table_versions[0].table.view_expanded_text #=> String
    #   resp.data.table_versions[0].table.table_type #=> String
    #   resp.data.table_versions[0].table.parameters #=> Hash<String, String>
    #   resp.data.table_versions[0].table.created_by #=> String
    #   resp.data.table_versions[0].table.is_registered_with_lake_formation #=> Boolean
    #   resp.data.table_versions[0].table.target_table #=> Types::TableIdentifier
    #   resp.data.table_versions[0].table.target_table.catalog_id #=> String
    #   resp.data.table_versions[0].table.target_table.database_name #=> String
    #   resp.data.table_versions[0].table.target_table.name #=> String
    #   resp.data.table_versions[0].table.catalog_id #=> String
    #   resp.data.table_versions[0].table.version_id #=> String
    #   resp.data.table_versions[0].version_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_table_versions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTableVersionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTableVersionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTableVersions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTableVersions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTableVersions,
        stubs: @stubs,
        params_class: Params::GetTableVersionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_table_versions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the definitions of some or all of the tables in a given
    #       <code>Database</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTablesInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the tables reside. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The database in the catalog whose tables to list. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [String] :expression
    #   <p>A regular expression pattern. If present, only those tables
    #         whose names match the pattern are returned.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, included if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of tables to return in a single response.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to read the table contents.</p>
    #
    # @option params [Time] :query_as_of_time
    #   <p>The time as of when to read the table contents. If not set, the most recent transaction commit time will be used. Cannot be specified along with <code>TransactionId</code>.</p>
    #
    # @return [Types::GetTablesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_tables(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     expression: 'Expression',
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     transaction_id: 'TransactionId',
    #     query_as_of_time: Time.now
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTablesOutput
    #   resp.data.table_list #=> Array<Table>
    #   resp.data.table_list[0] #=> Types::Table
    #   resp.data.table_list[0].name #=> String
    #   resp.data.table_list[0].database_name #=> String
    #   resp.data.table_list[0].description #=> String
    #   resp.data.table_list[0].owner #=> String
    #   resp.data.table_list[0].create_time #=> Time
    #   resp.data.table_list[0].update_time #=> Time
    #   resp.data.table_list[0].last_access_time #=> Time
    #   resp.data.table_list[0].last_analyzed_time #=> Time
    #   resp.data.table_list[0].retention #=> Integer
    #   resp.data.table_list[0].storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table_list[0].storage_descriptor.columns #=> Array<Column>
    #   resp.data.table_list[0].storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table_list[0].storage_descriptor.columns[0].name #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].type #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].comment #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table_list[0].storage_descriptor.location #=> String
    #   resp.data.table_list[0].storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.input_format #=> String
    #   resp.data.table_list[0].storage_descriptor.output_format #=> String
    #   resp.data.table_list[0].storage_descriptor.compressed #=> Boolean
    #   resp.data.table_list[0].storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table_list[0].storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table_list[0].storage_descriptor.serde_info.name #=> String
    #   resp.data.table_list[0].storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table_list[0].storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table_list[0].storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table_list[0].storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table_list[0].storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table_list[0].partition_keys #=> Array<Column>
    #   resp.data.table_list[0].view_original_text #=> String
    #   resp.data.table_list[0].view_expanded_text #=> String
    #   resp.data.table_list[0].table_type #=> String
    #   resp.data.table_list[0].parameters #=> Hash<String, String>
    #   resp.data.table_list[0].created_by #=> String
    #   resp.data.table_list[0].is_registered_with_lake_formation #=> Boolean
    #   resp.data.table_list[0].target_table #=> Types::TableIdentifier
    #   resp.data.table_list[0].target_table.catalog_id #=> String
    #   resp.data.table_list[0].target_table.database_name #=> String
    #   resp.data.table_list[0].target_table.name #=> String
    #   resp.data.table_list[0].catalog_id #=> String
    #   resp.data.table_list[0].version_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_tables(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTablesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTablesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTables
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTables
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTables,
        stubs: @stubs,
        params_class: Params::GetTablesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_tables
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a list of tags associated with a resource.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTagsInput}.
    #
    # @option params [String] :resource_arn
    #   <p>The Amazon Resource Name (ARN) of the resource for which to retrieve tags.</p>
    #
    # @return [Types::GetTagsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_tags(
    #     resource_arn: 'ResourceArn' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTagsOutput
    #   resp.data.tags #=> Hash<String, String>
    #   resp.data.tags['key'] #=> String
    #
    def get_tags(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTagsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTagsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTags
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTags
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTags,
        stubs: @stubs,
        params_class: Params::GetTagsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_tags
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the definition of a trigger.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger to retrieve.</p>
    #
    # @return [Types::GetTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_trigger(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTriggerOutput
    #   resp.data.trigger #=> Types::Trigger
    #   resp.data.trigger.name #=> String
    #   resp.data.trigger.workflow_name #=> String
    #   resp.data.trigger.id #=> String
    #   resp.data.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.trigger.description #=> String
    #   resp.data.trigger.schedule #=> String
    #   resp.data.trigger.actions #=> Array<Action>
    #   resp.data.trigger.actions[0] #=> Types::Action
    #   resp.data.trigger.actions[0].job_name #=> String
    #   resp.data.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.trigger.actions[0].timeout #=> Integer
    #   resp.data.trigger.actions[0].security_configuration #=> String
    #   resp.data.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.trigger.actions[0].crawler_name #=> String
    #   resp.data.trigger.predicate #=> Types::Predicate
    #   resp.data.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.trigger.event_batching_condition.batch_window #=> Integer
    #
    def get_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTrigger,
        stubs: @stubs,
        params_class: Params::GetTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Gets all the triggers associated with a job.</p>
    #
    # @param [Hash] params
    #   See {Types::GetTriggersInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [String] :dependent_job_name
    #   <p>The name of the job to retrieve triggers for. The trigger that can start this job is
    #         returned, and if there is no such trigger, all triggers are returned.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of the response.</p>
    #
    # @return [Types::GetTriggersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_triggers(
    #     next_token: 'NextToken',
    #     dependent_job_name: 'DependentJobName',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetTriggersOutput
    #   resp.data.triggers #=> Array<Trigger>
    #   resp.data.triggers[0] #=> Types::Trigger
    #   resp.data.triggers[0].name #=> String
    #   resp.data.triggers[0].workflow_name #=> String
    #   resp.data.triggers[0].id #=> String
    #   resp.data.triggers[0].type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.triggers[0].state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.triggers[0].description #=> String
    #   resp.data.triggers[0].schedule #=> String
    #   resp.data.triggers[0].actions #=> Array<Action>
    #   resp.data.triggers[0].actions[0] #=> Types::Action
    #   resp.data.triggers[0].actions[0].job_name #=> String
    #   resp.data.triggers[0].actions[0].arguments #=> Hash<String, String>
    #   resp.data.triggers[0].actions[0].arguments['key'] #=> String
    #   resp.data.triggers[0].actions[0].timeout #=> Integer
    #   resp.data.triggers[0].actions[0].security_configuration #=> String
    #   resp.data.triggers[0].actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.triggers[0].actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.triggers[0].actions[0].crawler_name #=> String
    #   resp.data.triggers[0].predicate #=> Types::Predicate
    #   resp.data.triggers[0].predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.triggers[0].predicate.conditions #=> Array<Condition>
    #   resp.data.triggers[0].predicate.conditions[0] #=> Types::Condition
    #   resp.data.triggers[0].predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.triggers[0].predicate.conditions[0].job_name #=> String
    #   resp.data.triggers[0].predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.triggers[0].predicate.conditions[0].crawler_name #=> String
    #   resp.data.triggers[0].predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.triggers[0].event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.triggers[0].event_batching_condition.batch_size #=> Integer
    #   resp.data.triggers[0].event_batching_condition.batch_window #=> Integer
    #   resp.data.next_token #=> String
    #
    def get_triggers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetTriggersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetTriggersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetTriggers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetTriggers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetTriggers,
        stubs: @stubs,
        params_class: Params::GetTriggersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_triggers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # @param [Hash] params
    #   See {Types::GetUnfilteredPartitionMetadataInput}.
    #
    # @option params [AuditContext] :audit_context
    #   <p>A structure containing information for audit.</p>
    #
    # @return [Types::GetUnfilteredPartitionMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_unfiltered_partition_metadata(
    #     catalog_id: 'CatalogId', # required
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ], # required
    #     audit_context: {
    #       additional_audit_context: 'AdditionalAuditContext',
    #       requested_columns: [
    #         'member'
    #       ],
    #       all_columns_requested: false
    #     },
    #     supported_permission_types: [
    #       'COLUMN_PERMISSION' # accepts ["COLUMN_PERMISSION", "CELL_FILTER_PERMISSION"]
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetUnfilteredPartitionMetadataOutput
    #   resp.data.partition #=> Types::Partition
    #   resp.data.partition.values #=> Array<String>
    #   resp.data.partition.values[0] #=> String
    #   resp.data.partition.database_name #=> String
    #   resp.data.partition.table_name #=> String
    #   resp.data.partition.creation_time #=> Time
    #   resp.data.partition.last_access_time #=> Time
    #   resp.data.partition.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.partition.storage_descriptor.columns #=> Array<Column>
    #   resp.data.partition.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.partition.storage_descriptor.columns[0].name #=> String
    #   resp.data.partition.storage_descriptor.columns[0].type #=> String
    #   resp.data.partition.storage_descriptor.columns[0].comment #=> String
    #   resp.data.partition.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.partition.storage_descriptor.location #=> String
    #   resp.data.partition.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.partition.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.partition.storage_descriptor.input_format #=> String
    #   resp.data.partition.storage_descriptor.output_format #=> String
    #   resp.data.partition.storage_descriptor.compressed #=> Boolean
    #   resp.data.partition.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.partition.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.partition.storage_descriptor.serde_info.name #=> String
    #   resp.data.partition.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.partition.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.partition.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.partition.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.partition.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.partition.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.partition.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.partition.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.partition.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.partition.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.partition.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.partition.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.partition.parameters #=> Hash<String, String>
    #   resp.data.partition.last_analyzed_time #=> Time
    #   resp.data.partition.catalog_id #=> String
    #   resp.data.authorized_columns #=> Array<String>
    #   resp.data.is_registered_with_lake_formation #=> Boolean
    #
    def get_unfiltered_partition_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetUnfilteredPartitionMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetUnfilteredPartitionMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetUnfilteredPartitionMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::PermissionTypeMismatchException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetUnfilteredPartitionMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetUnfilteredPartitionMetadata,
        stubs: @stubs,
        params_class: Params::GetUnfilteredPartitionMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_unfiltered_partition_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # @param [Hash] params
    #   See {Types::GetUnfilteredPartitionsMetadataInput}.
    #
    # @option params [AuditContext] :audit_context
    #   <p>A structure containing information for audit.</p>
    #
    # @option params [Segment] :segment
    #   <p>Defines a non-overlapping region of a table's partitions, allowing
    #         multiple requests to be run in parallel.</p>
    #
    # @return [Types::GetUnfilteredPartitionsMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_unfiltered_partitions_metadata(
    #     catalog_id: 'CatalogId', # required
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     expression: 'Expression',
    #     audit_context: {
    #       additional_audit_context: 'AdditionalAuditContext',
    #       requested_columns: [
    #         'member'
    #       ],
    #       all_columns_requested: false
    #     },
    #     supported_permission_types: [
    #       'COLUMN_PERMISSION' # accepts ["COLUMN_PERMISSION", "CELL_FILTER_PERMISSION"]
    #     ], # required
    #     next_token: 'NextToken',
    #     segment: {
    #       segment_number: 1, # required
    #       total_segments: 1 # required
    #     },
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetUnfilteredPartitionsMetadataOutput
    #   resp.data.unfiltered_partitions #=> Array<UnfilteredPartition>
    #   resp.data.unfiltered_partitions[0] #=> Types::UnfilteredPartition
    #   resp.data.unfiltered_partitions[0].partition #=> Types::Partition
    #   resp.data.unfiltered_partitions[0].partition.values #=> Array<String>
    #   resp.data.unfiltered_partitions[0].partition.values[0] #=> String
    #   resp.data.unfiltered_partitions[0].partition.database_name #=> String
    #   resp.data.unfiltered_partitions[0].partition.table_name #=> String
    #   resp.data.unfiltered_partitions[0].partition.creation_time #=> Time
    #   resp.data.unfiltered_partitions[0].partition.last_access_time #=> Time
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns #=> Array<Column>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0].name #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0].type #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0].comment #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.location #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.input_format #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.output_format #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.compressed #=> Boolean
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.serde_info.name #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.unfiltered_partitions[0].partition.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.unfiltered_partitions[0].partition.parameters #=> Hash<String, String>
    #   resp.data.unfiltered_partitions[0].partition.last_analyzed_time #=> Time
    #   resp.data.unfiltered_partitions[0].partition.catalog_id #=> String
    #   resp.data.unfiltered_partitions[0].authorized_columns #=> Array<String>
    #   resp.data.unfiltered_partitions[0].is_registered_with_lake_formation #=> Boolean
    #   resp.data.next_token #=> String
    #
    def get_unfiltered_partitions_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetUnfilteredPartitionsMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetUnfilteredPartitionsMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetUnfilteredPartitionsMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::PermissionTypeMismatchException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetUnfilteredPartitionsMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetUnfilteredPartitionsMetadata,
        stubs: @stubs,
        params_class: Params::GetUnfilteredPartitionsMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_unfiltered_partitions_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # @param [Hash] params
    #   See {Types::GetUnfilteredTableMetadataInput}.
    #
    # @option params [AuditContext] :audit_context
    #   <p>A structure containing information for audit.</p>
    #
    # @return [Types::GetUnfilteredTableMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_unfiltered_table_metadata(
    #     catalog_id: 'CatalogId', # required
    #     database_name: 'DatabaseName', # required
    #     name: 'Name', # required
    #     audit_context: {
    #       additional_audit_context: 'AdditionalAuditContext',
    #       requested_columns: [
    #         'member'
    #       ],
    #       all_columns_requested: false
    #     },
    #     supported_permission_types: [
    #       'COLUMN_PERMISSION' # accepts ["COLUMN_PERMISSION", "CELL_FILTER_PERMISSION"]
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetUnfilteredTableMetadataOutput
    #   resp.data.table #=> Types::Table
    #   resp.data.table.name #=> String
    #   resp.data.table.database_name #=> String
    #   resp.data.table.description #=> String
    #   resp.data.table.owner #=> String
    #   resp.data.table.create_time #=> Time
    #   resp.data.table.update_time #=> Time
    #   resp.data.table.last_access_time #=> Time
    #   resp.data.table.last_analyzed_time #=> Time
    #   resp.data.table.retention #=> Integer
    #   resp.data.table.storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table.storage_descriptor.columns #=> Array<Column>
    #   resp.data.table.storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table.storage_descriptor.columns[0].name #=> String
    #   resp.data.table.storage_descriptor.columns[0].type #=> String
    #   resp.data.table.storage_descriptor.columns[0].comment #=> String
    #   resp.data.table.storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table.storage_descriptor.location #=> String
    #   resp.data.table.storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table.storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table.storage_descriptor.input_format #=> String
    #   resp.data.table.storage_descriptor.output_format #=> String
    #   resp.data.table.storage_descriptor.compressed #=> Boolean
    #   resp.data.table.storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table.storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table.storage_descriptor.serde_info.name #=> String
    #   resp.data.table.storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table.storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table.storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table.storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table.storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table.storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table.storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table.storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table.storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table.storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table.storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table.storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table.storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table.partition_keys #=> Array<Column>
    #   resp.data.table.view_original_text #=> String
    #   resp.data.table.view_expanded_text #=> String
    #   resp.data.table.table_type #=> String
    #   resp.data.table.parameters #=> Hash<String, String>
    #   resp.data.table.created_by #=> String
    #   resp.data.table.is_registered_with_lake_formation #=> Boolean
    #   resp.data.table.target_table #=> Types::TableIdentifier
    #   resp.data.table.target_table.catalog_id #=> String
    #   resp.data.table.target_table.database_name #=> String
    #   resp.data.table.target_table.name #=> String
    #   resp.data.table.catalog_id #=> String
    #   resp.data.table.version_id #=> String
    #   resp.data.authorized_columns #=> Array<String>
    #   resp.data.is_registered_with_lake_formation #=> Boolean
    #   resp.data.cell_filters #=> Array<ColumnRowFilter>
    #   resp.data.cell_filters[0] #=> Types::ColumnRowFilter
    #   resp.data.cell_filters[0].column_name #=> String
    #   resp.data.cell_filters[0].row_filter_expression #=> String
    #
    def get_unfiltered_table_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetUnfilteredTableMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetUnfilteredTableMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetUnfilteredTableMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::PermissionTypeMismatchException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetUnfilteredTableMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetUnfilteredTableMetadata,
        stubs: @stubs,
        params_class: Params::GetUnfilteredTableMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_unfiltered_table_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves a specified function definition from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetUserDefinedFunctionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the function to be retrieved is located. If none is
    #         provided, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the function is located.</p>
    #
    # @option params [String] :function_name
    #   <p>The name of the function.</p>
    #
    # @return [Types::GetUserDefinedFunctionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_user_defined_function(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     function_name: 'FunctionName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetUserDefinedFunctionOutput
    #   resp.data.user_defined_function #=> Types::UserDefinedFunction
    #   resp.data.user_defined_function.function_name #=> String
    #   resp.data.user_defined_function.database_name #=> String
    #   resp.data.user_defined_function.class_name #=> String
    #   resp.data.user_defined_function.owner_name #=> String
    #   resp.data.user_defined_function.owner_type #=> String, one of ["USER", "ROLE", "GROUP"]
    #   resp.data.user_defined_function.create_time #=> Time
    #   resp.data.user_defined_function.resource_uris #=> Array<ResourceUri>
    #   resp.data.user_defined_function.resource_uris[0] #=> Types::ResourceUri
    #   resp.data.user_defined_function.resource_uris[0].resource_type #=> String, one of ["JAR", "FILE", "ARCHIVE"]
    #   resp.data.user_defined_function.resource_uris[0].uri #=> String
    #   resp.data.user_defined_function.catalog_id #=> String
    #
    def get_user_defined_function(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetUserDefinedFunctionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetUserDefinedFunctionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetUserDefinedFunction
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetUserDefinedFunction
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetUserDefinedFunction,
        stubs: @stubs,
        params_class: Params::GetUserDefinedFunctionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_user_defined_function
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves multiple function definitions from the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::GetUserDefinedFunctionsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the functions to be retrieved are located. If none is
    #         provided, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the functions are located. If none is provided, functions from all the
    #         databases across the catalog will be returned.</p>
    #
    # @option params [String] :pattern
    #   <p>An optional function-name pattern string that filters the function
    #         definitions returned.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of functions to return in one response.</p>
    #
    # @return [Types::GetUserDefinedFunctionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_user_defined_functions(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName',
    #     pattern: 'Pattern', # required
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetUserDefinedFunctionsOutput
    #   resp.data.user_defined_functions #=> Array<UserDefinedFunction>
    #   resp.data.user_defined_functions[0] #=> Types::UserDefinedFunction
    #   resp.data.user_defined_functions[0].function_name #=> String
    #   resp.data.user_defined_functions[0].database_name #=> String
    #   resp.data.user_defined_functions[0].class_name #=> String
    #   resp.data.user_defined_functions[0].owner_name #=> String
    #   resp.data.user_defined_functions[0].owner_type #=> String, one of ["USER", "ROLE", "GROUP"]
    #   resp.data.user_defined_functions[0].create_time #=> Time
    #   resp.data.user_defined_functions[0].resource_uris #=> Array<ResourceUri>
    #   resp.data.user_defined_functions[0].resource_uris[0] #=> Types::ResourceUri
    #   resp.data.user_defined_functions[0].resource_uris[0].resource_type #=> String, one of ["JAR", "FILE", "ARCHIVE"]
    #   resp.data.user_defined_functions[0].resource_uris[0].uri #=> String
    #   resp.data.user_defined_functions[0].catalog_id #=> String
    #   resp.data.next_token #=> String
    #
    def get_user_defined_functions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetUserDefinedFunctionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetUserDefinedFunctionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetUserDefinedFunctions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetUserDefinedFunctions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetUserDefinedFunctions,
        stubs: @stubs,
        params_class: Params::GetUserDefinedFunctionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_user_defined_functions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves resource metadata for a workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::GetWorkflowInput}.
    #
    # @option params [String] :name
    #   <p>The name of the workflow to retrieve.</p>
    #
    # @option params [Boolean] :include_graph
    #   <p>Specifies whether to include a graph when returning the workflow resource metadata.</p>
    #
    # @return [Types::GetWorkflowOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_workflow(
    #     name: 'Name', # required
    #     include_graph: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetWorkflowOutput
    #   resp.data.workflow #=> Types::Workflow
    #   resp.data.workflow.name #=> String
    #   resp.data.workflow.description #=> String
    #   resp.data.workflow.default_run_properties #=> Hash<String, String>
    #   resp.data.workflow.default_run_properties['key'] #=> String
    #   resp.data.workflow.created_on #=> Time
    #   resp.data.workflow.last_modified_on #=> Time
    #   resp.data.workflow.last_run #=> Types::WorkflowRun
    #   resp.data.workflow.last_run.name #=> String
    #   resp.data.workflow.last_run.workflow_run_id #=> String
    #   resp.data.workflow.last_run.previous_run_id #=> String
    #   resp.data.workflow.last_run.workflow_run_properties #=> Hash<String, String>
    #   resp.data.workflow.last_run.started_on #=> Time
    #   resp.data.workflow.last_run.completed_on #=> Time
    #   resp.data.workflow.last_run.status #=> String, one of ["RUNNING", "COMPLETED", "STOPPING", "STOPPED", "ERROR"]
    #   resp.data.workflow.last_run.error_message #=> String
    #   resp.data.workflow.last_run.statistics #=> Types::WorkflowRunStatistics
    #   resp.data.workflow.last_run.statistics.total_actions #=> Integer
    #   resp.data.workflow.last_run.statistics.timeout_actions #=> Integer
    #   resp.data.workflow.last_run.statistics.failed_actions #=> Integer
    #   resp.data.workflow.last_run.statistics.stopped_actions #=> Integer
    #   resp.data.workflow.last_run.statistics.succeeded_actions #=> Integer
    #   resp.data.workflow.last_run.statistics.running_actions #=> Integer
    #   resp.data.workflow.last_run.graph #=> Types::WorkflowGraph
    #   resp.data.workflow.last_run.graph.nodes #=> Array<Node>
    #   resp.data.workflow.last_run.graph.nodes[0] #=> Types::Node
    #   resp.data.workflow.last_run.graph.nodes[0].type #=> String, one of ["CRAWLER", "JOB", "TRIGGER"]
    #   resp.data.workflow.last_run.graph.nodes[0].name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].unique_id #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details #=> Types::TriggerNodeDetails
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger #=> Types::Trigger
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.workflow_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.id #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.description #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.schedule #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions #=> Array<Action>
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0] #=> Types::Action
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].job_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].timeout #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].security_configuration #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.actions[0].crawler_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate #=> Types::Predicate
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_window #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details #=> Types::JobNodeDetails
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs #=> Array<JobRun>
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0] #=> Types::JobRun
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].id #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].attempt #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].previous_run_id #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].trigger_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].job_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].started_on #=> Time
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].last_modified_on #=> Time
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].completed_on #=> Time
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].arguments #=> Hash<String, String>
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].error_message #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs #=> Array<Predecessor>
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].job_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].run_id #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].allocated_capacity #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].execution_time #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].timeout #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].max_capacity #=> Float
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].number_of_workers #=> Integer
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].security_configuration #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].log_group_name #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].glue_version #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].job_details.job_runs[0].dpu_seconds #=> Float
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details #=> Types::CrawlerNodeDetails
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls #=> Array<Crawl>
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0] #=> Types::Crawl
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].started_on #=> Time
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].completed_on #=> Time
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].error_message #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].log_group #=> String
    #   resp.data.workflow.last_run.graph.nodes[0].crawler_details.crawls[0].log_stream #=> String
    #   resp.data.workflow.last_run.graph.edges #=> Array<Edge>
    #   resp.data.workflow.last_run.graph.edges[0] #=> Types::Edge
    #   resp.data.workflow.last_run.graph.edges[0].source_id #=> String
    #   resp.data.workflow.last_run.graph.edges[0].destination_id #=> String
    #   resp.data.workflow.last_run.starting_event_batch_condition #=> Types::StartingEventBatchCondition
    #   resp.data.workflow.last_run.starting_event_batch_condition.batch_size #=> Integer
    #   resp.data.workflow.last_run.starting_event_batch_condition.batch_window #=> Integer
    #   resp.data.workflow.graph #=> Types::WorkflowGraph
    #   resp.data.workflow.max_concurrent_runs #=> Integer
    #   resp.data.workflow.blueprint_details #=> Types::BlueprintDetails
    #   resp.data.workflow.blueprint_details.blueprint_name #=> String
    #   resp.data.workflow.blueprint_details.run_id #=> String
    #
    def get_workflow(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetWorkflowInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetWorkflowInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetWorkflow
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetWorkflow
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetWorkflow,
        stubs: @stubs,
        params_class: Params::GetWorkflowOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_workflow
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the metadata for a given workflow run. </p>
    #
    # @param [Hash] params
    #   See {Types::GetWorkflowRunInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow being run.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the workflow run.</p>
    #
    # @option params [Boolean] :include_graph
    #   <p>Specifies whether to include the workflow graph in response or not.</p>
    #
    # @return [Types::GetWorkflowRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_workflow_run(
    #     name: 'Name', # required
    #     run_id: 'RunId', # required
    #     include_graph: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetWorkflowRunOutput
    #   resp.data.run #=> Types::WorkflowRun
    #   resp.data.run.name #=> String
    #   resp.data.run.workflow_run_id #=> String
    #   resp.data.run.previous_run_id #=> String
    #   resp.data.run.workflow_run_properties #=> Hash<String, String>
    #   resp.data.run.workflow_run_properties['key'] #=> String
    #   resp.data.run.started_on #=> Time
    #   resp.data.run.completed_on #=> Time
    #   resp.data.run.status #=> String, one of ["RUNNING", "COMPLETED", "STOPPING", "STOPPED", "ERROR"]
    #   resp.data.run.error_message #=> String
    #   resp.data.run.statistics #=> Types::WorkflowRunStatistics
    #   resp.data.run.statistics.total_actions #=> Integer
    #   resp.data.run.statistics.timeout_actions #=> Integer
    #   resp.data.run.statistics.failed_actions #=> Integer
    #   resp.data.run.statistics.stopped_actions #=> Integer
    #   resp.data.run.statistics.succeeded_actions #=> Integer
    #   resp.data.run.statistics.running_actions #=> Integer
    #   resp.data.run.graph #=> Types::WorkflowGraph
    #   resp.data.run.graph.nodes #=> Array<Node>
    #   resp.data.run.graph.nodes[0] #=> Types::Node
    #   resp.data.run.graph.nodes[0].type #=> String, one of ["CRAWLER", "JOB", "TRIGGER"]
    #   resp.data.run.graph.nodes[0].name #=> String
    #   resp.data.run.graph.nodes[0].unique_id #=> String
    #   resp.data.run.graph.nodes[0].trigger_details #=> Types::TriggerNodeDetails
    #   resp.data.run.graph.nodes[0].trigger_details.trigger #=> Types::Trigger
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.workflow_name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.id #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.description #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.schedule #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions #=> Array<Action>
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0] #=> Types::Action
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].job_name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].timeout #=> Integer
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].security_configuration #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.actions[0].crawler_name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate #=> Types::Predicate
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.run.graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_window #=> Integer
    #   resp.data.run.graph.nodes[0].job_details #=> Types::JobNodeDetails
    #   resp.data.run.graph.nodes[0].job_details.job_runs #=> Array<JobRun>
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0] #=> Types::JobRun
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].id #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].attempt #=> Integer
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].previous_run_id #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].trigger_name #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].job_name #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].started_on #=> Time
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].last_modified_on #=> Time
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].completed_on #=> Time
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].arguments #=> Hash<String, String>
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].error_message #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].predecessor_runs #=> Array<Predecessor>
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].job_name #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].run_id #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].allocated_capacity #=> Integer
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].execution_time #=> Integer
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].timeout #=> Integer
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].max_capacity #=> Float
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].number_of_workers #=> Integer
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].security_configuration #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].log_group_name #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].glue_version #=> String
    #   resp.data.run.graph.nodes[0].job_details.job_runs[0].dpu_seconds #=> Float
    #   resp.data.run.graph.nodes[0].crawler_details #=> Types::CrawlerNodeDetails
    #   resp.data.run.graph.nodes[0].crawler_details.crawls #=> Array<Crawl>
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0] #=> Types::Crawl
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].started_on #=> Time
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].completed_on #=> Time
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].error_message #=> String
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].log_group #=> String
    #   resp.data.run.graph.nodes[0].crawler_details.crawls[0].log_stream #=> String
    #   resp.data.run.graph.edges #=> Array<Edge>
    #   resp.data.run.graph.edges[0] #=> Types::Edge
    #   resp.data.run.graph.edges[0].source_id #=> String
    #   resp.data.run.graph.edges[0].destination_id #=> String
    #   resp.data.run.starting_event_batch_condition #=> Types::StartingEventBatchCondition
    #   resp.data.run.starting_event_batch_condition.batch_size #=> Integer
    #   resp.data.run.starting_event_batch_condition.batch_window #=> Integer
    #
    def get_workflow_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetWorkflowRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetWorkflowRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetWorkflowRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetWorkflowRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetWorkflowRun,
        stubs: @stubs,
        params_class: Params::GetWorkflowRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_workflow_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the workflow run properties which were set during the run.</p>
    #
    # @param [Hash] params
    #   See {Types::GetWorkflowRunPropertiesInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow which was run.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the workflow run whose run properties should be returned.</p>
    #
    # @return [Types::GetWorkflowRunPropertiesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_workflow_run_properties(
    #     name: 'Name', # required
    #     run_id: 'RunId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetWorkflowRunPropertiesOutput
    #   resp.data.run_properties #=> Hash<String, String>
    #   resp.data.run_properties['key'] #=> String
    #
    def get_workflow_run_properties(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetWorkflowRunPropertiesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetWorkflowRunPropertiesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetWorkflowRunProperties
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetWorkflowRunProperties
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetWorkflowRunProperties,
        stubs: @stubs,
        params_class: Params::GetWorkflowRunPropertiesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_workflow_run_properties
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves metadata for all runs of a given workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::GetWorkflowRunsInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow whose metadata of runs should be returned.</p>
    #
    # @option params [Boolean] :include_graph
    #   <p>Specifies whether to include the workflow graph in response or not.</p>
    #
    # @option params [String] :next_token
    #   <p>The maximum size of the response.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of workflow runs to be included in the response.</p>
    #
    # @return [Types::GetWorkflowRunsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.get_workflow_runs(
    #     name: 'Name', # required
    #     include_graph: false,
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::GetWorkflowRunsOutput
    #   resp.data.runs #=> Array<WorkflowRun>
    #   resp.data.runs[0] #=> Types::WorkflowRun
    #   resp.data.runs[0].name #=> String
    #   resp.data.runs[0].workflow_run_id #=> String
    #   resp.data.runs[0].previous_run_id #=> String
    #   resp.data.runs[0].workflow_run_properties #=> Hash<String, String>
    #   resp.data.runs[0].workflow_run_properties['key'] #=> String
    #   resp.data.runs[0].started_on #=> Time
    #   resp.data.runs[0].completed_on #=> Time
    #   resp.data.runs[0].status #=> String, one of ["RUNNING", "COMPLETED", "STOPPING", "STOPPED", "ERROR"]
    #   resp.data.runs[0].error_message #=> String
    #   resp.data.runs[0].statistics #=> Types::WorkflowRunStatistics
    #   resp.data.runs[0].statistics.total_actions #=> Integer
    #   resp.data.runs[0].statistics.timeout_actions #=> Integer
    #   resp.data.runs[0].statistics.failed_actions #=> Integer
    #   resp.data.runs[0].statistics.stopped_actions #=> Integer
    #   resp.data.runs[0].statistics.succeeded_actions #=> Integer
    #   resp.data.runs[0].statistics.running_actions #=> Integer
    #   resp.data.runs[0].graph #=> Types::WorkflowGraph
    #   resp.data.runs[0].graph.nodes #=> Array<Node>
    #   resp.data.runs[0].graph.nodes[0] #=> Types::Node
    #   resp.data.runs[0].graph.nodes[0].type #=> String, one of ["CRAWLER", "JOB", "TRIGGER"]
    #   resp.data.runs[0].graph.nodes[0].name #=> String
    #   resp.data.runs[0].graph.nodes[0].unique_id #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details #=> Types::TriggerNodeDetails
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger #=> Types::Trigger
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.workflow_name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.id #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.description #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.schedule #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions #=> Array<Action>
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0] #=> Types::Action
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].job_name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].timeout #=> Integer
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].security_configuration #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.actions[0].crawler_name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate #=> Types::Predicate
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.runs[0].graph.nodes[0].trigger_details.trigger.event_batching_condition.batch_window #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details #=> Types::JobNodeDetails
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs #=> Array<JobRun>
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0] #=> Types::JobRun
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].id #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].attempt #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].previous_run_id #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].trigger_name #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].job_name #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].started_on #=> Time
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].last_modified_on #=> Time
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].completed_on #=> Time
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].job_run_state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].arguments #=> Hash<String, String>
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].error_message #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].predecessor_runs #=> Array<Predecessor>
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].predecessor_runs[0] #=> Types::Predecessor
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].job_name #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].predecessor_runs[0].run_id #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].allocated_capacity #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].execution_time #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].timeout #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].max_capacity #=> Float
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].worker_type #=> String, one of ["Standard", "G.1X", "G.2X"]
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].number_of_workers #=> Integer
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].security_configuration #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].log_group_name #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].notification_property #=> Types::NotificationProperty
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].glue_version #=> String
    #   resp.data.runs[0].graph.nodes[0].job_details.job_runs[0].dpu_seconds #=> Float
    #   resp.data.runs[0].graph.nodes[0].crawler_details #=> Types::CrawlerNodeDetails
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls #=> Array<Crawl>
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0] #=> Types::Crawl
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].started_on #=> Time
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].completed_on #=> Time
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].error_message #=> String
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].log_group #=> String
    #   resp.data.runs[0].graph.nodes[0].crawler_details.crawls[0].log_stream #=> String
    #   resp.data.runs[0].graph.edges #=> Array<Edge>
    #   resp.data.runs[0].graph.edges[0] #=> Types::Edge
    #   resp.data.runs[0].graph.edges[0].source_id #=> String
    #   resp.data.runs[0].graph.edges[0].destination_id #=> String
    #   resp.data.runs[0].starting_event_batch_condition #=> Types::StartingEventBatchCondition
    #   resp.data.runs[0].starting_event_batch_condition.batch_size #=> Integer
    #   resp.data.runs[0].starting_event_batch_condition.batch_window #=> Integer
    #   resp.data.next_token #=> String
    #
    def get_workflow_runs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::GetWorkflowRunsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::GetWorkflowRunsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::GetWorkflowRuns
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::GetWorkflowRuns
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::GetWorkflowRuns,
        stubs: @stubs,
        params_class: Params::GetWorkflowRunsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :get_workflow_runs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Imports an existing Amazon Athena Data Catalog to Glue.</p>
    #
    # @param [Hash] params
    #   See {Types::ImportCatalogToGlueInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the catalog to import. Currently, this should be the Amazon Web Services account ID.</p>
    #
    # @return [Types::ImportCatalogToGlueOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.import_catalog_to_glue(
    #     catalog_id: 'CatalogId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ImportCatalogToGlueOutput
    #
    def import_catalog_to_glue(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ImportCatalogToGlueInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ImportCatalogToGlueInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ImportCatalogToGlue
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ImportCatalogToGlue
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ImportCatalogToGlue,
        stubs: @stubs,
        params_class: Params::ImportCatalogToGlueOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :import_catalog_to_glue
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Lists all the blueprint names in an account.</p>
    #
    # @param [Hash] params
    #   See {Types::ListBlueprintsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Filters the list by an Amazon Web Services resource tag.</p>
    #
    # @return [Types::ListBlueprintsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_blueprints(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListBlueprintsOutput
    #   resp.data.blueprints #=> Array<String>
    #   resp.data.blueprints[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_blueprints(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListBlueprintsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListBlueprintsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListBlueprints
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListBlueprints
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListBlueprints,
        stubs: @stubs,
        params_class: Params::ListBlueprintsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_blueprints
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the names of all crawler resources in this Amazon Web Services account, or the
    #       resources with the specified tag. This operation allows you to see which
    #       resources are available in your account, and their names.</p>
    #
    #          <p>This operation takes the optional <code>Tags</code> field, which you can use as a filter on
    #       the response so that tagged resources can be retrieved as a group. If you choose to use tags
    #       filtering, only resources with the tag are retrieved.</p>
    #
    # @param [Hash] params
    #   See {Types::ListCrawlersInput}.
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Specifies to return only these tagged resources.</p>
    #
    # @return [Types::ListCrawlersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_crawlers(
    #     max_results: 1,
    #     next_token: 'NextToken',
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListCrawlersOutput
    #   resp.data.crawler_names #=> Array<String>
    #   resp.data.crawler_names[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_crawlers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListCrawlersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListCrawlersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListCrawlers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::OperationTimeoutException]),
        data_parser: Parsers::ListCrawlers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListCrawlers,
        stubs: @stubs,
        params_class: Params::ListCrawlersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_crawlers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Lists all the custom patterns that have been created.</p>
    #
    # @param [Hash] params
    #   See {Types::ListCustomEntityTypesInput}.
    #
    # @option params [String] :next_token
    #   <p>A paginated token to offset the results.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of results to return.</p>
    #
    # @return [Types::ListCustomEntityTypesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_custom_entity_types(
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListCustomEntityTypesOutput
    #   resp.data.custom_entity_types #=> Array<CustomEntityType>
    #   resp.data.custom_entity_types[0] #=> Types::CustomEntityType
    #   resp.data.custom_entity_types[0].name #=> String
    #   resp.data.custom_entity_types[0].regex_string #=> String
    #   resp.data.custom_entity_types[0].context_words #=> Array<String>
    #   resp.data.custom_entity_types[0].context_words[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_custom_entity_types(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListCustomEntityTypesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListCustomEntityTypesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListCustomEntityTypes
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListCustomEntityTypes
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListCustomEntityTypes,
        stubs: @stubs,
        params_class: Params::ListCustomEntityTypesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_custom_entity_types
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the names of all <code>DevEndpoint</code> resources in this Amazon Web Services account, or the
    #       resources with the specified tag. This operation allows you to see which resources are
    #       available in your account, and their names.</p>
    #
    #          <p>This operation takes the optional <code>Tags</code> field, which you can use as a filter on
    #       the response so that tagged resources can be retrieved as a group. If you choose to use tags
    #       filtering, only resources with the tag are retrieved.</p>
    #
    # @param [Hash] params
    #   See {Types::ListDevEndpointsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Specifies to return only these tagged resources.</p>
    #
    # @return [Types::ListDevEndpointsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_dev_endpoints(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListDevEndpointsOutput
    #   resp.data.dev_endpoint_names #=> Array<String>
    #   resp.data.dev_endpoint_names[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_dev_endpoints(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListDevEndpointsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListDevEndpointsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListDevEndpoints
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListDevEndpoints
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListDevEndpoints,
        stubs: @stubs,
        params_class: Params::ListDevEndpointsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_dev_endpoints
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the names of all job resources in this Amazon Web Services account, or the resources with the specified tag. This operation allows you to see which resources are available in your account, and their names.</p>
    # 	
    # 	        <p>This operation takes the optional <code>Tags</code> field, which you can use as a filter on
    #       the response so that tagged resources can be retrieved as a group. If you choose to use tags
    #       filtering, only resources with the tag are retrieved.</p>
    #
    # @param [Hash] params
    #   See {Types::ListJobsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Specifies to return only these tagged resources.</p>
    #
    # @return [Types::ListJobsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_jobs(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListJobsOutput
    #   resp.data.job_names #=> Array<String>
    #   resp.data.job_names[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_jobs(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListJobsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListJobsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListJobs
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListJobs
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListJobs,
        stubs: @stubs,
        params_class: Params::ListJobsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_jobs
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p> Retrieves a sortable, filterable list of existing Glue machine learning transforms in this Amazon Web Services account,
    #        or the resources with the specified tag. This operation takes the optional <code>Tags</code> field, which you can use as
    #        a filter of the responses so that tagged resources can be retrieved as a group. If you choose to use tag
    #        filtering, only resources with the tags are retrieved.
    #      </p>
    #
    # @param [Hash] params
    #   See {Types::ListMLTransformsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [TransformFilterCriteria] :filter
    #   <p>A <code>TransformFilterCriteria</code> used to filter the machine learning transforms.</p>
    #
    # @option params [TransformSortCriteria] :sort
    #   <p>A <code>TransformSortCriteria</code> used to sort the machine learning transforms.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Specifies to return only these tagged resources.</p>
    #
    # @return [Types::ListMLTransformsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_ml_transforms(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     filter: {
    #       name: 'Name',
    #       transform_type: 'FIND_MATCHES', # accepts ["FIND_MATCHES"]
    #       status: 'NOT_READY', # accepts ["NOT_READY", "READY", "DELETING"]
    #       glue_version: 'GlueVersion',
    #       created_before: Time.now,
    #       created_after: Time.now,
    #       last_modified_before: Time.now,
    #       last_modified_after: Time.now,
    #       schema: [
    #         {
    #           name: 'Name',
    #           data_type: 'DataType'
    #         }
    #       ]
    #     },
    #     sort: {
    #       column: 'NAME', # required - accepts ["NAME", "TRANSFORM_TYPE", "STATUS", "CREATED", "LAST_MODIFIED"]
    #       sort_direction: 'DESCENDING' # required - accepts ["DESCENDING", "ASCENDING"]
    #     },
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListMLTransformsOutput
    #   resp.data.transform_ids #=> Array<String>
    #   resp.data.transform_ids[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_ml_transforms(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListMLTransformsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListMLTransformsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListMLTransforms
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListMLTransforms
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListMLTransforms,
        stubs: @stubs,
        params_class: Params::ListMLTransformsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_ml_transforms
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of registries that you have created, with minimal registry information. Registries in the <code>Deleting</code> status will not be included in the results. Empty results will be returned if there are no registries available.</p>
    #
    # @param [Hash] params
    #   See {Types::ListRegistriesInput}.
    #
    # @option params [Integer] :max_results
    #   <p>Maximum number of results required per page. If the value is not supplied, this will be defaulted to 25 per page.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::ListRegistriesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_registries(
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListRegistriesOutput
    #   resp.data.registries #=> Array<RegistryListItem>
    #   resp.data.registries[0] #=> Types::RegistryListItem
    #   resp.data.registries[0].registry_name #=> String
    #   resp.data.registries[0].registry_arn #=> String
    #   resp.data.registries[0].description #=> String
    #   resp.data.registries[0].status #=> String, one of ["AVAILABLE", "DELETING"]
    #   resp.data.registries[0].created_time #=> String
    #   resp.data.registries[0].updated_time #=> String
    #   resp.data.next_token #=> String
    #
    def list_registries(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListRegistriesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListRegistriesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListRegistries
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::ListRegistries
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListRegistries,
        stubs: @stubs,
        params_class: Params::ListRegistriesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_registries
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of schema versions that you have created, with minimal information. Schema versions in Deleted status will not be included in the results. Empty results will be returned if there are no schema versions available.</p>
    #
    # @param [Hash] params
    #   See {Types::ListSchemaVersionsInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [Integer] :max_results
    #   <p>Maximum number of results required per page. If the value is not supplied, this will be defaulted to 25 per page.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::ListSchemaVersionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_schema_versions(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListSchemaVersionsOutput
    #   resp.data.schemas #=> Array<SchemaVersionListItem>
    #   resp.data.schemas[0] #=> Types::SchemaVersionListItem
    #   resp.data.schemas[0].schema_arn #=> String
    #   resp.data.schemas[0].schema_version_id #=> String
    #   resp.data.schemas[0].version_number #=> Integer
    #   resp.data.schemas[0].status #=> String, one of ["AVAILABLE", "PENDING", "FAILURE", "DELETING"]
    #   resp.data.schemas[0].created_time #=> String
    #   resp.data.next_token #=> String
    #
    def list_schema_versions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListSchemaVersionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListSchemaVersionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListSchemaVersions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::ListSchemaVersions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListSchemaVersions,
        stubs: @stubs,
        params_class: Params::ListSchemaVersionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_schema_versions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Returns a list of schemas with minimal details. Schemas in Deleting status will not be included in the results. Empty results will be returned if there are no schemas available.</p>
    #          <p>When the <code>RegistryId</code> is not provided, all the schemas across registries will be part of the API response.</p>
    #
    # @param [Hash] params
    #   See {Types::ListSchemasInput}.
    #
    # @option params [RegistryId] :registry_id
    #   <p>A wrapper structure that may contain the registry name and Amazon Resource Name (ARN).</p>
    #
    # @option params [Integer] :max_results
    #   <p>Maximum number of results required per page. If the value is not supplied, this will be defaulted to 25 per page.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::ListSchemasOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_schemas(
    #     registry_id: {
    #       registry_name: 'RegistryName',
    #       registry_arn: 'RegistryArn'
    #     },
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListSchemasOutput
    #   resp.data.schemas #=> Array<SchemaListItem>
    #   resp.data.schemas[0] #=> Types::SchemaListItem
    #   resp.data.schemas[0].registry_name #=> String
    #   resp.data.schemas[0].schema_name #=> String
    #   resp.data.schemas[0].schema_arn #=> String
    #   resp.data.schemas[0].description #=> String
    #   resp.data.schemas[0].schema_status #=> String, one of ["AVAILABLE", "PENDING", "DELETING"]
    #   resp.data.schemas[0].created_time #=> String
    #   resp.data.schemas[0].updated_time #=> String
    #   resp.data.next_token #=> String
    #
    def list_schemas(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListSchemasInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListSchemasInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListSchemas
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::ListSchemas
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListSchemas,
        stubs: @stubs,
        params_class: Params::ListSchemasOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_schemas
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieve a session..</p>
    #
    # @param [Hash] params
    #   See {Types::ListSessionsInput}.
    #
    # @option params [String] :next_token
    #   <p>The token for the next set of results, or null if there are no more result. </p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of results. </p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Tags belonging to the session. </p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request. </p>
    #
    # @return [Types::ListSessionsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_sessions(
    #     next_token: 'NextToken',
    #     max_results: 1,
    #     tags: {
    #       'key' => 'value'
    #     },
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListSessionsOutput
    #   resp.data.ids #=> Array<String>
    #   resp.data.ids[0] #=> String
    #   resp.data.sessions #=> Array<Session>
    #   resp.data.sessions[0] #=> Types::Session
    #   resp.data.sessions[0].id #=> String
    #   resp.data.sessions[0].created_on #=> Time
    #   resp.data.sessions[0].status #=> String, one of ["PROVISIONING", "READY", "FAILED", "TIMEOUT", "STOPPING", "STOPPED"]
    #   resp.data.sessions[0].error_message #=> String
    #   resp.data.sessions[0].description #=> String
    #   resp.data.sessions[0].role #=> String
    #   resp.data.sessions[0].command #=> Types::SessionCommand
    #   resp.data.sessions[0].command.name #=> String
    #   resp.data.sessions[0].command.python_version #=> String
    #   resp.data.sessions[0].default_arguments #=> Hash<String, String>
    #   resp.data.sessions[0].default_arguments['key'] #=> String
    #   resp.data.sessions[0].connections #=> Types::ConnectionsList
    #   resp.data.sessions[0].connections.connections #=> Array<String>
    #   resp.data.sessions[0].connections.connections[0] #=> String
    #   resp.data.sessions[0].progress #=> Float
    #   resp.data.sessions[0].max_capacity #=> Float
    #   resp.data.sessions[0].security_configuration #=> String
    #   resp.data.sessions[0].glue_version #=> String
    #   resp.data.next_token #=> String
    #
    def list_sessions(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListSessionsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListSessionsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListSessions
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::ListSessions
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListSessions,
        stubs: @stubs,
        params_class: Params::ListSessionsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_sessions
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Lists statements for the session.</p>
    #
    # @param [Hash] params
    #   See {Types::ListStatementsInput}.
    #
    # @option params [String] :session_id
    #   <p>The Session ID of the statements.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request to list statements.</p>
    #
    # @return [Types::ListStatementsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_statements(
    #     session_id: 'SessionId', # required
    #     request_origin: 'RequestOrigin',
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListStatementsOutput
    #   resp.data.statements #=> Array<Statement>
    #   resp.data.statements[0] #=> Types::Statement
    #   resp.data.statements[0].id #=> Integer
    #   resp.data.statements[0].code #=> String
    #   resp.data.statements[0].state #=> String, one of ["WAITING", "RUNNING", "AVAILABLE", "CANCELLING", "CANCELLED", "ERROR"]
    #   resp.data.statements[0].output #=> Types::StatementOutput
    #   resp.data.statements[0].output.data #=> Types::StatementOutputData
    #   resp.data.statements[0].output.data.text_plain #=> String
    #   resp.data.statements[0].output.execution_count #=> Integer
    #   resp.data.statements[0].output.status #=> String, one of ["WAITING", "RUNNING", "AVAILABLE", "CANCELLING", "CANCELLED", "ERROR"]
    #   resp.data.statements[0].output.error_name #=> String
    #   resp.data.statements[0].output.error_value #=> String
    #   resp.data.statements[0].output.traceback #=> Array<String>
    #   resp.data.statements[0].output.traceback[0] #=> String
    #   resp.data.statements[0].progress #=> Float
    #   resp.data.statements[0].started_on #=> Integer
    #   resp.data.statements[0].completed_on #=> Integer
    #   resp.data.next_token #=> String
    #
    def list_statements(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListStatementsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListStatementsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListStatements
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::ListStatements
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListStatements,
        stubs: @stubs,
        params_class: Params::ListStatementsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_statements
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Retrieves the names of all trigger resources in this Amazon Web Services account, or the resources with the specified tag. This operation allows you to see which resources are available in your account, and their names.</p>
    #
    #          <p>This operation takes the optional <code>Tags</code> field, which you can use as a filter on
    #       the response so that tagged resources can be retrieved as a group. If you choose to use tags
    #       filtering, only resources with the tag are retrieved.</p>
    #
    # @param [Hash] params
    #   See {Types::ListTriggersInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [String] :dependent_job_name
    #   <p> The name of the job for which to retrieve triggers. The trigger that can start this job
    #         is returned. If there is no such trigger, all triggers are returned.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @option params [Hash<String, String>] :tags
    #   <p>Specifies to return only these tagged resources.</p>
    #
    # @return [Types::ListTriggersOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_triggers(
    #     next_token: 'NextToken',
    #     dependent_job_name: 'DependentJobName',
    #     max_results: 1,
    #     tags: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListTriggersOutput
    #   resp.data.trigger_names #=> Array<String>
    #   resp.data.trigger_names[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_triggers(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListTriggersInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListTriggersInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListTriggers
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListTriggers
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListTriggers,
        stubs: @stubs,
        params_class: Params::ListTriggersOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_triggers
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Lists names of workflows created in the account.</p>
    #
    # @param [Hash] params
    #   See {Types::ListWorkflowsInput}.
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation request.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum size of a list to return.</p>
    #
    # @return [Types::ListWorkflowsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.list_workflows(
    #     next_token: 'NextToken',
    #     max_results: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ListWorkflowsOutput
    #   resp.data.workflows #=> Array<String>
    #   resp.data.workflows[0] #=> String
    #   resp.data.next_token #=> String
    #
    def list_workflows(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ListWorkflowsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ListWorkflowsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ListWorkflows
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ListWorkflows
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ListWorkflows,
        stubs: @stubs,
        params_class: Params::ListWorkflowsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :list_workflows
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Sets the security configuration for a specified catalog. After the configuration has been
    #       set, the specified encryption is applied to every catalog write thereafter.</p>
    #
    # @param [Hash] params
    #   See {Types::PutDataCatalogEncryptionSettingsInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog to set the security configuration for. If none is provided, the
    #         Amazon Web Services account ID is used by default.</p>
    #
    # @option params [DataCatalogEncryptionSettings] :data_catalog_encryption_settings
    #   <p>The security configuration to set.</p>
    #
    # @return [Types::PutDataCatalogEncryptionSettingsOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.put_data_catalog_encryption_settings(
    #     catalog_id: 'CatalogId',
    #     data_catalog_encryption_settings: {
    #       encryption_at_rest: {
    #         catalog_encryption_mode: 'DISABLED', # required - accepts ["DISABLED", "SSE-KMS"]
    #         sse_aws_kms_key_id: 'SseAwsKmsKeyId'
    #       },
    #       connection_password_encryption: {
    #         return_connection_password_encrypted: false, # required
    #         aws_kms_key_id: 'AwsKmsKeyId'
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::PutDataCatalogEncryptionSettingsOutput
    #
    def put_data_catalog_encryption_settings(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::PutDataCatalogEncryptionSettingsInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::PutDataCatalogEncryptionSettingsInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::PutDataCatalogEncryptionSettings
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::PutDataCatalogEncryptionSettings
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::PutDataCatalogEncryptionSettings,
        stubs: @stubs,
        params_class: Params::PutDataCatalogEncryptionSettingsOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :put_data_catalog_encryption_settings
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Sets the Data Catalog resource policy for access control.</p>
    #
    # @param [Hash] params
    #   See {Types::PutResourcePolicyInput}.
    #
    # @option params [String] :policy_in_json
    #   <p>Contains the policy document to set, in JSON format.</p>
    #
    # @option params [String] :resource_arn
    #   <p>Do not use. For internal use only.</p>
    #
    # @option params [String] :policy_hash_condition
    #   <p>The hash value returned when the previous policy was set using
    #           <code>PutResourcePolicy</code>. Its purpose is to prevent concurrent modifications of a
    #         policy. Do not use this parameter if no previous policy has been set.</p>
    #
    # @option params [String] :policy_exists_condition
    #   <p>A value of <code>MUST_EXIST</code> is used to update a policy. A value of
    #           <code>NOT_EXIST</code> is used to create a new policy. If a value of <code>NONE</code> or a
    #         null value is used, the call does not depend on the existence of a policy.</p>
    #
    # @option params [String] :enable_hybrid
    #   <p>If <code>'TRUE'</code>, indicates that you are using both methods to grant cross-account
    #         access to Data Catalog resources:</p>
    #            <ul>
    #               <li>
    #                  <p>By directly updating the resource policy with <code>PutResourePolicy</code>
    #                  </p>
    #               </li>
    #               <li>
    #                  <p>By using the <b>Grant permissions</b> command on the Amazon Web Services Management Console.</p>
    #               </li>
    #            </ul>
    #            <p>Must be set to <code>'TRUE'</code> if you have already used the Management Console to
    #         grant cross-account access, otherwise the call fails. Default is 'FALSE'.</p>
    #
    # @return [Types::PutResourcePolicyOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.put_resource_policy(
    #     policy_in_json: 'PolicyInJson', # required
    #     resource_arn: 'ResourceArn',
    #     policy_hash_condition: 'PolicyHashCondition',
    #     policy_exists_condition: 'MUST_EXIST', # accepts ["MUST_EXIST", "NOT_EXIST", "NONE"]
    #     enable_hybrid: 'TRUE' # accepts ["TRUE", "FALSE"]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::PutResourcePolicyOutput
    #   resp.data.policy_hash #=> String
    #
    def put_resource_policy(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::PutResourcePolicyInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::PutResourcePolicyInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::PutResourcePolicy
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConditionCheckFailureException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::PutResourcePolicy
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::PutResourcePolicy,
        stubs: @stubs,
        params_class: Params::PutResourcePolicyOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :put_resource_policy
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Puts the metadata key value pair for a specified schema version ID. A maximum of 10 key value pairs will be allowed per schema version. They can be added over one or more calls.</p>
    #
    # @param [Hash] params
    #   See {Types::PutSchemaVersionMetadataInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>The unique ID for the schema.</p>
    #
    # @option params [SchemaVersionNumber] :schema_version_number
    #   <p>The version number of the schema.</p>
    #
    # @option params [String] :schema_version_id
    #   <p>The unique version ID of the schema version.</p>
    #
    # @option params [MetadataKeyValuePair] :metadata_key_value
    #   <p>The metadata key's corresponding value.</p>
    #
    # @return [Types::PutSchemaVersionMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.put_schema_version_metadata(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     },
    #     schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     },
    #     schema_version_id: 'SchemaVersionId',
    #     metadata_key_value: {
    #       metadata_key: 'MetadataKey',
    #       metadata_value: 'MetadataValue'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::PutSchemaVersionMetadataOutput
    #   resp.data.schema_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.registry_name #=> String
    #   resp.data.latest_version #=> Boolean
    #   resp.data.version_number #=> Integer
    #   resp.data.schema_version_id #=> String
    #   resp.data.metadata_key #=> String
    #   resp.data.metadata_value #=> String
    #
    def put_schema_version_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::PutSchemaVersionMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::PutSchemaVersionMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::PutSchemaVersionMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException]),
        data_parser: Parsers::PutSchemaVersionMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::PutSchemaVersionMetadata,
        stubs: @stubs,
        params_class: Params::PutSchemaVersionMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :put_schema_version_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Puts the specified workflow run properties for the given workflow run. If a property already exists for the specified run, then it overrides the value otherwise adds the property to existing properties.</p>
    #
    # @param [Hash] params
    #   See {Types::PutWorkflowRunPropertiesInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow which was run.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the workflow run for which the run properties should be updated.</p>
    #
    # @option params [Hash<String, String>] :run_properties
    #   <p>The properties to put for the specified run.</p>
    #
    # @return [Types::PutWorkflowRunPropertiesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.put_workflow_run_properties(
    #     name: 'Name', # required
    #     run_id: 'RunId', # required
    #     run_properties: {
    #       'key' => 'value'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::PutWorkflowRunPropertiesOutput
    #
    def put_workflow_run_properties(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::PutWorkflowRunPropertiesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::PutWorkflowRunPropertiesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::PutWorkflowRunProperties
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::AlreadyExistsException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::PutWorkflowRunProperties
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::PutWorkflowRunProperties,
        stubs: @stubs,
        params_class: Params::PutWorkflowRunPropertiesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :put_workflow_run_properties
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Queries for the schema version metadata information. </p>
    #
    # @param [Hash] params
    #   See {Types::QuerySchemaVersionMetadataInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>A wrapper structure that may contain the schema name and Amazon Resource Name (ARN).</p>
    #
    # @option params [SchemaVersionNumber] :schema_version_number
    #   <p>The version number of the schema.</p>
    #
    # @option params [String] :schema_version_id
    #   <p>The unique version ID of the schema version.</p>
    #
    # @option params [Array<MetadataKeyValuePair>] :metadata_list
    #   <p>Search key-value pairs for metadata, if they are not provided all the metadata information will be fetched.</p>
    #
    # @option params [Integer] :max_results
    #   <p>Maximum number of results required per page. If the value is not supplied, this will be defaulted to 25 per page.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, if this is a continuation call.</p>
    #
    # @return [Types::QuerySchemaVersionMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.query_schema_version_metadata(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     },
    #     schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     },
    #     schema_version_id: 'SchemaVersionId',
    #     metadata_list: [
    #       {
    #         metadata_key: 'MetadataKey',
    #         metadata_value: 'MetadataValue'
    #       }
    #     ],
    #     max_results: 1,
    #     next_token: 'NextToken'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::QuerySchemaVersionMetadataOutput
    #   resp.data.metadata_info_map #=> Hash<String, MetadataInfo>
    #   resp.data.metadata_info_map['key'] #=> Types::MetadataInfo
    #   resp.data.metadata_info_map['key'].metadata_value #=> String
    #   resp.data.metadata_info_map['key'].created_time #=> String
    #   resp.data.metadata_info_map['key'].other_metadata_value_list #=> Array<OtherMetadataValueListItem>
    #   resp.data.metadata_info_map['key'].other_metadata_value_list[0] #=> Types::OtherMetadataValueListItem
    #   resp.data.metadata_info_map['key'].other_metadata_value_list[0].metadata_value #=> String
    #   resp.data.metadata_info_map['key'].other_metadata_value_list[0].created_time #=> String
    #   resp.data.schema_version_id #=> String
    #   resp.data.next_token #=> String
    #
    def query_schema_version_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::QuerySchemaVersionMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::QuerySchemaVersionMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::QuerySchemaVersionMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::QuerySchemaVersionMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::QuerySchemaVersionMetadata,
        stubs: @stubs,
        params_class: Params::QuerySchemaVersionMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :query_schema_version_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Adds a new version to the existing schema. Returns an error if new version of schema does not meet the compatibility requirements of the schema set. This API will not create a new schema set and will return a 404 error if the schema set is not already present in the Schema Registry.</p>
    #          <p>If this is the first schema definition to be registered in the Schema Registry, this API will store the schema version and return immediately. Otherwise, this call has the potential to run longer than other operations due to compatibility modes. You can call the <code>GetSchemaVersion</code> API with the <code>SchemaVersionId</code> to check compatibility modes.</p>
    # 	        <p>If the same schema definition is already stored in Schema Registry as a version, the schema ID of the existing schema is returned to the caller.</p>
    #
    # @param [Hash] params
    #   See {Types::RegisterSchemaVersionInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. Either <code>SchemaArn</code> or <code>SchemaName</code> and <code>RegistryName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :schema_definition
    #   <p>The schema definition using the <code>DataFormat</code> setting for the <code>SchemaName</code>.</p>
    #
    # @return [Types::RegisterSchemaVersionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.register_schema_version(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     schema_definition: 'SchemaDefinition' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::RegisterSchemaVersionOutput
    #   resp.data.schema_version_id #=> String
    #   resp.data.version_number #=> Integer
    #   resp.data.status #=> String, one of ["AVAILABLE", "PENDING", "FAILURE", "DELETING"]
    #
    def register_schema_version(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::RegisterSchemaVersionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::RegisterSchemaVersionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::RegisterSchemaVersion
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException]),
        data_parser: Parsers::RegisterSchemaVersion
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::RegisterSchemaVersion,
        stubs: @stubs,
        params_class: Params::RegisterSchemaVersionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :register_schema_version
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes a key value pair from the schema version metadata for the specified schema version ID.</p>
    #
    # @param [Hash] params
    #   See {Types::RemoveSchemaVersionMetadataInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>A wrapper structure that may contain the schema name and Amazon Resource Name (ARN).</p>
    #
    # @option params [SchemaVersionNumber] :schema_version_number
    #   <p>The version number of the schema.</p>
    #
    # @option params [String] :schema_version_id
    #   <p>The unique version ID of the schema version.</p>
    #
    # @option params [MetadataKeyValuePair] :metadata_key_value
    #   <p>The value of the metadata key.</p>
    #
    # @return [Types::RemoveSchemaVersionMetadataOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.remove_schema_version_metadata(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     },
    #     schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     },
    #     schema_version_id: 'SchemaVersionId',
    #     metadata_key_value: {
    #       metadata_key: 'MetadataKey',
    #       metadata_value: 'MetadataValue'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::RemoveSchemaVersionMetadataOutput
    #   resp.data.schema_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.registry_name #=> String
    #   resp.data.latest_version #=> Boolean
    #   resp.data.version_number #=> Integer
    #   resp.data.schema_version_id #=> String
    #   resp.data.metadata_key #=> String
    #   resp.data.metadata_value #=> String
    #
    def remove_schema_version_metadata(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::RemoveSchemaVersionMetadataInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::RemoveSchemaVersionMetadataInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::RemoveSchemaVersionMetadata
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::RemoveSchemaVersionMetadata
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::RemoveSchemaVersionMetadata,
        stubs: @stubs,
        params_class: Params::RemoveSchemaVersionMetadataOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :remove_schema_version_metadata
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Resets a bookmark entry.</p>
    #
    # @param [Hash] params
    #   See {Types::ResetJobBookmarkInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job in question.</p>
    #
    # @option params [String] :run_id
    #   <p>The unique run identifier associated with this job run.</p>
    #
    # @return [Types::ResetJobBookmarkOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.reset_job_bookmark(
    #     job_name: 'JobName', # required
    #     run_id: 'RunId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ResetJobBookmarkOutput
    #   resp.data.job_bookmark_entry #=> Types::JobBookmarkEntry
    #   resp.data.job_bookmark_entry.job_name #=> String
    #   resp.data.job_bookmark_entry.version #=> Integer
    #   resp.data.job_bookmark_entry.run #=> Integer
    #   resp.data.job_bookmark_entry.attempt #=> Integer
    #   resp.data.job_bookmark_entry.previous_run_id #=> String
    #   resp.data.job_bookmark_entry.run_id #=> String
    #   resp.data.job_bookmark_entry.job_bookmark #=> String
    #
    def reset_job_bookmark(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ResetJobBookmarkInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ResetJobBookmarkInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ResetJobBookmark
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ResetJobBookmark
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ResetJobBookmark,
        stubs: @stubs,
        params_class: Params::ResetJobBookmarkOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :reset_job_bookmark
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Restarts selected nodes of a previous partially completed workflow run and resumes the workflow run. The selected nodes and all nodes that are downstream from the selected nodes are run.</p>
    #
    # @param [Hash] params
    #   See {Types::ResumeWorkflowRunInput}.
    #
    # @option params [String] :name
    #   <p>The name of the workflow to resume.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the workflow run to resume.</p>
    #
    # @option params [Array<String>] :node_ids
    #   <p>A list of the node IDs for the nodes you want to restart. The nodes that are to be restarted must have a run attempt in the original run.</p>
    #
    # @return [Types::ResumeWorkflowRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.resume_workflow_run(
    #     name: 'Name', # required
    #     run_id: 'RunId', # required
    #     node_ids: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::ResumeWorkflowRunOutput
    #   resp.data.run_id #=> String
    #   resp.data.node_ids #=> Array<String>
    #   resp.data.node_ids[0] #=> String
    #
    def resume_workflow_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::ResumeWorkflowRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::ResumeWorkflowRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::ResumeWorkflowRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalWorkflowStateException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::ResumeWorkflowRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::ResumeWorkflowRun,
        stubs: @stubs,
        params_class: Params::ResumeWorkflowRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :resume_workflow_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Executes the statement.</p>
    #
    # @param [Hash] params
    #   See {Types::RunStatementInput}.
    #
    # @option params [String] :session_id
    #   <p>The Session Id of the statement to be run.</p>
    #
    # @option params [String] :code
    #   <p>The statement code to be run.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request.</p>
    #
    # @return [Types::RunStatementOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.run_statement(
    #     session_id: 'SessionId', # required
    #     code: 'Code', # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::RunStatementOutput
    #   resp.data.id #=> Integer
    #
    def run_statement(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::RunStatementInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::RunStatementInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::RunStatement
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::ValidationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException, Errors::AccessDeniedException]),
        data_parser: Parsers::RunStatement
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::RunStatement,
        stubs: @stubs,
        params_class: Params::RunStatementOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :run_statement
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Searches a set of tables based on properties in the table metadata as well as on the parent database. You can search against text or filter conditions. </p>
    #          <p>You can only get tables that you have access to based on the security policies defined in Lake Formation. You need at least a read-only access to the table for it to be returned. If you do not have access to all the columns in the table, these columns will not be searched against when returning the list of tables back to you. If you have access to the columns but not the data in the columns, those columns and the associated metadata for those columns will be included in the search. </p>
    #
    # @param [Hash] params
    #   See {Types::SearchTablesInput}.
    #
    # @option params [String] :catalog_id
    #   <p>A unique identifier, consisting of <code>
    #                  <i>account_id</i>
    #               </code>.</p>
    #
    # @option params [String] :next_token
    #   <p>A continuation token, included if this is a continuation call.</p>
    #
    # @option params [Array<PropertyPredicate>] :filters
    #   <p>A list of key-value pairs, and a comparator used to filter the search results. Returns all entities matching the predicate.</p>
    #   	
    #   	        <p>The <code>Comparator</code> member of the <code>PropertyPredicate</code> struct is used only for time fields, and can be omitted for other field types. Also, when comparing string values, such as when <code>Key=Name</code>, a fuzzy match algorithm is used. The <code>Key</code> field (for example, the value of the <code>Name</code> field) is split on certain punctuation characters, for example, -, :, #, etc. into tokens. Then each token is exact-match compared with the <code>Value</code> member of <code>PropertyPredicate</code>. For example, if <code>Key=Name</code> and <code>Value=link</code>, tables named <code>customer-link</code> and <code>xx-link-yy</code> are returned, but <code>xxlinkyy</code> is not returned.</p>
    #
    # @option params [String] :search_text
    #   <p>A string used for a text search.</p>
    #   	        <p>Specifying a value in quotes filters based on an exact match to the value.</p>
    #
    # @option params [Array<SortCriterion>] :sort_criteria
    #   <p>A list of criteria for sorting the results by a field name, in an ascending or descending order.</p>
    #
    # @option params [Integer] :max_results
    #   <p>The maximum number of tables to return in a single response.</p>
    #
    # @option params [String] :resource_share_type
    #   <p>Allows you to specify that you want to search the tables shared with your account. The allowable values are <code>FOREIGN</code> or <code>ALL</code>. </p>
    #   	
    #   	        <ul>
    #               <li>
    #                  <p>If set to <code>FOREIGN</code>, will search the tables shared with your account. </p>
    #               </li>
    #               <li>
    #                  <p>If set to <code>ALL</code>, will search the tables shared with your account, as well as the tables in yor local account. </p>
    #               </li>
    #            </ul>
    #
    # @return [Types::SearchTablesOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.search_tables(
    #     catalog_id: 'CatalogId',
    #     next_token: 'NextToken',
    #     filters: [
    #       {
    #         key: 'Key',
    #         value: 'Value',
    #         comparator: 'EQUALS' # accepts ["EQUALS", "GREATER_THAN", "LESS_THAN", "GREATER_THAN_EQUALS", "LESS_THAN_EQUALS"]
    #       }
    #     ],
    #     search_text: 'SearchText',
    #     sort_criteria: [
    #       {
    #         field_name: 'FieldName',
    #         sort: 'ASC' # accepts ["ASC", "DESC"]
    #       }
    #     ],
    #     max_results: 1,
    #     resource_share_type: 'FOREIGN' # accepts ["FOREIGN", "ALL"]
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::SearchTablesOutput
    #   resp.data.next_token #=> String
    #   resp.data.table_list #=> Array<Table>
    #   resp.data.table_list[0] #=> Types::Table
    #   resp.data.table_list[0].name #=> String
    #   resp.data.table_list[0].database_name #=> String
    #   resp.data.table_list[0].description #=> String
    #   resp.data.table_list[0].owner #=> String
    #   resp.data.table_list[0].create_time #=> Time
    #   resp.data.table_list[0].update_time #=> Time
    #   resp.data.table_list[0].last_access_time #=> Time
    #   resp.data.table_list[0].last_analyzed_time #=> Time
    #   resp.data.table_list[0].retention #=> Integer
    #   resp.data.table_list[0].storage_descriptor #=> Types::StorageDescriptor
    #   resp.data.table_list[0].storage_descriptor.columns #=> Array<Column>
    #   resp.data.table_list[0].storage_descriptor.columns[0] #=> Types::Column
    #   resp.data.table_list[0].storage_descriptor.columns[0].name #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].type #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].comment #=> String
    #   resp.data.table_list[0].storage_descriptor.columns[0].parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.columns[0].parameters['key'] #=> String
    #   resp.data.table_list[0].storage_descriptor.location #=> String
    #   resp.data.table_list[0].storage_descriptor.additional_locations #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.additional_locations[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.input_format #=> String
    #   resp.data.table_list[0].storage_descriptor.output_format #=> String
    #   resp.data.table_list[0].storage_descriptor.compressed #=> Boolean
    #   resp.data.table_list[0].storage_descriptor.number_of_buckets #=> Integer
    #   resp.data.table_list[0].storage_descriptor.serde_info #=> Types::SerDeInfo
    #   resp.data.table_list[0].storage_descriptor.serde_info.name #=> String
    #   resp.data.table_list[0].storage_descriptor.serde_info.serialization_library #=> String
    #   resp.data.table_list[0].storage_descriptor.serde_info.parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.bucket_columns #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.bucket_columns[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.sort_columns #=> Array<Order>
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0] #=> Types::Order
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0].column #=> String
    #   resp.data.table_list[0].storage_descriptor.sort_columns[0].sort_order #=> Integer
    #   resp.data.table_list[0].storage_descriptor.parameters #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info #=> Types::SkewedInfo
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_names #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_values #=> Array<String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_values[0] #=> String
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_value_location_maps #=> Hash<String, String>
    #   resp.data.table_list[0].storage_descriptor.skewed_info.skewed_column_value_location_maps['key'] #=> String
    #   resp.data.table_list[0].storage_descriptor.stored_as_sub_directories #=> Boolean
    #   resp.data.table_list[0].storage_descriptor.schema_reference #=> Types::SchemaReference
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id #=> Types::SchemaId
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.schema_arn #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.schema_name #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_id.registry_name #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_version_id #=> String
    #   resp.data.table_list[0].storage_descriptor.schema_reference.schema_version_number #=> Integer
    #   resp.data.table_list[0].partition_keys #=> Array<Column>
    #   resp.data.table_list[0].view_original_text #=> String
    #   resp.data.table_list[0].view_expanded_text #=> String
    #   resp.data.table_list[0].table_type #=> String
    #   resp.data.table_list[0].parameters #=> Hash<String, String>
    #   resp.data.table_list[0].created_by #=> String
    #   resp.data.table_list[0].is_registered_with_lake_formation #=> Boolean
    #   resp.data.table_list[0].target_table #=> Types::TableIdentifier
    #   resp.data.table_list[0].target_table.catalog_id #=> String
    #   resp.data.table_list[0].target_table.database_name #=> String
    #   resp.data.table_list[0].target_table.name #=> String
    #   resp.data.table_list[0].catalog_id #=> String
    #   resp.data.table_list[0].version_id #=> String
    #
    def search_tables(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::SearchTablesInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::SearchTablesInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::SearchTables
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::SearchTables
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::SearchTables,
        stubs: @stubs,
        params_class: Params::SearchTablesOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :search_tables
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts a new run of the specified blueprint.</p>
    #
    # @param [Hash] params
    #   See {Types::StartBlueprintRunInput}.
    #
    # @option params [String] :blueprint_name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [String] :parameters
    #   <p>Specifies the parameters as a <code>BlueprintParameters</code> object.</p>
    #
    # @option params [String] :role_arn
    #   <p>Specifies the IAM role used to create the workflow.</p>
    #
    # @return [Types::StartBlueprintRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_blueprint_run(
    #     blueprint_name: 'BlueprintName', # required
    #     parameters: 'Parameters',
    #     role_arn: 'RoleArn' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartBlueprintRunOutput
    #   resp.data.run_id #=> String
    #
    def start_blueprint_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartBlueprintRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartBlueprintRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartBlueprintRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalBlueprintStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::StartBlueprintRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartBlueprintRun,
        stubs: @stubs,
        params_class: Params::StartBlueprintRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_blueprint_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts a crawl using the specified crawler, regardless
    #       of what is scheduled. If the crawler is already running, returns a
    #       <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-exceptions.html#aws-glue-api-exceptions-CrawlerRunningException">CrawlerRunningException</a>.</p>
    #
    # @param [Hash] params
    #   See {Types::StartCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>Name of the crawler to start.</p>
    #
    # @return [Types::StartCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_crawler(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartCrawlerOutput
    #
    def start_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::OperationTimeoutException, Errors::CrawlerRunningException]),
        data_parser: Parsers::StartCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartCrawler,
        stubs: @stubs,
        params_class: Params::StartCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Changes the schedule state of the specified crawler to
    #       <code>SCHEDULED</code>, unless the crawler is already running or the
    #       schedule state is already <code>SCHEDULED</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::StartCrawlerScheduleInput}.
    #
    # @option params [String] :crawler_name
    #   <p>Name of the crawler to schedule.</p>
    #
    # @return [Types::StartCrawlerScheduleOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_crawler_schedule(
    #     crawler_name: 'CrawlerName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartCrawlerScheduleOutput
    #
    def start_crawler_schedule(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartCrawlerScheduleInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartCrawlerScheduleInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartCrawlerSchedule
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::SchedulerTransitioningException, Errors::NoScheduleException, Errors::EntityNotFoundException, Errors::OperationTimeoutException, Errors::SchedulerRunningException]),
        data_parser: Parsers::StartCrawlerSchedule
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartCrawlerSchedule,
        stubs: @stubs,
        params_class: Params::StartCrawlerScheduleOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_crawler_schedule
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Begins an asynchronous task to export all labeled data for a particular transform. This
    #       task is the only label-related API call that is not part of the typical active learning
    #       workflow. You typically use <code>StartExportLabelsTaskRun</code> when you want to work with
    #       all of your existing labels at the same time, such as when you want to remove or change labels
    #       that were previously submitted as truth. This API operation accepts the
    #         <code>TransformId</code> whose labels you want to export and an Amazon Simple Storage
    #       Service (Amazon S3) path to export the labels to. The operation returns a
    #         <code>TaskRunId</code>. You can check on the status of your task run by calling the
    #         <code>GetMLTaskRun</code> API.</p>
    #
    # @param [Hash] params
    #   See {Types::StartExportLabelsTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :output_s3_path
    #   <p>The Amazon S3 path where you export the labels.</p>
    #
    # @return [Types::StartExportLabelsTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_export_labels_task_run(
    #     transform_id: 'TransformId', # required
    #     output_s3_path: 'OutputS3Path' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartExportLabelsTaskRunOutput
    #   resp.data.task_run_id #=> String
    #
    def start_export_labels_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartExportLabelsTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartExportLabelsTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartExportLabelsTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::StartExportLabelsTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartExportLabelsTaskRun,
        stubs: @stubs,
        params_class: Params::StartExportLabelsTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_export_labels_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Enables you to provide additional labels (examples of truth) to be used to teach the
    #       machine learning transform and improve its quality. This API operation is generally used as
    #       part of the active learning workflow that starts with the
    #         <code>StartMLLabelingSetGenerationTaskRun</code> call and that ultimately results in
    #       improving the quality of your machine learning transform. </p>
    #
    #          <p>After the <code>StartMLLabelingSetGenerationTaskRun</code> finishes, Glue machine learning
    #       will have generated a series of questions for humans to answer. (Answering these questions is
    #       often called 'labeling' in the machine learning workflows). In the case of the
    #         <code>FindMatches</code> transform, these questions are of the form, “What is the correct
    #       way to group these rows together into groups composed entirely of matching records?” After the
    #       labeling process is finished, users upload their answers/labels with a call to
    #         <code>StartImportLabelsTaskRun</code>. After <code>StartImportLabelsTaskRun</code> finishes,
    #       all future runs of the machine learning transform use the new and improved labels and perform
    #       a higher-quality transformation.</p>
    #
    #          <p>By default, <code>StartMLLabelingSetGenerationTaskRun</code> continually learns from and
    #       combines all labels that you upload unless you set <code>Replace</code> to true. If you set
    #         <code>Replace</code> to true, <code>StartImportLabelsTaskRun</code> deletes and forgets all
    #       previously uploaded labels and learns only from the exact set that you upload. Replacing
    #       labels can be helpful if you realize that you previously uploaded incorrect labels, and you
    #       believe that they are having a negative effect on your transform quality.</p>
    #
    #          <p>You can check on the status of your task run by calling the <code>GetMLTaskRun</code>
    #       operation. </p>
    #
    # @param [Hash] params
    #   See {Types::StartImportLabelsTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :input_s3_path
    #   <p>The Amazon Simple Storage Service (Amazon S3) path from where you import the
    #         labels.</p>
    #
    # @option params [Boolean] :replace_all_labels
    #   <p>Indicates whether to overwrite your existing labels.</p>
    #
    # @return [Types::StartImportLabelsTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_import_labels_task_run(
    #     transform_id: 'TransformId', # required
    #     input_s3_path: 'InputS3Path', # required
    #     replace_all_labels: false
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartImportLabelsTaskRunOutput
    #   resp.data.task_run_id #=> String
    #
    def start_import_labels_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartImportLabelsTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartImportLabelsTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartImportLabelsTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::StartImportLabelsTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartImportLabelsTaskRun,
        stubs: @stubs,
        params_class: Params::StartImportLabelsTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_import_labels_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts a job run using a job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::StartJobRunInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition to use.</p>
    #
    # @option params [String] :job_run_id
    #   <p>The ID of a previous <code>JobRun</code> to retry.</p>
    #
    # @option params [Hash<String, String>] :arguments
    #   <p>The job arguments specifically for this run. For this job run, they replace the default arguments set in the job definition itself.</p>
    #            <p>You can specify arguments here that your own job-execution script
    #         consumes, as well as arguments that Glue itself consumes.</p>
    #            <p>For information about how to specify and consume your own Job arguments, see the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-calling.html">Calling Glue APIs in Python</a> topic in the developer guide.</p>
    #            <p>For information about the key-value pairs that Glue consumes to set up your job, see the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html">Special Parameters Used by Glue</a> topic in the developer guide.</p>
    #
    # @option params [Integer] :allocated_capacity
    #   <p>This field is deprecated. Use <code>MaxCapacity</code> instead.</p>
    #
    #            <p>The number of Glue data processing units (DPUs) to allocate to this JobRun.
    #         From 2 to 100 DPUs can be allocated; the default is 10. A DPU is a relative measure
    #         of processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory.
    #         For more information, see the <a href="https://aws.amazon.com/glue/pricing/">Glue
    #           pricing page</a>.</p>
    #
    # @option params [Integer] :timeout
    #   <p>The <code>JobRun</code> timeout in minutes. This is the maximum time that a job run can
    #         consume resources before it is terminated and enters <code>TIMEOUT</code> status. The default
    #         is 2,880 minutes (48 hours). This overrides the timeout value set in the parent job.</p>
    #
    # @option params [Float] :max_capacity
    #   <p>The number of Glue data processing units (DPUs) that can be allocated when this job runs. A DPU is a relative measure
    #         of processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory.
    #         For more information, see the <a href="https://aws.amazon.com/glue/pricing/">Glue
    #           pricing page</a>.</p>
    #
    #            <p>Do not set <code>Max Capacity</code> if using <code>WorkerType</code> and <code>NumberOfWorkers</code>.</p>
    #
    #            <p>The value that can be allocated for <code>MaxCapacity</code> depends on whether you are
    #         running a Python shell job, or an Apache Spark ETL job:</p>
    #            <ul>
    #               <li>
    #                  <p>When you specify a Python shell job (<code>JobCommand.Name</code>="pythonshell"), you can
    #           allocate either 0.0625 or 1 DPU. The default is 0.0625 DPU.</p>
    #               </li>
    #               <li>
    #                  <p>When you specify an Apache Spark ETL job (<code>JobCommand.Name</code>="glueetl"), you can allocate from 2 to 100 DPUs. The default is 10 DPUs. This job type cannot have a fractional DPU allocation.</p>
    #               </li>
    #            </ul>
    #
    # @option params [String] :security_configuration
    #   <p>The name of the <code>SecurityConfiguration</code> structure to be used with this job
    #         run.</p>
    #
    # @option params [NotificationProperty] :notification_property
    #   <p>Specifies configuration properties of a job run notification.</p>
    #
    # @option params [String] :worker_type
    #   <p>The type of predefined worker that is allocated when a job runs. Accepts a value of Standard, G.1X, or G.2X.</p>
    #            <ul>
    #               <li>
    #                  <p>For the <code>Standard</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 50GB disk, and 2 executors per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.1X</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 64GB disk, and 1 executor per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.2X</code> worker type, each worker provides 8 vCPU, 32 GB of memory and a 128GB disk, and 1 executor per worker.</p>
    #               </li>
    #            </ul>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers of a defined <code>workerType</code> that are allocated when a job runs.</p>
    #
    #            <p>The maximum number of workers you can define are 299 for <code>G.1X</code>, and 149 for <code>G.2X</code>. </p>
    #
    # @return [Types::StartJobRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_job_run(
    #     job_name: 'JobName', # required
    #     job_run_id: 'JobRunId',
    #     arguments: {
    #       'key' => 'value'
    #     },
    #     allocated_capacity: 1,
    #     timeout: 1,
    #     max_capacity: 1.0,
    #     security_configuration: 'SecurityConfiguration',
    #     notification_property: {
    #       notify_delay_after: 1
    #     },
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     number_of_workers: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartJobRunOutput
    #   resp.data.job_run_id #=> String
    #
    def start_job_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartJobRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartJobRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartJobRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::StartJobRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartJobRun,
        stubs: @stubs,
        params_class: Params::StartJobRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_job_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts a task to estimate the quality of the transform. </p>
    # 	
    # 	        <p>When you provide label sets as examples of truth, Glue machine learning uses some of
    #       those examples to learn from them. The rest of the labels are used as a test to estimate
    #       quality.</p>
    # 	
    # 	        <p>Returns a unique identifier for the run. You can call <code>GetMLTaskRun</code> to get more
    #       information about the stats of the <code>EvaluationTaskRun</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::StartMLEvaluationTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @return [Types::StartMLEvaluationTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_ml_evaluation_task_run(
    #     transform_id: 'TransformId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartMLEvaluationTaskRunOutput
    #   resp.data.task_run_id #=> String
    #
    def start_ml_evaluation_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartMLEvaluationTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartMLEvaluationTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartMLEvaluationTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::MLTransformNotReadyException]),
        data_parser: Parsers::StartMLEvaluationTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartMLEvaluationTaskRun,
        stubs: @stubs,
        params_class: Params::StartMLEvaluationTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_ml_evaluation_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts the active learning workflow for your machine learning transform to improve the
    #       transform's quality by generating label sets and adding labels.</p>
    # 	
    # 	        <p>When the <code>StartMLLabelingSetGenerationTaskRun</code> finishes, Glue will have
    #       generated a "labeling set" or a set of questions for humans to answer.</p>
    #
    #          <p>In the case of the <code>FindMatches</code> transform, these questions are of the form,
    #       “What is the correct way to group these rows together into groups composed entirely of
    #       matching records?” </p>
    #
    #          <p>After the labeling process is finished, you can upload your labels with a call to
    #         <code>StartImportLabelsTaskRun</code>. After <code>StartImportLabelsTaskRun</code> finishes,
    #       all future runs of the machine learning transform will use the new and improved labels and
    #       perform a higher-quality transformation.</p>
    #
    # @param [Hash] params
    #   See {Types::StartMLLabelingSetGenerationTaskRunInput}.
    #
    # @option params [String] :transform_id
    #   <p>The unique identifier of the machine learning transform.</p>
    #
    # @option params [String] :output_s3_path
    #   <p>The Amazon Simple Storage Service (Amazon S3) path where you generate the labeling
    #         set.</p>
    #
    # @return [Types::StartMLLabelingSetGenerationTaskRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_ml_labeling_set_generation_task_run(
    #     transform_id: 'TransformId', # required
    #     output_s3_path: 'OutputS3Path' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartMLLabelingSetGenerationTaskRunOutput
    #   resp.data.task_run_id #=> String
    #
    def start_ml_labeling_set_generation_task_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartMLLabelingSetGenerationTaskRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartMLLabelingSetGenerationTaskRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartMLLabelingSetGenerationTaskRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::StartMLLabelingSetGenerationTaskRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartMLLabelingSetGenerationTaskRun,
        stubs: @stubs,
        params_class: Params::StartMLLabelingSetGenerationTaskRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_ml_labeling_set_generation_task_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts an existing trigger. See <a href="https://docs.aws.amazon.com/glue/latest/dg/trigger-job.html">Triggering
    #       Jobs</a> for information about how different types of trigger are
    #       started.</p>
    #
    # @param [Hash] params
    #   See {Types::StartTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger to start.</p>
    #
    # @return [Types::StartTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_trigger(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartTriggerOutput
    #   resp.data.name #=> String
    #
    def start_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::StartTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartTrigger,
        stubs: @stubs,
        params_class: Params::StartTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Starts a new run of the specified workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::StartWorkflowRunInput}.
    #
    # @option params [String] :name
    #   <p>The name of the workflow to start.</p>
    #
    # @option params [Hash<String, String>] :run_properties
    #   <p>The workflow run properties for the new workflow run.</p>
    #
    # @return [Types::StartWorkflowRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.start_workflow_run(
    #     name: 'Name', # required
    #     run_properties: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StartWorkflowRunOutput
    #   resp.data.run_id #=> String
    #
    def start_workflow_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StartWorkflowRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StartWorkflowRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StartWorkflowRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::ConcurrentRunsExceededException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::StartWorkflowRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StartWorkflowRun,
        stubs: @stubs,
        params_class: Params::StartWorkflowRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :start_workflow_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>If the specified crawler is running, stops the crawl.</p>
    #
    # @param [Hash] params
    #   See {Types::StopCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>Name of the crawler to stop.</p>
    #
    # @return [Types::StopCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.stop_crawler(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StopCrawlerOutput
    #
    def stop_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StopCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StopCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StopCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::CrawlerNotRunningException, Errors::CrawlerStoppingException, Errors::OperationTimeoutException]),
        data_parser: Parsers::StopCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StopCrawler,
        stubs: @stubs,
        params_class: Params::StopCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :stop_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Sets the schedule state of the specified crawler to
    #       <code>NOT_SCHEDULED</code>, but does not stop the crawler if it is
    #       already running.</p>
    #
    # @param [Hash] params
    #   See {Types::StopCrawlerScheduleInput}.
    #
    # @option params [String] :crawler_name
    #   <p>Name of the crawler whose schedule state to set.</p>
    #
    # @return [Types::StopCrawlerScheduleOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.stop_crawler_schedule(
    #     crawler_name: 'CrawlerName' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StopCrawlerScheduleOutput
    #
    def stop_crawler_schedule(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StopCrawlerScheduleInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StopCrawlerScheduleInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StopCrawlerSchedule
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::SchedulerTransitioningException, Errors::EntityNotFoundException, Errors::OperationTimeoutException, Errors::SchedulerNotRunningException]),
        data_parser: Parsers::StopCrawlerSchedule
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StopCrawlerSchedule,
        stubs: @stubs,
        params_class: Params::StopCrawlerScheduleOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :stop_crawler_schedule
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Stops the session.</p>
    #
    # @param [Hash] params
    #   See {Types::StopSessionInput}.
    #
    # @option params [String] :id
    #   <p>The ID of the session to be stopped.</p>
    #
    # @option params [String] :request_origin
    #   <p>The origin of the request.</p>
    #
    # @return [Types::StopSessionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.stop_session(
    #     id: 'Id', # required
    #     request_origin: 'RequestOrigin'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StopSessionOutput
    #   resp.data.id #=> String
    #
    def stop_session(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StopSessionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StopSessionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StopSession
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::IllegalSessionStateException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::StopSession
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StopSession,
        stubs: @stubs,
        params_class: Params::StopSessionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :stop_session
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Stops a specified trigger.</p>
    #
    # @param [Hash] params
    #   See {Types::StopTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger to stop.</p>
    #
    # @return [Types::StopTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.stop_trigger(
    #     name: 'Name' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StopTriggerOutput
    #   resp.data.name #=> String
    #
    def stop_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StopTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StopTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StopTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::StopTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StopTrigger,
        stubs: @stubs,
        params_class: Params::StopTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :stop_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Stops the execution of the specified workflow run.</p>
    #
    # @param [Hash] params
    #   See {Types::StopWorkflowRunInput}.
    #
    # @option params [String] :name
    #   <p>The name of the workflow to stop.</p>
    #
    # @option params [String] :run_id
    #   <p>The ID of the workflow run to stop.</p>
    #
    # @return [Types::StopWorkflowRunOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.stop_workflow_run(
    #     name: 'Name', # required
    #     run_id: 'RunId' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::StopWorkflowRunOutput
    #
    def stop_workflow_run(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::StopWorkflowRunInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::StopWorkflowRunInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::StopWorkflowRun
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::IllegalWorkflowStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::StopWorkflowRun
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::StopWorkflowRun,
        stubs: @stubs,
        params_class: Params::StopWorkflowRunOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :stop_workflow_run
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Adds tags to a resource. A tag is a label you can assign to an Amazon Web Services resource.
    #       In Glue, you can tag only certain resources. For information about what
    #       resources you can tag, see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html">Amazon Web Services Tags in Glue</a>.</p>
    #
    # @param [Hash] params
    #   See {Types::TagResourceInput}.
    #
    # @option params [String] :resource_arn
    #   <p>The ARN of the Glue resource to which to add the tags. For more
    #         information about Glue resource ARNs, see the <a href="https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-common.html#aws-glue-api-regex-aws-glue-arn-id">Glue ARN string pattern</a>.</p>
    #
    # @option params [Hash<String, String>] :tags_to_add
    #   <p>Tags to add to this resource.</p>
    #
    # @return [Types::TagResourceOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.tag_resource(
    #     resource_arn: 'ResourceArn', # required
    #     tags_to_add: {
    #       'key' => 'value'
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::TagResourceOutput
    #
    def tag_resource(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::TagResourceInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::TagResourceInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::TagResource
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::TagResource
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::TagResource,
        stubs: @stubs,
        params_class: Params::TagResourceOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :tag_resource
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Removes tags from a resource.</p>
    #
    # @param [Hash] params
    #   See {Types::UntagResourceInput}.
    #
    # @option params [String] :resource_arn
    #   <p>The Amazon Resource Name (ARN) of the resource from which to remove the tags.</p>
    #
    # @option params [Array<String>] :tags_to_remove
    #   <p>Tags to remove from this resource.</p>
    #
    # @return [Types::UntagResourceOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.untag_resource(
    #     resource_arn: 'ResourceArn', # required
    #     tags_to_remove: [
    #       'member'
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UntagResourceOutput
    #
    def untag_resource(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UntagResourceInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UntagResourceInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UntagResource
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UntagResource
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UntagResource,
        stubs: @stubs,
        params_class: Params::UntagResourceOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :untag_resource
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a registered blueprint.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateBlueprintInput}.
    #
    # @option params [String] :name
    #   <p>The name of the blueprint.</p>
    #
    # @option params [String] :description
    #   <p>A description of the blueprint.</p>
    #
    # @option params [String] :blueprint_location
    #   <p>Specifies a path in Amazon S3 where the blueprint is published.</p>
    #
    # @return [Types::UpdateBlueprintOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_blueprint(
    #     name: 'Name', # required
    #     description: 'Description',
    #     blueprint_location: 'BlueprintLocation' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateBlueprintOutput
    #   resp.data.name #=> String
    #
    def update_blueprint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateBlueprintInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateBlueprintInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateBlueprint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::IllegalBlueprintStateException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateBlueprint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateBlueprint,
        stubs: @stubs,
        params_class: Params::UpdateBlueprintOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_blueprint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Modifies an existing classifier (a <code>GrokClassifier</code>,
    #       an <code>XMLClassifier</code>, a <code>JsonClassifier</code>, or a <code>CsvClassifier</code>, depending on
    #       which field is present).</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateClassifierInput}.
    #
    # @option params [UpdateGrokClassifierRequest] :grok_classifier
    #   <p>A <code>GrokClassifier</code> object with updated fields.</p>
    #
    # @option params [UpdateXMLClassifierRequest] :xml_classifier
    #   <p>An <code>XMLClassifier</code> object with updated fields.</p>
    #
    # @option params [UpdateJsonClassifierRequest] :json_classifier
    #   <p>A <code>JsonClassifier</code> object with updated fields.</p>
    #
    # @option params [UpdateCsvClassifierRequest] :csv_classifier
    #   <p>A <code>CsvClassifier</code> object with updated fields.</p>
    #
    # @return [Types::UpdateClassifierOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_classifier(
    #     grok_classifier: {
    #       name: 'Name', # required
    #       classification: 'Classification',
    #       grok_pattern: 'GrokPattern',
    #       custom_patterns: 'CustomPatterns'
    #     },
    #     xml_classifier: {
    #       name: 'Name', # required
    #       classification: 'Classification',
    #       row_tag: 'RowTag'
    #     },
    #     json_classifier: {
    #       name: 'Name', # required
    #       json_path: 'JsonPath'
    #     },
    #     csv_classifier: {
    #       name: 'Name', # required
    #       delimiter: 'Delimiter',
    #       quote_symbol: 'QuoteSymbol',
    #       contains_header: 'UNKNOWN', # accepts ["UNKNOWN", "PRESENT", "ABSENT"]
    #       header: [
    #         'member'
    #       ],
    #       disable_value_trimming: false,
    #       allow_single_column: false
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateClassifierOutput
    #
    def update_classifier(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateClassifierInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateClassifierInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateClassifier
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::VersionMismatchException]),
        data_parser: Parsers::UpdateClassifier
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateClassifier,
        stubs: @stubs,
        params_class: Params::UpdateClassifierOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_classifier
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates or updates partition statistics of columns.</p>
    # 	
    #          <p>The Identity and Access Management (IAM) permission required for this operation is <code>UpdatePartition</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateColumnStatisticsForPartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<String>] :partition_values
    #   <p>A list of partition values identifying the partition.</p>
    #
    # @option params [Array<ColumnStatistics>] :column_statistics_list
    #   <p>A list of the column statistics.</p>
    #
    # @return [Types::UpdateColumnStatisticsForPartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_column_statistics_for_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_values: [
    #       'member'
    #     ], # required
    #     column_statistics_list: [
    #       {
    #         column_name: 'ColumnName', # required
    #         column_type: 'ColumnType', # required
    #         analyzed_time: Time.now, # required
    #         statistics_data: {
    #           type: 'BOOLEAN', # required - accepts ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #           boolean_column_statistics_data: {
    #             number_of_trues: 1, # required
    #             number_of_falses: 1, # required
    #             number_of_nulls: 1 # required
    #           },
    #           date_column_statistics_data: {
    #             minimum_value: Time.now,
    #             maximum_value: Time.now,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           decimal_column_statistics_data: {
    #             minimum_value: {
    #               unscaled_value: 'UnscaledValue', # required
    #               scale: 1 # required
    #             },
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           double_column_statistics_data: {
    #             minimum_value: 1.0,
    #             maximum_value: 1.0,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           long_column_statistics_data: {
    #             minimum_value: 1,
    #             maximum_value: 1,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           string_column_statistics_data: {
    #             maximum_length: 1, # required
    #             average_length: 1.0, # required
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           binary_column_statistics_data: {
    #             maximum_length: 1, # required
    #             average_length: 1.0, # required
    #             number_of_nulls: 1 # required
    #           }
    #         } # required
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateColumnStatisticsForPartitionOutput
    #   resp.data.errors #=> Array<ColumnStatisticsError>
    #   resp.data.errors[0] #=> Types::ColumnStatisticsError
    #   resp.data.errors[0].column_statistics #=> Types::ColumnStatistics
    #   resp.data.errors[0].column_statistics.column_name #=> String
    #   resp.data.errors[0].column_statistics.column_type #=> String
    #   resp.data.errors[0].column_statistics.analyzed_time #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data #=> Types::ColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.type #=> String, one of ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data #=> Types::BooleanColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_trues #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_falses #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data #=> Types::DateColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.minimum_value #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.maximum_value #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data #=> Types::DecimalColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value #=> Types::DecimalNumber
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value.unscaled_value #=> String
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value.scale #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.maximum_value #=> Types::DecimalNumber
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data #=> Types::DoubleColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.minimum_value #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.maximum_value #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data #=> Types::LongColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.minimum_value #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.maximum_value #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data #=> Types::StringColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.maximum_length #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.average_length #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data #=> Types::BinaryColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.maximum_length #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.average_length #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].error #=> Types::ErrorDetail
    #   resp.data.errors[0].error.error_code #=> String
    #   resp.data.errors[0].error.error_message #=> String
    #
    def update_column_statistics_for_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateColumnStatisticsForPartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateColumnStatisticsForPartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateColumnStatisticsForPartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateColumnStatisticsForPartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateColumnStatisticsForPartition,
        stubs: @stubs,
        params_class: Params::UpdateColumnStatisticsForPartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_column_statistics_for_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Creates or updates table statistics of columns.</p>
    # 	
    # 	        <p>The Identity and Access Management (IAM) permission required for this operation is <code>UpdateTable</code>.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateColumnStatisticsForTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partitions in question reside.
    #         If none is supplied, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the partitions reside.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the partitions' table.</p>
    #
    # @option params [Array<ColumnStatistics>] :column_statistics_list
    #   <p>A list of the column statistics.</p>
    #
    # @return [Types::UpdateColumnStatisticsForTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_column_statistics_for_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     column_statistics_list: [
    #       {
    #         column_name: 'ColumnName', # required
    #         column_type: 'ColumnType', # required
    #         analyzed_time: Time.now, # required
    #         statistics_data: {
    #           type: 'BOOLEAN', # required - accepts ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #           boolean_column_statistics_data: {
    #             number_of_trues: 1, # required
    #             number_of_falses: 1, # required
    #             number_of_nulls: 1 # required
    #           },
    #           date_column_statistics_data: {
    #             minimum_value: Time.now,
    #             maximum_value: Time.now,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           decimal_column_statistics_data: {
    #             minimum_value: {
    #               unscaled_value: 'UnscaledValue', # required
    #               scale: 1 # required
    #             },
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           double_column_statistics_data: {
    #             minimum_value: 1.0,
    #             maximum_value: 1.0,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           long_column_statistics_data: {
    #             minimum_value: 1,
    #             maximum_value: 1,
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           string_column_statistics_data: {
    #             maximum_length: 1, # required
    #             average_length: 1.0, # required
    #             number_of_nulls: 1, # required
    #             number_of_distinct_values: 1 # required
    #           },
    #           binary_column_statistics_data: {
    #             maximum_length: 1, # required
    #             average_length: 1.0, # required
    #             number_of_nulls: 1 # required
    #           }
    #         } # required
    #       }
    #     ] # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateColumnStatisticsForTableOutput
    #   resp.data.errors #=> Array<ColumnStatisticsError>
    #   resp.data.errors[0] #=> Types::ColumnStatisticsError
    #   resp.data.errors[0].column_statistics #=> Types::ColumnStatistics
    #   resp.data.errors[0].column_statistics.column_name #=> String
    #   resp.data.errors[0].column_statistics.column_type #=> String
    #   resp.data.errors[0].column_statistics.analyzed_time #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data #=> Types::ColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.type #=> String, one of ["BOOLEAN", "DATE", "DECIMAL", "DOUBLE", "LONG", "STRING", "BINARY"]
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data #=> Types::BooleanColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_trues #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_falses #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.boolean_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data #=> Types::DateColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.minimum_value #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.maximum_value #=> Time
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.date_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data #=> Types::DecimalColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value #=> Types::DecimalNumber
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value.unscaled_value #=> String
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.minimum_value.scale #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.maximum_value #=> Types::DecimalNumber
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.decimal_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data #=> Types::DoubleColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.minimum_value #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.maximum_value #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.double_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data #=> Types::LongColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.minimum_value #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.maximum_value #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.long_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data #=> Types::StringColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.maximum_length #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.average_length #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.string_column_statistics_data.number_of_distinct_values #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data #=> Types::BinaryColumnStatisticsData
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.maximum_length #=> Integer
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.average_length #=> Float
    #   resp.data.errors[0].column_statistics.statistics_data.binary_column_statistics_data.number_of_nulls #=> Integer
    #   resp.data.errors[0].error #=> Types::ErrorDetail
    #   resp.data.errors[0].error.error_code #=> String
    #   resp.data.errors[0].error.error_message #=> String
    #
    def update_column_statistics_for_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateColumnStatisticsForTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateColumnStatisticsForTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateColumnStatisticsForTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateColumnStatisticsForTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateColumnStatisticsForTable,
        stubs: @stubs,
        params_class: Params::UpdateColumnStatisticsForTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_column_statistics_for_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a connection definition in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateConnectionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the connection resides. If none is provided, the Amazon Web Services
    #         account ID is used by default.</p>
    #
    # @option params [String] :name
    #   <p>The name of the connection definition to update.</p>
    #
    # @option params [ConnectionInput] :connection_input
    #   <p>A <code>ConnectionInput</code> object that redefines the connection
    #         in question.</p>
    #
    # @return [Types::UpdateConnectionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_connection(
    #     catalog_id: 'CatalogId',
    #     name: 'Name', # required
    #     connection_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       connection_type: 'JDBC', # required - accepts ["JDBC", "SFTP", "MONGODB", "KAFKA", "NETWORK", "MARKETPLACE", "CUSTOM"]
    #       match_criteria: [
    #         'member'
    #       ],
    #       connection_properties: {
    #         'key' => 'value'
    #       }, # required
    #       physical_connection_requirements: {
    #         subnet_id: 'SubnetId',
    #         security_group_id_list: [
    #           'member'
    #         ],
    #         availability_zone: 'AvailabilityZone'
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateConnectionOutput
    #
    def update_connection(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateConnectionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateConnectionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateConnection
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateConnection
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateConnection,
        stubs: @stubs,
        params_class: Params::UpdateConnectionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_connection
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a crawler. If a crawler is
    #       running, you must stop it using <code>StopCrawler</code> before updating
    #       it.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateCrawlerInput}.
    #
    # @option params [String] :name
    #   <p>Name of the new crawler.</p>
    #
    # @option params [String] :role
    #   <p>The IAM role or Amazon Resource Name (ARN) of an IAM role that is used by the new crawler
    #         to access customer resources.</p>
    #
    # @option params [String] :database_name
    #   <p>The Glue database where results are stored, such as:
    #           <code>arn:aws:daylight:us-east-1::database/sometable/*</code>.</p>
    #
    # @option params [String] :description
    #   <p>A description of the new crawler.</p>
    #
    # @option params [CrawlerTargets] :targets
    #   <p>A list of targets to crawl.</p>
    #
    # @option params [String] :schedule
    #   <p>A <code>cron</code> expression used to specify the schedule (see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html">Time-Based Schedules for Jobs and Crawlers</a>. For example, to run
    #         something every day at 12:15 UTC, you would specify:
    #         <code>cron(15 12 * * ? *)</code>.</p>
    #
    # @option params [Array<String>] :classifiers
    #   <p>A list of custom classifiers that the user
    #         has registered. By default, all built-in classifiers are included in a crawl,
    #         but these custom classifiers always override the default classifiers
    #         for a given classification.</p>
    #
    # @option params [String] :table_prefix
    #   <p>The table prefix used for catalog tables that are created.</p>
    #
    # @option params [SchemaChangePolicy] :schema_change_policy
    #   <p>The policy for the crawler's update and deletion behavior.</p>
    #
    # @option params [RecrawlPolicy] :recrawl_policy
    #   <p>A policy that specifies whether to crawl the entire dataset again, or to crawl only folders that were added since the last crawler run.</p>
    #
    # @option params [LineageConfiguration] :lineage_configuration
    #   <p>Specifies data lineage configuration settings for the crawler.</p>
    #
    # @option params [LakeFormationConfiguration] :lake_formation_configuration
    #   <p>Specifies AWS Lake Formation configuration settings for the crawler.</p>
    #
    # @option params [String] :configuration
    #   <p>Crawler configuration information. This versioned JSON string allows users
    #           to specify aspects of a crawler's behavior.
    #           For more information, see <a href="https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html">Configuring a Crawler</a>.</p>
    #
    # @option params [String] :crawler_security_configuration
    #   <p>The name of the <code>SecurityConfiguration</code> structure to be used by this
    #         crawler.</p>
    #
    # @return [Types::UpdateCrawlerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_crawler(
    #     name: 'Name', # required
    #     role: 'Role',
    #     database_name: 'DatabaseName',
    #     description: 'Description',
    #     targets: {
    #       s3_targets: [
    #         {
    #           path: 'Path',
    #           exclusions: [
    #             'member'
    #           ],
    #           connection_name: 'ConnectionName',
    #           sample_size: 1,
    #           event_queue_arn: 'EventQueueArn',
    #           dlq_event_queue_arn: 'DlqEventQueueArn'
    #         }
    #       ],
    #       jdbc_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           path: 'Path',
    #         }
    #       ],
    #       mongo_db_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           path: 'Path',
    #           scan_all: false
    #         }
    #       ],
    #       dynamo_db_targets: [
    #         {
    #           path: 'Path',
    #           scan_all: false,
    #           scan_rate: 1.0
    #         }
    #       ],
    #       catalog_targets: [
    #         {
    #           database_name: 'DatabaseName', # required
    #           tables: [
    #             'member'
    #           ], # required
    #           connection_name: 'ConnectionName'
    #         }
    #       ],
    #       delta_targets: [
    #         {
    #           connection_name: 'ConnectionName',
    #           write_manifest: false
    #         }
    #       ]
    #     },
    #     schedule: 'Schedule',
    #     classifiers: [
    #       'member'
    #     ],
    #     table_prefix: 'TablePrefix',
    #     schema_change_policy: {
    #       update_behavior: 'LOG', # accepts ["LOG", "UPDATE_IN_DATABASE"]
    #       delete_behavior: 'LOG' # accepts ["LOG", "DELETE_FROM_DATABASE", "DEPRECATE_IN_DATABASE"]
    #     },
    #     recrawl_policy: {
    #       recrawl_behavior: 'CRAWL_EVERYTHING' # accepts ["CRAWL_EVERYTHING", "CRAWL_NEW_FOLDERS_ONLY", "CRAWL_EVENT_MODE"]
    #     },
    #     lineage_configuration: {
    #       crawler_lineage_settings: 'ENABLE' # accepts ["ENABLE", "DISABLE"]
    #     },
    #     lake_formation_configuration: {
    #       use_lake_formation_credentials: false,
    #       account_id: 'AccountId'
    #     },
    #     configuration: 'Configuration',
    #     crawler_security_configuration: 'CrawlerSecurityConfiguration'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateCrawlerOutput
    #
    def update_crawler(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateCrawlerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateCrawlerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateCrawler
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::VersionMismatchException, Errors::CrawlerRunningException]),
        data_parser: Parsers::UpdateCrawler
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateCrawler,
        stubs: @stubs,
        params_class: Params::UpdateCrawlerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_crawler
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates the schedule of a crawler using a <code>cron</code> expression. </p>
    #
    # @param [Hash] params
    #   See {Types::UpdateCrawlerScheduleInput}.
    #
    # @option params [String] :crawler_name
    #   <p>The name of the crawler whose schedule to update.</p>
    #
    # @option params [String] :schedule
    #   <p>The updated <code>cron</code> expression used to specify the schedule (see <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html">Time-Based Schedules for Jobs and Crawlers</a>. For example, to run
    #         something every day at 12:15 UTC, you would specify:
    #         <code>cron(15 12 * * ? *)</code>.</p>
    #
    # @return [Types::UpdateCrawlerScheduleOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_crawler_schedule(
    #     crawler_name: 'CrawlerName', # required
    #     schedule: 'Schedule'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateCrawlerScheduleOutput
    #
    def update_crawler_schedule(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateCrawlerScheduleInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateCrawlerScheduleInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateCrawlerSchedule
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::SchedulerTransitioningException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::VersionMismatchException]),
        data_parser: Parsers::UpdateCrawlerSchedule
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateCrawlerSchedule,
        stubs: @stubs,
        params_class: Params::UpdateCrawlerScheduleOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_crawler_schedule
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing database definition in a Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateDatabaseInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog in which the metadata database resides. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :name
    #   <p>The name of the database to update in the catalog. For Hive
    #         compatibility, this is folded to lowercase.</p>
    #
    # @option params [DatabaseInput] :database_input
    #   <p>A <code>DatabaseInput</code> object specifying the new definition
    #         of the metadata database in the catalog.</p>
    #
    # @return [Types::UpdateDatabaseOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_database(
    #     catalog_id: 'CatalogId',
    #     name: 'Name', # required
    #     database_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       location_uri: 'LocationUri',
    #       parameters: {
    #         'key' => 'value'
    #       },
    #       create_table_default_permissions: [
    #         {
    #           principal: {
    #             data_lake_principal_identifier: 'DataLakePrincipalIdentifier'
    #           },
    #           permissions: [
    #             'ALL' # accepts ["ALL", "SELECT", "ALTER", "DROP", "DELETE", "INSERT", "CREATE_DATABASE", "CREATE_TABLE", "DATA_LOCATION_ACCESS"]
    #           ]
    #         }
    #       ],
    #       target_database: {
    #         catalog_id: 'CatalogId',
    #         database_name: 'DatabaseName'
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateDatabaseOutput
    #
    def update_database(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateDatabaseInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateDatabaseInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateDatabase
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateDatabase
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateDatabase,
        stubs: @stubs,
        params_class: Params::UpdateDatabaseOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_database
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a specified development endpoint.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateDevEndpointInput}.
    #
    # @option params [String] :endpoint_name
    #   <p>The name of the <code>DevEndpoint</code> to be updated.</p>
    #
    # @option params [String] :public_key
    #   <p>The public key for the <code>DevEndpoint</code> to use.</p>
    #
    # @option params [Array<String>] :add_public_keys
    #   <p>The list of public keys for the <code>DevEndpoint</code> to use.</p>
    #
    # @option params [Array<String>] :delete_public_keys
    #   <p>The list of public keys to be deleted from the <code>DevEndpoint</code>.</p>
    #
    # @option params [DevEndpointCustomLibraries] :custom_libraries
    #   <p>Custom Python or Java libraries to be loaded in the <code>DevEndpoint</code>.</p>
    #
    # @option params [Boolean] :update_etl_libraries
    #   <p>
    #               <code>True</code> if the list of custom libraries to be loaded in the development endpoint
    #         needs to be updated, or <code>False</code> if otherwise.</p>
    #
    # @option params [Array<String>] :delete_arguments
    #   <p>The list of argument keys to be deleted from the map of arguments used to configure the
    #           <code>DevEndpoint</code>.</p>
    #
    # @option params [Hash<String, String>] :add_arguments
    #   <p>The map of arguments to add the map of arguments used to configure the
    #           <code>DevEndpoint</code>.</p>
    #   		
    #   	        <p>Valid arguments are:</p>
    #   	        <ul>
    #               <li>
    #                  <p>
    #                     <code>"--enable-glue-datacatalog": ""</code>
    #                  </p>
    #               </li>
    #            </ul>
    #   	
    #            <p>You can specify a version of Python support for development endpoints by using the <code>Arguments</code> parameter in the <code>CreateDevEndpoint</code> or <code>UpdateDevEndpoint</code> APIs. If no arguments are provided, the version defaults to Python 2.</p>
    #
    # @return [Types::UpdateDevEndpointOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_dev_endpoint(
    #     endpoint_name: 'EndpointName', # required
    #     public_key: 'PublicKey',
    #     add_public_keys: [
    #       'member'
    #     ],
    #     custom_libraries: {
    #       extra_python_libs_s3_path: 'ExtraPythonLibsS3Path',
    #       extra_jars_s3_path: 'ExtraJarsS3Path'
    #     },
    #     update_etl_libraries: false,
    #     delete_arguments: [
    #       'member'
    #     ],
    #     add_arguments: {
    #       'key' => 'value'
    #     }
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateDevEndpointOutput
    #
    def update_dev_endpoint(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateDevEndpointInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateDevEndpointInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateDevEndpoint
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::ValidationException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateDevEndpoint
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateDevEndpoint,
        stubs: @stubs,
        params_class: Params::UpdateDevEndpointOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_dev_endpoint
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing job definition.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateJobInput}.
    #
    # @option params [String] :job_name
    #   <p>The name of the job definition to update.</p>
    #
    # @option params [JobUpdate] :job_update
    #   <p>Specifies the values with which to update the job definition.</p>
    #
    # @return [Types::UpdateJobOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_job(
    #     job_name: 'JobName', # required
    #     job_update: {
    #       description: 'Description',
    #       log_uri: 'LogUri',
    #       role: 'Role',
    #       execution_property: {
    #         max_concurrent_runs: 1
    #       },
    #       command: {
    #         name: 'Name',
    #         script_location: 'ScriptLocation',
    #         python_version: 'PythonVersion'
    #       },
    #       default_arguments: {
    #         'key' => 'value'
    #       },
    #       connections: {
    #         connections: [
    #           'member'
    #         ]
    #       },
    #       max_retries: 1,
    #       allocated_capacity: 1,
    #       timeout: 1,
    #       max_capacity: 1.0,
    #       worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #       number_of_workers: 1,
    #       security_configuration: 'SecurityConfiguration',
    #       notification_property: {
    #         notify_delay_after: 1
    #       },
    #       glue_version: 'GlueVersion',
    #       code_gen_configuration_nodes: {
    #         'key' => {
    #           athena_connector_source: {
    #             name: 'Name', # required
    #             connection_name: 'ConnectionName', # required
    #             connector_name: 'ConnectorName', # required
    #             connection_type: 'ConnectionType', # required
    #             connection_table: 'ConnectionTable',
    #             schema_name: 'SchemaName', # required
    #             output_schemas: [
    #               {
    #                 columns: [
    #                   {
    #                     name: 'Name', # required
    #                     type: 'Type'
    #                   }
    #                 ]
    #               }
    #             ]
    #           },
    #           jdbc_connector_source: {
    #             name: 'Name', # required
    #             connection_name: 'ConnectionName', # required
    #             connector_name: 'ConnectorName', # required
    #             connection_type: 'ConnectionType', # required
    #             additional_options: {
    #               filter_predicate: 'FilterPredicate',
    #               partition_column: 'PartitionColumn',
    #               lower_bound: 1,
    #               upper_bound: 1,
    #               num_partitions: 1,
    #               job_bookmark_keys: [
    #                 'member'
    #               ],
    #               job_bookmark_keys_sort_order: 'JobBookmarkKeysSortOrder',
    #               data_type_mapping: {
    #                 'key' => 'DATE' # accepts ["DATE", "STRING", "TIMESTAMP", "INT", "FLOAT", "LONG", "BIGDECIMAL", "BYTE", "SHORT", "DOUBLE"]
    #               }
    #             },
    #             connection_table: 'ConnectionTable',
    #             query: 'Query',
    #           },
    #           spark_connector_source: {
    #             name: 'Name', # required
    #             connection_name: 'ConnectionName', # required
    #             connector_name: 'ConnectorName', # required
    #             connection_type: 'ConnectionType', # required
    #             additional_options: {
    #               'key' => 'value'
    #             },
    #           },
    #           catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           redshift_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table', # required
    #             redshift_tmp_dir: 'RedshiftTmpDir',
    #             tmp_dir_iam_role: 'TmpDirIAMRole'
    #           },
    #           s3_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table', # required
    #             partition_predicate: 'PartitionPredicate',
    #             additional_options: {
    #               bounded_size: 1,
    #               bounded_files: 1
    #             }
    #           },
    #           s3_csv_source: {
    #             name: 'Name', # required
    #             compression_type: 'gzip', # accepts ["gzip", "bzip2"]
    #             group_size: 'GroupSize',
    #             group_files: 'GroupFiles',
    #             recurse: false,
    #             max_band: 1,
    #             max_files_in_band: 1,
    #             additional_options: {
    #               bounded_size: 1,
    #               bounded_files: 1,
    #               enable_sample_path: false,
    #               sample_path: 'SamplePath'
    #             },
    #             separator: 'comma', # required - accepts ["comma", "ctrla", "pipe", "semicolon", "tab"]
    #             escaper: 'Escaper',
    #             quote_char: 'quote', # required - accepts ["quote", "quillemet", "single_quote", "disabled"]
    #             multiline: false,
    #             with_header: false,
    #             write_header: false,
    #             skip_first: false,
    #             optimize_performance: false,
    #           },
    #           s3_json_source: {
    #             name: 'Name', # required
    #             compression_type: 'gzip', # accepts ["gzip", "bzip2"]
    #             group_size: 'GroupSize',
    #             group_files: 'GroupFiles',
    #             recurse: false,
    #             max_band: 1,
    #             max_files_in_band: 1,
    #             json_path: 'JsonPath',
    #             multiline: false,
    #           },
    #           s3_parquet_source: {
    #             name: 'Name', # required
    #             compression_type: 'snappy', # accepts ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #             group_size: 'GroupSize',
    #             group_files: 'GroupFiles',
    #             recurse: false,
    #             max_band: 1,
    #             max_files_in_band: 1,
    #           },
    #           relational_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           dynamo_db_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           jdbc_connector_target: {
    #             name: 'Name', # required
    #             inputs: [
    #               'member'
    #             ], # required
    #             connection_name: 'ConnectionName', # required
    #             connection_table: 'ConnectionTable', # required
    #             connector_name: 'ConnectorName', # required
    #             connection_type: 'ConnectionType', # required
    #           },
    #           spark_connector_target: {
    #             name: 'Name', # required
    #             connection_name: 'ConnectionName', # required
    #             connector_name: 'ConnectorName', # required
    #             connection_type: 'ConnectionType', # required
    #           },
    #           catalog_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           redshift_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table', # required
    #             redshift_tmp_dir: 'RedshiftTmpDir',
    #             tmp_dir_iam_role: 'TmpDirIAMRole',
    #             upsert_redshift_options: {
    #               table_location: 'TableLocation',
    #               connection_name: 'ConnectionName',
    #               upsert_keys: [
    #                 'member'
    #               ]
    #             }
    #           },
    #           s3_catalog_target: {
    #             name: 'Name', # required
    #             table: 'Table', # required
    #             database: 'Database', # required
    #             schema_change_policy: {
    #               enable_update_catalog: false,
    #               update_behavior: 'UPDATE_IN_DATABASE' # accepts ["UPDATE_IN_DATABASE", "LOG"]
    #             }
    #           },
    #           s3_glue_parquet_target: {
    #             name: 'Name', # required
    #             path: 'Path', # required
    #             compression: 'snappy', # accepts ["snappy", "lzo", "gzip", "uncompressed", "none"]
    #             schema_change_policy: {
    #               enable_update_catalog: false,
    #               update_behavior: 'UPDATE_IN_DATABASE', # accepts ["UPDATE_IN_DATABASE", "LOG"]
    #               table: 'Table',
    #               database: 'Database'
    #             }
    #           },
    #           s3_direct_target: {
    #             name: 'Name', # required
    #             path: 'Path', # required
    #             compression: 'Compression',
    #             format: 'json', # required - accepts ["json", "csv", "avro", "orc", "parquet"]
    #           },
    #           apply_mapping: {
    #             name: 'Name', # required
    #             mapping: [
    #               {
    #                 to_key: 'ToKey',
    #                 from_type: 'FromType',
    #                 to_type: 'ToType',
    #                 dropped: false,
    #               }
    #             ] # required
    #           },
    #           select_fields: {
    #             name: 'Name', # required
    #           },
    #           drop_fields: {
    #             name: 'Name', # required
    #           },
    #           rename_field: {
    #             name: 'Name', # required
    #           },
    #           spigot: {
    #             name: 'Name', # required
    #             path: 'Path', # required
    #             topk: 1,
    #             prob: 1.0
    #           },
    #           join: {
    #             name: 'Name', # required
    #             inputs: [
    #               'member'
    #             ], # required
    #             join_type: 'equijoin', # required - accepts ["equijoin", "left", "right", "outer", "leftsemi", "leftanti"]
    #             columns: [
    #               {
    #                 from: 'From', # required
    #               }
    #             ] # required
    #           },
    #           split_fields: {
    #             name: 'Name', # required
    #           },
    #           select_from_collection: {
    #             name: 'Name', # required
    #             index: 1 # required
    #           },
    #           fill_missing_values: {
    #             name: 'Name', # required
    #             imputed_path: 'ImputedPath', # required
    #             filled_path: 'FilledPath'
    #           },
    #           filter: {
    #             name: 'Name', # required
    #             logical_operator: 'AND', # required - accepts ["AND", "OR"]
    #             filters: [
    #               {
    #                 operation: 'EQ', # required - accepts ["EQ", "LT", "GT", "LTE", "GTE", "REGEX", "ISNULL"]
    #                 negated: false,
    #                 values: [
    #                   {
    #                     type: 'COLUMNEXTRACTED', # required - accepts ["COLUMNEXTRACTED", "CONSTANT"]
    #                   }
    #                 ] # required
    #               }
    #             ] # required
    #           },
    #           custom_code: {
    #             name: 'Name', # required
    #             inputs: [
    #               'member'
    #             ], # required
    #             code: 'Code', # required
    #             class_name: 'ClassName', # required
    #           },
    #           spark_sql: {
    #             name: 'Name', # required
    #             sql_query: 'SqlQuery', # required
    #             sql_aliases: [
    #               {
    #                 from: 'From', # required
    #                 alias: 'Alias' # required
    #               }
    #             ], # required
    #           },
    #           direct_kinesis_source: {
    #             name: 'Name', # required
    #             window_size: 1,
    #             detect_schema: false,
    #             streaming_options: {
    #               endpoint_url: 'EndpointUrl',
    #               stream_name: 'StreamName',
    #               classification: 'Classification',
    #               delimiter: 'Delimiter',
    #               starting_position: 'latest', # accepts ["latest", "trim_horizon", "earliest"]
    #               max_fetch_time_in_ms: 1,
    #               max_fetch_records_per_shard: 1,
    #               max_record_per_read: 1,
    #               add_idle_time_between_reads: false,
    #               idle_time_between_reads_in_ms: 1,
    #               describe_shard_interval: 1,
    #               num_retries: 1,
    #               retry_interval_ms: 1,
    #               max_retry_interval_ms: 1,
    #               avoid_empty_batches: false,
    #               stream_arn: 'StreamArn',
    #               role_arn: 'RoleArn',
    #               role_session_name: 'RoleSessionName'
    #             },
    #             data_preview_options: {
    #               polling_time: 1,
    #               record_polling_limit: 1
    #             }
    #           },
    #           direct_kafka_source: {
    #             name: 'Name', # required
    #             streaming_options: {
    #               bootstrap_servers: 'BootstrapServers',
    #               security_protocol: 'SecurityProtocol',
    #               connection_name: 'ConnectionName',
    #               topic_name: 'TopicName',
    #               assign: 'Assign',
    #               subscribe_pattern: 'SubscribePattern',
    #               classification: 'Classification',
    #               delimiter: 'Delimiter',
    #               starting_offsets: 'StartingOffsets',
    #               ending_offsets: 'EndingOffsets',
    #               poll_timeout_ms: 1,
    #               num_retries: 1,
    #               retry_interval_ms: 1,
    #               max_offsets_per_trigger: 1,
    #               min_partitions: 1
    #             },
    #             window_size: 1,
    #             detect_schema: false,
    #           },
    #           catalog_kinesis_source: {
    #             name: 'Name', # required
    #             window_size: 1,
    #             detect_schema: false,
    #             table: 'Table', # required
    #             database: 'Database', # required
    #           },
    #           catalog_kafka_source: {
    #             name: 'Name', # required
    #             window_size: 1,
    #             detect_schema: false,
    #             table: 'Table', # required
    #             database: 'Database', # required
    #           },
    #           drop_null_fields: {
    #             name: 'Name', # required
    #             null_check_box_list: {
    #               is_empty: false,
    #               is_null_string: false,
    #               is_neg_one: false
    #             },
    #             null_text_list: [
    #               {
    #                 value: 'Value', # required
    #                 datatype: {
    #                   id: 'Id', # required
    #                   label: 'Label' # required
    #                 } # required
    #               }
    #             ]
    #           },
    #           merge: {
    #             name: 'Name', # required
    #             source: 'Source', # required
    #           },
    #           union: {
    #             name: 'Name', # required
    #             union_type: 'ALL' # required - accepts ["ALL", "DISTINCT"]
    #           },
    #           pii_detection: {
    #             name: 'Name', # required
    #             pii_type: 'RowAudit', # required - accepts ["RowAudit", "RowMasking", "ColumnAudit", "ColumnMasking"]
    #             output_column_name: 'OutputColumnName',
    #             sample_fraction: 1.0,
    #             threshold_fraction: 1.0,
    #             mask_value: 'MaskValue'
    #           },
    #           aggregate: {
    #             name: 'Name', # required
    #             aggs: [
    #               {
    #                 agg_func: 'avg' # required - accepts ["avg", "countDistinct", "count", "first", "last", "kurtosis", "max", "min", "skewness", "stddev_samp", "stddev_pop", "sum", "sumDistinct", "var_samp", "var_pop"]
    #               }
    #             ] # required
    #           },
    #           drop_duplicates: {
    #             name: 'Name', # required
    #             columns: [
    #               [
    #                 'member'
    #               ]
    #             ]
    #           },
    #           governed_catalog_target: {
    #             name: 'Name', # required
    #             table: 'Table', # required
    #             database: 'Database', # required
    #           },
    #           governed_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table', # required
    #             partition_predicate: 'PartitionPredicate',
    #           },
    #           microsoft_sql_server_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           my_sql_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           oracle_sql_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           postgre_sql_catalog_source: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           microsoft_sql_server_catalog_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           my_sql_catalog_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           oracle_sql_catalog_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           },
    #           postgre_sql_catalog_target: {
    #             name: 'Name', # required
    #             database: 'Database', # required
    #             table: 'Table' # required
    #           }
    #         }
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateJobOutput
    #   resp.data.job_name #=> String
    #
    def update_job(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateJobInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateJobInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateJob
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateJob
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateJob,
        stubs: @stubs,
        params_class: Params::UpdateJobOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_job
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing machine learning transform. Call this operation to tune the algorithm parameters to achieve better results.</p>
    # 	
    #          <p>After calling this operation, you can call the <code>StartMLEvaluationTaskRun</code>
    #       operation to assess how well your new parameters achieved your goals (such as improving the
    #       quality of your machine learning transform, or making it more cost-effective).</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateMLTransformInput}.
    #
    # @option params [String] :transform_id
    #   <p>A unique identifier that was generated when the transform was created.</p>
    #
    # @option params [String] :name
    #   <p>The unique name that you gave the transform when you created it.</p>
    #
    # @option params [String] :description
    #   <p>A description of the transform. The default is an empty string.</p>
    #
    # @option params [TransformParameters] :parameters
    #   <p>The configuration parameters that are specific to the transform type (algorithm) used.
    #         Conditionally dependent on the transform type.</p>
    #
    # @option params [String] :role
    #   <p>The name or Amazon Resource Name (ARN) of the IAM role with the required
    #         permissions.</p>
    #
    # @option params [String] :glue_version
    #   <p>This value determines which version of Glue this machine learning transform is compatible with. Glue 1.0 is recommended for most customers. If the value is not set, the Glue compatibility defaults to Glue 0.9.  For more information, see <a href="https://docs.aws.amazon.com/glue/latest/dg/release-notes.html#release-notes-versions">Glue Versions</a> in the developer guide.</p>
    #
    # @option params [Float] :max_capacity
    #   <p>The number of Glue data processing units (DPUs) that are allocated to task runs for this transform. You can allocate from 2 to 100 DPUs; the default is 10. A DPU is a relative measure of
    #         processing power that consists of 4 vCPUs of compute capacity and 16 GB of memory. For more
    #         information, see the <a href="https://aws.amazon.com/glue/pricing/">Glue pricing
    #           page</a>. </p>
    #   		
    #            <p>When the <code>WorkerType</code> field is set to a value other than <code>Standard</code>, the <code>MaxCapacity</code> field is set automatically and becomes read-only.</p>
    #
    # @option params [String] :worker_type
    #   <p>The type of predefined worker that is allocated when this task runs. Accepts a value of Standard, G.1X, or G.2X.</p>
    #   	        <ul>
    #               <li>
    #                  <p>For the <code>Standard</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 50GB disk, and 2 executors per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.1X</code> worker type, each worker provides 4 vCPU, 16 GB of memory and a 64GB disk, and 1 executor per worker.</p>
    #               </li>
    #               <li>
    #                  <p>For the <code>G.2X</code> worker type, each worker provides 8 vCPU, 32 GB of memory and a 128GB disk, and 1 executor per worker.</p>
    #               </li>
    #            </ul>
    #
    # @option params [Integer] :number_of_workers
    #   <p>The number of workers of a defined <code>workerType</code> that are allocated when this task runs.</p>
    #
    # @option params [Integer] :timeout
    #   <p>The timeout for a task run for this transform in minutes. This is the maximum time that a task run for this transform can consume resources before it is terminated and enters <code>TIMEOUT</code> status. The default is 2,880 minutes (48 hours).</p>
    #
    # @option params [Integer] :max_retries
    #   <p>The maximum number of times to retry a task for this transform after a task run fails.</p>
    #
    # @return [Types::UpdateMLTransformOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_ml_transform(
    #     transform_id: 'TransformId', # required
    #     name: 'Name',
    #     description: 'Description',
    #     parameters: {
    #       transform_type: 'FIND_MATCHES', # required - accepts ["FIND_MATCHES"]
    #       find_matches_parameters: {
    #         primary_key_column_name: 'PrimaryKeyColumnName',
    #         precision_recall_tradeoff: 1.0,
    #         accuracy_cost_tradeoff: 1.0,
    #         enforce_provided_labels: false
    #       }
    #     },
    #     role: 'Role',
    #     glue_version: 'GlueVersion',
    #     max_capacity: 1.0,
    #     worker_type: 'Standard', # accepts ["Standard", "G.1X", "G.2X"]
    #     number_of_workers: 1,
    #     timeout: 1,
    #     max_retries: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateMLTransformOutput
    #   resp.data.transform_id #=> String
    #
    def update_ml_transform(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateMLTransformInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateMLTransformInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateMLTransform
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::AccessDeniedException]),
        data_parser: Parsers::UpdateMLTransform
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateMLTransform,
        stubs: @stubs,
        params_class: Params::UpdateMLTransformOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_ml_transform
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a partition.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdatePartitionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the partition to be updated resides. If none is provided,
    #         the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the table in question
    #         resides.</p>
    #
    # @option params [String] :table_name
    #   <p>The name of the table in which the partition to be updated is located.</p>
    #
    # @option params [Array<String>] :partition_value_list
    #   <p>List of partition key values that define the partition to update.</p>
    #
    # @option params [PartitionInput] :partition_input
    #   <p>The new partition object to update the partition to.</p>
    #   	
    #   	        <p>The <code>Values</code> property can't be changed. If you want to change the partition key values for a partition, delete and recreate the partition.</p>
    #
    # @return [Types::UpdatePartitionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_partition(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_name: 'TableName', # required
    #     partition_value_list: [
    #       'member'
    #     ], # required
    #     partition_input: {
    #       values: [
    #         'member'
    #       ],
    #       last_access_time: Time.now,
    #       storage_descriptor: {
    #         columns: [
    #           {
    #             name: 'Name', # required
    #             type: 'Type',
    #             comment: 'Comment',
    #             parameters: {
    #               'key' => 'value'
    #             }
    #           }
    #         ],
    #         location: 'Location',
    #         additional_locations: [
    #           'member'
    #         ],
    #         input_format: 'InputFormat',
    #         output_format: 'OutputFormat',
    #         compressed: false,
    #         number_of_buckets: 1,
    #         serde_info: {
    #           name: 'Name',
    #           serialization_library: 'SerializationLibrary',
    #         },
    #         bucket_columns: [
    #           'member'
    #         ],
    #         sort_columns: [
    #           {
    #             column: 'Column', # required
    #             sort_order: 1 # required
    #           }
    #         ],
    #         skewed_info: {
    #           skewed_column_values: [
    #             'member'
    #           ],
    #           skewed_column_value_location_maps: {
    #             'key' => 'value'
    #           }
    #         },
    #         stored_as_sub_directories: false,
    #         schema_reference: {
    #           schema_id: {
    #             schema_arn: 'SchemaArn',
    #             schema_name: 'SchemaName',
    #             registry_name: 'RegistryName'
    #           },
    #           schema_version_id: 'SchemaVersionId',
    #           schema_version_number: 1
    #         }
    #       },
    #       last_analyzed_time: Time.now
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdatePartitionOutput
    #
    def update_partition(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdatePartitionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdatePartitionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdatePartition
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdatePartition
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdatePartition,
        stubs: @stubs,
        params_class: Params::UpdatePartitionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_partition
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing registry which is used to hold a collection of schemas. The updated properties relate to the registry, and do not modify any of the schemas within the registry. </p>
    #
    # @param [Hash] params
    #   See {Types::UpdateRegistryInput}.
    #
    # @option params [RegistryId] :registry_id
    #   <p>This is a wrapper structure that may contain the registry name and Amazon Resource Name (ARN).</p>
    #
    # @option params [String] :description
    #   <p>A description of the registry. If description is not provided, this field will not be updated.</p>
    #
    # @return [Types::UpdateRegistryOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_registry(
    #     registry_id: {
    #       registry_name: 'RegistryName',
    #       registry_arn: 'RegistryArn'
    #     }, # required
    #     description: 'Description' # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateRegistryOutput
    #   resp.data.registry_name #=> String
    #   resp.data.registry_arn #=> String
    #
    def update_registry(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateRegistryInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateRegistryInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateRegistry
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::UpdateRegistry
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateRegistry,
        stubs: @stubs,
        params_class: Params::UpdateRegistryOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_registry
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates the description, compatibility setting, or version checkpoint for a schema set.</p>
    # 	        <p>For updating the compatibility setting, the call will not validate compatibility for the entire set of schema versions with the new compatibility setting. If the value for <code>Compatibility</code> is provided, the <code>VersionNumber</code> (a checkpoint) is also required. The API will validate the checkpoint version number for consistency.</p>
    #          <p>If the value for the <code>VersionNumber</code> (checkpoint) is provided, <code>Compatibility</code> is optional and this can be used to set/reset a checkpoint for the schema.</p>
    # 	        <p>This update will happen only if the schema is in the AVAILABLE state.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateSchemaInput}.
    #
    # @option params [SchemaId] :schema_id
    #   <p>This is a wrapper structure to contain schema identity fields. The structure contains:</p>
    #   	        <ul>
    #               <li>
    #                  <p>SchemaId$SchemaArn: The Amazon Resource Name (ARN) of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #               <li>
    #                  <p>SchemaId$SchemaName: The name of the schema. One of <code>SchemaArn</code> or <code>SchemaName</code> has to be provided.</p>
    #               </li>
    #            </ul>
    #
    # @option params [SchemaVersionNumber] :schema_version_number
    #   <p>Version number required for check pointing. One of <code>VersionNumber</code> or <code>Compatibility</code> has to be provided.</p>
    #
    # @option params [String] :compatibility
    #   <p>The new compatibility setting for the schema.</p>
    #
    # @option params [String] :description
    #   <p>The new description for the schema.</p>
    #
    # @return [Types::UpdateSchemaOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_schema(
    #     schema_id: {
    #       schema_arn: 'SchemaArn',
    #       schema_name: 'SchemaName',
    #       registry_name: 'RegistryName'
    #     }, # required
    #     schema_version_number: {
    #       latest_version: false,
    #       version_number: 1
    #     },
    #     compatibility: 'NONE', # accepts ["NONE", "DISABLED", "BACKWARD", "BACKWARD_ALL", "FORWARD", "FORWARD_ALL", "FULL", "FULL_ALL"]
    #     description: 'Description'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateSchemaOutput
    #   resp.data.schema_arn #=> String
    #   resp.data.schema_name #=> String
    #   resp.data.registry_name #=> String
    #
    def update_schema(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateSchemaInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateSchemaInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateSchema
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::AccessDeniedException]),
        data_parser: Parsers::UpdateSchema
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateSchema,
        stubs: @stubs,
        params_class: Params::UpdateSchemaOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_schema
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a metadata table in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateTableInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the table resides. If none is provided, the Amazon Web Services account
    #         ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database in which the table resides. For Hive
    #         compatibility, this name is entirely lowercase.</p>
    #
    # @option params [TableInput] :table_input
    #   <p>An updated <code>TableInput</code> object to define the metadata table
    #         in the catalog.</p>
    #
    # @option params [Boolean] :skip_archive
    #   <p>By default, <code>UpdateTable</code> always creates an archived version of the table
    #         before updating it. However, if <code>skipArchive</code> is set to true,
    #           <code>UpdateTable</code> does not create the archived version.</p>
    #
    # @option params [String] :transaction_id
    #   <p>The transaction ID at which to update the table contents. </p>
    #
    # @return [Types::UpdateTableOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_table(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     table_input: {
    #       name: 'Name', # required
    #       description: 'Description',
    #       owner: 'Owner',
    #       last_access_time: Time.now,
    #       last_analyzed_time: Time.now,
    #       retention: 1,
    #       storage_descriptor: {
    #         columns: [
    #           {
    #             name: 'Name', # required
    #             type: 'Type',
    #             comment: 'Comment',
    #             parameters: {
    #               'key' => 'value'
    #             }
    #           }
    #         ],
    #         location: 'Location',
    #         additional_locations: [
    #           'member'
    #         ],
    #         input_format: 'InputFormat',
    #         output_format: 'OutputFormat',
    #         compressed: false,
    #         number_of_buckets: 1,
    #         serde_info: {
    #           name: 'Name',
    #           serialization_library: 'SerializationLibrary',
    #         },
    #         bucket_columns: [
    #           'member'
    #         ],
    #         sort_columns: [
    #           {
    #             column: 'Column', # required
    #             sort_order: 1 # required
    #           }
    #         ],
    #         skewed_info: {
    #           skewed_column_values: [
    #             'member'
    #           ],
    #           skewed_column_value_location_maps: {
    #             'key' => 'value'
    #           }
    #         },
    #         stored_as_sub_directories: false,
    #         schema_reference: {
    #           schema_id: {
    #             schema_arn: 'SchemaArn',
    #             schema_name: 'SchemaName',
    #             registry_name: 'RegistryName'
    #           },
    #           schema_version_id: 'SchemaVersionId',
    #           schema_version_number: 1
    #         }
    #       },
    #       view_original_text: 'ViewOriginalText',
    #       view_expanded_text: 'ViewExpandedText',
    #       table_type: 'TableType',
    #       target_table: {
    #         catalog_id: 'CatalogId',
    #         database_name: 'DatabaseName',
    #         name: 'Name'
    #       }
    #     }, # required
    #     skip_archive: false,
    #     transaction_id: 'TransactionId',
    #     version_id: 'VersionId'
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateTableOutput
    #
    def update_table(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateTableInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateTableInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateTable
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::ResourceNotReadyException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException, Errors::ResourceNumberLimitExceededException]),
        data_parser: Parsers::UpdateTable
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateTable,
        stubs: @stubs,
        params_class: Params::UpdateTableOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_table
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates a trigger definition.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateTriggerInput}.
    #
    # @option params [String] :name
    #   <p>The name of the trigger to update.</p>
    #
    # @option params [TriggerUpdate] :trigger_update
    #   <p>The new values with which to update the trigger.</p>
    #
    # @return [Types::UpdateTriggerOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_trigger(
    #     name: 'Name', # required
    #     trigger_update: {
    #       name: 'Name',
    #       description: 'Description',
    #       schedule: 'Schedule',
    #       actions: [
    #         {
    #           job_name: 'JobName',
    #           arguments: {
    #             'key' => 'value'
    #           },
    #           timeout: 1,
    #           security_configuration: 'SecurityConfiguration',
    #           notification_property: {
    #             notify_delay_after: 1
    #           },
    #           crawler_name: 'CrawlerName'
    #         }
    #       ],
    #       predicate: {
    #         logical: 'AND', # accepts ["AND", "ANY"]
    #         conditions: [
    #           {
    #             logical_operator: 'EQUALS', # accepts ["EQUALS"]
    #             job_name: 'JobName',
    #             state: 'STARTING', # accepts ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #             crawler_name: 'CrawlerName',
    #             crawl_state: 'RUNNING' # accepts ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #           }
    #         ]
    #       },
    #       event_batching_condition: {
    #         batch_size: 1, # required
    #         batch_window: 1
    #       }
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateTriggerOutput
    #   resp.data.trigger #=> Types::Trigger
    #   resp.data.trigger.name #=> String
    #   resp.data.trigger.workflow_name #=> String
    #   resp.data.trigger.id #=> String
    #   resp.data.trigger.type #=> String, one of ["SCHEDULED", "CONDITIONAL", "ON_DEMAND", "EVENT"]
    #   resp.data.trigger.state #=> String, one of ["CREATING", "CREATED", "ACTIVATING", "ACTIVATED", "DEACTIVATING", "DEACTIVATED", "DELETING", "UPDATING"]
    #   resp.data.trigger.description #=> String
    #   resp.data.trigger.schedule #=> String
    #   resp.data.trigger.actions #=> Array<Action>
    #   resp.data.trigger.actions[0] #=> Types::Action
    #   resp.data.trigger.actions[0].job_name #=> String
    #   resp.data.trigger.actions[0].arguments #=> Hash<String, String>
    #   resp.data.trigger.actions[0].arguments['key'] #=> String
    #   resp.data.trigger.actions[0].timeout #=> Integer
    #   resp.data.trigger.actions[0].security_configuration #=> String
    #   resp.data.trigger.actions[0].notification_property #=> Types::NotificationProperty
    #   resp.data.trigger.actions[0].notification_property.notify_delay_after #=> Integer
    #   resp.data.trigger.actions[0].crawler_name #=> String
    #   resp.data.trigger.predicate #=> Types::Predicate
    #   resp.data.trigger.predicate.logical #=> String, one of ["AND", "ANY"]
    #   resp.data.trigger.predicate.conditions #=> Array<Condition>
    #   resp.data.trigger.predicate.conditions[0] #=> Types::Condition
    #   resp.data.trigger.predicate.conditions[0].logical_operator #=> String, one of ["EQUALS"]
    #   resp.data.trigger.predicate.conditions[0].job_name #=> String
    #   resp.data.trigger.predicate.conditions[0].state #=> String, one of ["STARTING", "RUNNING", "STOPPING", "STOPPED", "SUCCEEDED", "FAILED", "TIMEOUT"]
    #   resp.data.trigger.predicate.conditions[0].crawler_name #=> String
    #   resp.data.trigger.predicate.conditions[0].crawl_state #=> String, one of ["RUNNING", "CANCELLING", "CANCELLED", "SUCCEEDED", "FAILED"]
    #   resp.data.trigger.event_batching_condition #=> Types::EventBatchingCondition
    #   resp.data.trigger.event_batching_condition.batch_size #=> Integer
    #   resp.data.trigger.event_batching_condition.batch_window #=> Integer
    #
    def update_trigger(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateTriggerInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateTriggerInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateTrigger
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateTrigger
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateTrigger,
        stubs: @stubs,
        params_class: Params::UpdateTriggerOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_trigger
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing function definition in the Data Catalog.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateUserDefinedFunctionInput}.
    #
    # @option params [String] :catalog_id
    #   <p>The ID of the Data Catalog where the function to be updated is located. If none is
    #         provided, the Amazon Web Services account ID is used by default.</p>
    #
    # @option params [String] :database_name
    #   <p>The name of the catalog database where the function to be updated is
    #         located.</p>
    #
    # @option params [String] :function_name
    #   <p>The name of the function.</p>
    #
    # @option params [UserDefinedFunctionInput] :function_input
    #   <p>A <code>FunctionInput</code> object that redefines the function in the Data
    #         Catalog.</p>
    #
    # @return [Types::UpdateUserDefinedFunctionOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_user_defined_function(
    #     catalog_id: 'CatalogId',
    #     database_name: 'DatabaseName', # required
    #     function_name: 'FunctionName', # required
    #     function_input: {
    #       function_name: 'FunctionName',
    #       class_name: 'ClassName',
    #       owner_name: 'OwnerName',
    #       owner_type: 'USER', # accepts ["USER", "ROLE", "GROUP"]
    #       resource_uris: [
    #         {
    #           resource_type: 'JAR', # accepts ["JAR", "FILE", "ARCHIVE"]
    #           uri: 'Uri'
    #         }
    #       ]
    #     } # required
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateUserDefinedFunctionOutput
    #
    def update_user_defined_function(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateUserDefinedFunctionInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateUserDefinedFunctionInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateUserDefinedFunction
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::GlueEncryptionException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateUserDefinedFunction
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateUserDefinedFunction,
        stubs: @stubs,
        params_class: Params::UpdateUserDefinedFunctionOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_user_defined_function
        )
      )
      raise resp.error if resp.error
      resp
    end

    # <p>Updates an existing workflow.</p>
    #
    # @param [Hash] params
    #   See {Types::UpdateWorkflowInput}.
    #
    # @option params [String] :name
    #   <p>Name of the workflow to be updated.</p>
    #
    # @option params [String] :description
    #   <p>The description of the workflow.</p>
    #
    # @option params [Hash<String, String>] :default_run_properties
    #   <p>A collection of properties to be used as part of each execution of the workflow.</p>
    #
    # @option params [Integer] :max_concurrent_runs
    #   <p>You can use this parameter to prevent unwanted multiple updates to data, to control costs, or in some cases, to prevent exceeding the maximum number of concurrent runs of any of the component jobs. If you leave this parameter blank, there is no limit to the number of concurrent workflow runs.</p>
    #
    # @return [Types::UpdateWorkflowOutput]
    #
    # @example Request syntax with placeholder values
    #
    #   resp = client.update_workflow(
    #     name: 'Name', # required
    #     description: 'Description',
    #     default_run_properties: {
    #       'key' => 'value'
    #     },
    #     max_concurrent_runs: 1
    #   )
    #
    # @example Response structure
    #
    #   resp.data #=> Types::UpdateWorkflowOutput
    #   resp.data.name #=> String
    #
    def update_workflow(params = {}, options = {}, &block)
      stack = Hearth::MiddlewareStack.new
      input = Params::UpdateWorkflowInput.build(params)
      response_body = ::StringIO.new
      stack.use(Hearth::Middleware::Validate,
        validator: Validators::UpdateWorkflowInput,
        validate_input: @config.validate_input
      )
      stack.use(Hearth::Middleware::Build,
        builder: Builders::UpdateWorkflow
      )
      stack.use(Hearth::HTTP::Middleware::ContentLength)
      stack.use(Hearth::Middleware::Retry,
        retry_mode: @config.retry_mode,
        error_inspector_class: Hearth::Retry::ErrorInspector,
        retry_quota: @retry_quota,
        max_attempts: @config.max_attempts,
        client_rate_limiter: @client_rate_limiter,
        adaptive_retry_wait_to_fill: @config.adaptive_retry_wait_to_fill
      )
      stack.use(AWS::SDK::Core::Middleware::Signer,
        signer: @config.signer
      )
      stack.use(Hearth::Middleware::Parse,
        error_parser: Hearth::HTTP::ErrorParser.new(error_module: Errors, success_status: 200, errors: [Errors::ConcurrentModificationException, Errors::InternalServiceException, Errors::EntityNotFoundException, Errors::InvalidInputException, Errors::OperationTimeoutException]),
        data_parser: Parsers::UpdateWorkflow
      )
      stack.use(Middleware::RequestId)
      stack.use(Hearth::Middleware::Send,
        stub_responses: @config.stub_responses,
        client: Hearth::HTTP::Client.new(logger: @config.logger, http_wire_trace: options.fetch(:http_wire_trace, @config.http_wire_trace)),
        stub_class: Stubs::UpdateWorkflow,
        stubs: @stubs,
        params_class: Params::UpdateWorkflowOutput
      )
      apply_middleware(stack, options[:middleware])

      resp = stack.run(
        input: input,
        context: Hearth::Context.new(
          request: Hearth::HTTP::Request.new(url: options.fetch(:endpoint, @config.endpoint)),
          response: Hearth::HTTP::Response.new(body: response_body),
          params: params,
          logger: @config.logger,
          operation_name: :update_workflow
        )
      )
      raise resp.error if resp.error
      resp
    end

    private

    def apply_middleware(middleware_stack, middleware)
      Client.middleware.apply(middleware_stack)
      @middleware.apply(middleware_stack)
      Hearth::MiddlewareBuilder.new(middleware).apply(middleware_stack)
    end
  end
end
